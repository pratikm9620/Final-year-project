Id,TITLE,ABSTRACT,INTRODUCTION TO IS,DATA & KNOWLEDGE MANAGEMENT,ETHICAL ISSUES & PRIVACY,SOCIAL COMPUTING,COMPUTER NETWORKS,IS WITHIN ORGANIZATION,COMPUTER SCIENCE
1,Computer Based Information Systems,"A Computer-Based Information System (CBIS) is a set of interconnected components working together to collect,process, store, and distribute information to support decision-making, coordination, control, analysis, and visualization in an organization.A computer-based information system is a technologically implemented medium for recording, storing, and disseminating linguistic expressions.",1,,,,,,
2,Components of Computer Based Information System,".Computer-Based Information Systems (CBIS) encompass a set of interconnected components designed to efficiently collect, process, store, and disseminate information within an organizational framework. At its core, CBIS consists of hardware, constituting the physical elements like the central processing unit, memory, input and output devices, and network components. Software, another integral component, comprises operating systems, application software, and system software, providing the necessary instructions for hardware operations. The data component involves raw information, categorized into structured, unstructured, or semi-structured formats, forming the basis for decision-making. Procedures, outlining rules and guidelines, dictate how the system operates, encompassing daily tasks, security protocols, and backup procedures. Lastly, personnel represent the individuals interacting with the system, including system analysts, programmers, database administrators, and end-users. The seamless interaction of these components ensures the effective functioning of the CBIS, facilitating organizational processes across various levels of management.",1,,,,,,
3,Types of Computer Based Information Systems,"Computer-Based Information Systems (CBIS) manifest in various types, each tailored to specific organizational needs. One prominent category is Transaction Processing Systems (TPS), designed to handle routine transactions, such as order processing and inventory management, ensuring accuracy and efficiency. Another crucial type is Management Information Systems (MIS), providing middle-level managers with summarized reports and information to support decision-making. Decision Support Systems (DSS) offer analytical tools and data to assist in complex decision-making processes, often involving ""what-if"" scenarios. Executive Information Systems (EIS) cater to top-level executives, offering strategic information and key performance indicators for organizational leadership. Enterprise Resource Planning (ERP) systems integrate various business processes, promoting efficiency and collaboration across different departments. Lastly, Knowledge Management Systems (KMS) focus on capturing, organizing, and retrieving an organization's collective knowledge to enhance decision-making and innovation. The diverse types of CBIS collectively contribute to streamlining organizational processes and enhancing overall productivity.",1,,,,,,
4,Roles of Computer Based Information Systems in Organizations,"Computer-Based Information Systems (CBIS) play multifaceted roles in organizations, serving as indispensable tools for enhancing operational efficiency, decision-making, and overall strategic management. One crucial role is in automating routine and repetitive tasks through Transaction Processing Systems (TPS). TPS streamline everyday operations, such as order processing and inventory management, ensuring accuracy and timeliness in data handling.
CBIS also contribute significantly to decision support through Decision Support Systems (DSS). These systems assist management in analyzing complex data sets, identifying trends, and making informed decisions. Moreover, Executive Information Systems (EIS) provide top-level executives with summarized reports and dashboards, offering a comprehensive overview of the organization's performance.
Facilitating collaboration and communication is another vital role of CBIS. Through networking and collaborative tools, these systems connect individuals and departments, fostering efficient information exchange and teamwork. Additionally, CBIS enable effective resource planning through Enterprise Resource Planning (ERP) systems, integrating various organizational functions like finance, human resources, and supply chain management.
Security and data management represent critical roles of CBIS, ensuring the confidentiality, integrity, and availability of information. Robust security measures, including access controls and encryption, protect sensitive data from unauthorized access. Meanwhile, Database Management Systems (DBMS) organize and manage vast amounts of data, enabling efficient retrieval and manipulation.
CBIS also support strategic planning and innovation. They provide a platform for organizations to adapt to technological advancements, implement new business models, and stay competitive in dynamic markets. Strategic Information Systems (SIS) align with organizational goals, helping in the formulation and execution of long-term strategies.
In conclusion, CBIS serve as the backbone of modern organizations, influencing various aspects of their functioning. From automating routine tasks to aiding decision-making and ensuring data security, CBIS contribute significantly to organizational success and resilience in an ever-evolving business landscape.",1,,,,,,
5,Challenges in Computer Based Information Systems,"Despite their myriad benefits, Computer-Based Information Systems (CBIS) encounter several challenges that organizations must navigate to ensure their effective implementation and sustained operation. One of the primary challenges is cybersecurity. With the increasing frequency and sophistication of cyber threats, protecting sensitive data from unauthorized access, breaches, and cyberattacks has become a paramount concern. Organizations must invest in robust cybersecurity measures, including firewalls, encryption, and employee training, to mitigate these risks.Another challenge is the rapid pace of technological advancements. CBIS components and software quickly become obsolete, necessitating continuous updates and upgrades. This not only demands financial investments but also poses the risk of system disruptions during transitions. Organizations need agile strategies to keep their CBIS up-to-date without compromising operational continuity.Interoperability issues present another significant challenge. As organizations adopt diverse CBIS components from different vendors, integrating these systems seamlessly becomes complex. Ensuring that various systems can communicate and share data without loss of information or functionality is an ongoing challenge in the CBIS landscape.Data management and privacy concerns also pose challenges. As organizations accumulate vast amounts of data, maintaining data quality, ensuring compliance with privacy regulations, and addressing ethical considerations become critical. CBIS must be designed to handle, store, and process data ethically and securely, respecting privacy regulations and safeguarding sensitive information.Human factors contribute to the challenges in CBIS implementation. Resistance to change among employees, inadequate training, and a lack of IT skills can hinder the successful adoption and utilization of CBIS. To overcome these challenges, organizations need comprehensive training programs, change management strategies, and ongoing support to foster a culture of technological fluency.In conclusion, while CBIS offer substantial advantages, organizations must confront and address various challenges to leverage these systems effectively. From cybersecurity concerns to technological obsolescence, interoperability issues, and human factors, a holistic and adaptive approach is essential to ensure the seamless integration and optimal performance of CBIS in the organizational context.",1,,,,,,
6,Applications of Computer Based Information Systems,"Computer-Based Information Systems (CBIS) find wide-ranging applications across diverse sectors, contributing significantly to enhanced efficiency, decision-making, and overall organizational performance. In the business domain, CBIS plays a pivotal role in supporting various business functions, including finance, human resources, marketing, and supply chain management. These systems automate routine tasks, facilitate data analysis, and provide real-time insights, enabling organizations to streamline operations and make informed strategic decisions.
In healthcare, CBIS is instrumental in managing patient records, scheduling appointments, and improving the overall quality of healthcare delivery. Electronic Health Records (EHRs) centralize patient information, promoting efficient communication among healthcare professionals and ensuring accurate and timely diagnoses and treatments. CBIS also supports medical research by enabling the analysis of vast datasets to identify trends and patterns, contributing to advancements in healthcare practices.
Education benefits significantly from CBIS through the implementation of Learning Management Systems (LMS) and educational software. These systems facilitate online learning, automate administrative tasks, and provide personalized learning experiences. CBIS enhances collaboration among students and educators, transcending geographical constraints and fostering a dynamic and interactive learning environment.
In the government sector, CBIS aids in public administration, policy formulation, and service delivery. E-Government initiatives leverage CBIS to enhance citizen services, automate administrative processes, and improve the accessibility of government information. This contributes to increased transparency, efficiency, and responsiveness in governance.
CBIS also plays a crucial role in scientific research and development. It enables data analysis, simulations, and modeling, accelerating the pace of scientific discovery and innovation. Researchers utilize CBIS to manage and process large datasets, conduct complex simulations, and collaborate on a global scale, fostering breakthroughs in various scientific disciplines.The applications of Computer-Based Information Systems span diverse fields, revolutionizing how organizations operate and deliver services. From business optimization to healthcare, education, government services, and scientific research, CBIS continues to be a transformative force, driving advancements and improvements across various sectors.",1,,,,,,
7,Impact of Information Technology on organizations,"The impact of Information Technology (IT) on organizations is profound, reshaping the way businesses operate, make decisions, and interact with stakeholders. Several key aspects highlight the transformative influence of IT on organizations 1. Operational Efficiency: IT systems streamline and automate business processes, reducing manual effort and minimizing errors. Enterprise Resource Planning (ERP) systems, for instance, integrate various functions like finance, human resources, and supply chain management, leading to increased operational efficiency.2. Decision-Making and Analytics: IT provides organizations with powerful tools for data analysis and business intelligence. Decision support systems and analytics enable leaders to make informed decisions by extracting meaningful insights from large datasets. Real-time reporting and dashboards further enhance the decision-making process.3. Communication and Collaboration: IT facilitates seamless communication within organizations and beyond. Email, instant messaging, video conferencing, and collaborative platforms enable effective communication and collaboration among employees, irrespective of geographical locations. This fosters a more connected and productive work environment.4. Customer Engagement: IT has revolutionized customer interactions and relationships. Customer Relationship Management (CRM) systems enable businesses to manage customer information, track interactions, and tailor services to individual needs. Online platforms and e-commerce systems provide new channels for customer engagement.5. Globalization: IT has played a pivotal role in the globalization of businesses. Through the internet and advanced communication technologies, organizations can operate on a global scale, reaching new markets and connecting with a diverse range of customers, suppliers, and partners.6. Innovation and Competitive Advantage: IT is a catalyst for innovation, allowing organizations to develop new products, services, and business models. Companies that effectively leverage IT for innovation gain a competitive edge in the market. Cloud computing, Internet of Things (IoT), and Artificial Intelligence (AI) are examples of transformative technologies driving innovation.7. Security and Risk Management: As organizations become more reliant on digital systems, IT security becomes a critical concern. Cybersecurity measures are essential to protect sensitive data and ensure the integrity of IT infrastructure. Risk management practices in IT help organizations anticipate and mitigate potential threats.8. Flexibility and Adaptability: IT provides organizations with the flexibility to adapt to changing business environments. Cloud computing allows for scalable and flexible infrastructure, enabling businesses to adjust their IT resources based on demand. This adaptability is crucial for navigating dynamic market conditions.In summary, the impact of IT on organizations is multi-faceted, influencing various aspects of operations, strategy, and competitiveness. Embracing technological advancements strategically allows organizations to stay agile, innovative, and responsive in an ever-evolving business landscape.",1,,,,,,
8,Impact of Information System to Society ,"The impact of Information Systems on society is extensive, permeating various facets of daily life. Information Systems, encompassing technologies, applications, and networks, have contributed significantly to societal development. One key area is communication, where Information Systems have revolutionized how individuals connect, share information, and collaborate. Social media platforms, messaging apps, and online communication tools facilitate instantaneous interaction and the exchange of ideas globally.In education, Information Systems have transformed learning methodologies. Online learning platforms, educational software, and digital resources provide access to knowledge beyond geographical constraints. E-learning has become a prominent feature, offering flexibility and opportunities for skill development. The healthcare sector has witnessed the integration of Information Systems for efficient patient care. Electronic Health Records (EHRs), telemedicine, and health information systems streamline processes, enhance data accuracy, and improve medical services. This digitization contributes to better healthcare management and outcomes.In governance, Information Systems play a vital role in public administration. E-Government initiatives leverage technology to deliver services, engage citizens, and enhance transparency. Digital platforms for public services, online voting systems, and information dissemination contribute to more accessible and responsive governance.Economic systems have been reshaped by Information Systems, fostering innovation and economic growth. E-commerce platforms, financial technologies, and digital banking services have revolutionized business operations and transactions. Information Systems provide tools for market analysis, customer engagement, and strategic decision-making.However, the widespread adoption of Information Systems also raises concerns, including privacy issues, cybersecurity threats, and the digital divide. Ensuring equitable access to information technologies and addressing ethical considerations are crucial aspects of harnessing the positive impact of Information Systems on society. Hence,The impact of Information Systems on society is multifaceted, influencing communication, education, healthcare, governance, and the economy. While presenting numerous opportunities for advancement, it also necessitates careful consideration of ethical, legal, and security implications to ensure a balanced and inclusive societal transformation.",1,,,,,,
9,Competitive Advantages  achieved in Information System,"Competitive advantages achieved through Information Systems (IS) are pivotal in contemporary business environments. IS provides organizations with tools and capabilities that enhance efficiency, decision-making, and overall strategic positioning. One significant advantage is improved operational efficiency. Information Systems automate routine tasks, streamline processes, and optimize resource utilization, leading to increased productivity and reduced operational costsStrategic decision-making is another area where Information Systems confer a competitive edge. Access to real-time data, analytical tools, and business intelligence systems empowers organizations to make informed and timely decisions. This agility in decision-making enables businesses to respond swiftly to market changes, capitalize on opportunities, and mitigate risks, fostering a strategic advantage.Customer relationship management (CRM) is greatly influenced by Information Systems. Organizations leverage IS to gather, analyze, and utilize customer data effectively. This results in personalized customer experiences, targeted marketing strategies, and improved customer satisfaction, all of which contribute to a competitive advantage in the marketplace.Innovation is a key driver of competitive advantage, and Information Systems play a vital role in fostering innovation. Collaborative platforms, data analytics, and research and development support systems enable organizations to stay ahead in terms of product or service innovation. The ability to adapt and introduce new technologies gives businesses a competitive edge in dynamic markets.Supply chain management is revolutionized by Information Systems, facilitating better coordination, inventory management, and logistics optimization. This leads to reduced lead times, lower costs, and improved overall efficiency, contributing significantly to competitive advantages in the marketplace.However, achieving and sustaining competitive advantages through Information Systems come with challenges. Organizations must invest in the latest technologies, ensure data security, and develop a robust IT infrastructure. Additionally, a skilled workforce capable of harnessing the full potential of Information Systems is essential.In conclusion, competitive advantages achieved through Information Systems are diverse, ranging from operational efficiency and strategic decision-making to customer relationship management and innovation. Organizations that effectively leverage Information Systems are better positioned to navigate the complexities of the business landscape and gain a competitive edge in their respective industries.",1,,,,,,
10,Business pressure faced by organizations ,"Organizations confront a myriad of business pressures that stem from internal and external factors, shaping the dynamic landscape in which they operate.External pressures often include the rapidly evolving market trends, fierce competition, and changes in customer preferences.Organizations must adapt to these external factors to stay relevant and competitive. Technological advancements present both opportunities and challenges, requiring businesses to continually invest in and integrate new technologies to remain innovative and efficient.Regulatory compliance is another critical external pressure. Governments and regulatory bodies impose rules and standards that organizations must adhere to, often leading to increased operational complexities and the need for robust governance and compliance frameworks.Globalization introduces a set of pressures as organizations expand their operations across borders. Dealing with diverse cultures, navigating international regulations, and managing global supply chains become significant challenges. Economic fluctuations and geopolitical uncertainties also contribute to the complexities associated with globalization.Internally, organizations face pressures related to cost efficiency, resource optimization, and internal processes. The need to cut costs while maintaining or improving quality can create tension within an organization. Workforce dynamics, including skill gaps and talent management, add another layer of internal pressure. Organizations must attract, develop, and retain a skilled workforce to sustain competitiveness.The pace of change itself is a pervasive pressure. In today's fast-paced business environment, organizations must continually adapt to technological, market, and societal changes. The ability to embrace change becomes a strategic necessity.Environmental sustainability is an emerging pressure as businesses are increasingly expected to operate responsibly. Meeting environmental standards, reducing carbon footprints, and adopting sustainable practices are challenges organizations face to align with societal expectations and address environmental concerns.
",1,,,,,,
11,Porter's Five Forces Model ,"Porter's Five Forces model, developed by Michael E. Porter, is a strategic framework that analyzes the competitive forces within an industry, shaping the overall attractiveness and competitiveness of that industry. The five forces include:
1. Threat of New Entrants: This force assesses the ease with which new competitors can enter the market. Factors such as barriers to entry, economies of scale, and brand loyalty influence the threat of new entrants. Higher barriers and strong brand loyalty can mitigate this threat.
2. Bargaining Power of Buyers: The power of buyers or customers to influence pricing and terms is crucial. Factors such as the availability of alternative products, buyer information, and the importance of each buyer to the overall market impact their bargaining power. Strong buyer power can limit profitability.
3. Bargaining Power of Suppliers: Suppliers' ability to influence prices and supply terms is another critical force. If suppliers have significant power, they can dictate terms to firms in the industry. Factors like the uniqueness of inputs and the availability of alternative suppliers determine supplier power.
4. Threat of Substitute Products or Services: The availability of substitutes poses a threat to an industry. If there are many substitutes, consumers can easily switch, limiting the pricing power of firms. The perceived uniqueness of products and brand loyalty play roles in evaluating this force.
5. Intensity of Competitive Rivalry: This force assesses the degree of competition among existing firms in the industry. Factors such as the number of competitors, industry growth, and differentiation among products influence rivalry intensity. High rivalry often leads to price wars and reduced profitability.
In applying Porter's Five Forces, businesses can gain insights into their industry's competitive dynamics, helping them formulate effective strategies to navigate challenges and capitalize on opportunities. The model's holistic approach provides a comprehensive understanding of the factors influencing industry competition and aids in strategic decision-making.",1,,,,,,
12,Organizational Stratergies for competitive advantage,"Organizational strategies for competitive advantage are diverse approaches that companies employ to distinguish themselves in the marketplace and gain a superior position relative to competitors. One such strategy is cost leadership, where a firm focuses on becoming the lowest-cost producer in its industry through efficient operations and cost control. Another approach is differentiation, involving the creation of unique and high-quality products or services to stand out in the market. The focus or niche strategy concentrates efforts on specific market segments, while innovation and technology leadership emphasize staying at the forefront of technological advancements. Strategic alliances, partnerships, and a customer-centric approach are also integral strategies. Agility and adaptability play a crucial role in responding swiftly to market changes, ensuring long-term success. Companies often integrate these strategies based on their unique circumstances and industry dynamics, aligning them with capabilities, resources, and long-term objectives for sustained competitive advantage. Regular evaluation and adjustment are imperative for continued effectiveness in the evolving business landscape.",1,,,,,,
13,Importance of Data in Today's Environment,"In today's environment, the importance of data cannot be overstated as it serves as the lifeblood of modern businesses and society. Data is a valuable asset that organizations leverage to make informed decisions, gain insights, and drive innovation. With the advent of technology and the digital age, the volume, velocity, and variety of data have increased exponentially. Businesses collect and analyze data to understand customer behavior, market trends, and operational efficiencyData plays a crucial role in enhancing customer experiences by personalizing products and services based on individual preferences and needs. Moreover, data-driven insights empower organizations to optimize processes, streamline operations, and identify new opportunities for growth. In fields such as healthcare and research, data is instrumental in advancing scientific discoveries, improving patient outcomes, and addressing global challenges.The emergence of big data analytics and artificial intelligence has further accentuated the significance of data. Predictive analytics, machine learning, and data modeling enable organizations to forecast trends, mitigate risks, and gain a competitive edge in the market. However, the importance of data is coupled with the responsibility to ensure data privacy, security, and ethical use. Data serves as a cornerstone in navigating the complexities of the contemporary world. Its effective management and utilization empower businesses, drive innovation, and contribute to societal progress. Recognizing the importance of data is pivotal for harnessing its full potential and reaping the benefits of a data-driven era.",1,,,,,,
14,Data Governance ,"Data governance is a comprehensive framework that defines the policies, procedures, and guidelines for managing, accessing, and protecting an organization's data assets. It encompasses the strategy and practices that ensure high data quality, integrity, and security throughout the data lifecycle. The primary goal of data governance is to establish accountability, transparency, and compliance in the management of data.Within the framework of data governance, organizations establish roles and responsibilities for data stewards and custodians who oversee data quality, standards, and usage. These individuals work to ensure that data is accurate, consistent, and aligned with the organization's objectives. Data governance also involves defining data classifications, ensuring compliance with regulations, and mitigating risks associated with data management.Key components of data governance include data policies, data standards, data quality management, and metadata management. Data policies outline the rules and guidelines for data usage, access, and handling, while data standards provide a set of conventions to maintain consistency and interoperability. Data quality management involves processes to monitor, cleanse, and enhance data quality, ensuring that data is reliable and fit for its intended purpose. Metadata management involves documenting and cataloging metadata to provide insights into the context, lineage, and usage of data.Effective data governance promotes a culture of data accountability and transparency, fostering trust in data-driven decision-making. It is essential for organizations aiming to derive maximum value from their data assets, comply with regulatory requirements, and mitigate the risks associated with data management. Data governance is an ongoing process that evolves with technological advancements and changes in organizational needs, reflecting its critical role in modern information management.",1,1,,,,,
15,Ethics and Business Ethics,"Ethics refers to the branch of philosophy that deals with what is considered morally right or wrong, good or bad. It involves the study of moral principles and values that guide human behavior, helping individuals distinguish between acceptable and unacceptable actions. Ethics provides a framework for making decisions that align with moral standards and societal norms.Business ethics is the application of ethical principles and moral values to the context of business and organizational behavior. It involves evaluating the ethical implications of business practices, ensuring that they are in line with societal expectations and standards. Business ethics addresses issues such as honesty, integrity, fairness, transparency, and corporate social responsibility, aiming to create a framework for ethical decision-making in the business world.",1,,1,,,,
16,The code of ethics,"The fundamental code of ethics represents a foundational set of principles that guides individuals in making ethical decisions and conducting themselves with integrity. It encompasses core values such as honesty, integrity, respect, fairness, and responsibility. This code outlines the professional standards expected within a specific context, emphasizing competence, confidentiality, and accountability. It defines duties and obligations to clients, colleagues, the organization, and the broader community. The code addresses conflicts of interest, emphasizing the identification, disclosure, and management of such conflicts. It underscores compliance with laws and regulations, ensuring that activities are conducted within legal boundaries. The code encourages reporting unethical behavior and provides protection for whistleblowers. It promotes ongoing professional development and learning, recognizing the importance of staying informed and continuously improving skills. Enforcement mechanisms and potential consequences for violations are outlined, fostering a commitment to accountability. The code is a living document subject to periodic review and updates to maintain its relevance and effectiveness. Overall, the fundamental code of ethics serves as a guiding framework for ethical conduct, fostering a culture of integrity and responsible behavior within a given profession or organization.",1,,1,,,,
17,Ethical Frameworks,"Ethical frameworks are structured approaches or systems that guide individuals and organizations in making ethical decisions and navigating moral dilemmas. These frameworks provide a set of principles, values, and guidelines to help assess the rightness or wrongness of actions in various situations. There are several prominent ethical frameworks, each offering a unique perspective on moral reasoning.One common ethical framework is deontology, which is based on the idea of duty and adherence to moral rules. Deontological ethics asserts that certain actions are inherently right or wrong, regardless of their consequences. Another significant framework is consequentialism, where the morality of an action is determined by its outcomes. Utilitarianism, a form of consequentialism, emphasizes maximizing overall happiness or well-being as the ethical goal.The five steps of an ethical framework provide a systematic approach to ethical decision-making, guiding individuals and organizations through a thoughtful process. These steps help ensure that ethical considerations are thoroughly examined before arriving at a decision.Here are the five steps:
1. Identify the Problem or Dilemma: Begin by clearly defining the ethical problem or dilemma at hand. This involves identifying the key issues, stakeholders, and potential consequences of different courses of action.
2. Gather Relevant Information: Collect all relevant facts and information related to the ethical issue. This step may involve researching, consulting experts, and considering various perspectives to gain a comprehensive understanding of the situation.
3. Identify Stakeholders: Identify and consider all individuals or groups who may be affected by the decision. Understanding the interests and concerns of stakeholders is crucial in ethical decision-making, as it helps weigh the potential impact on different parties.
4. Explore Options and Alternatives: Generate a range of possible solutions or courses of action. Evaluate each option based on ethical principles, organizational values, and potential consequences. Consider how each alternative aligns with moral frameworks and the overall goals of the organization.
5. Make a Decision and Implement: After careful consideration of the options, make a decision that reflects ethical principles and values. Implement the chosen course of action and monitor its outcomes. It's essential to communicate the decision transparently and be prepared to reassess if new information emerges.
These five steps provide a structured and comprehensive approach to navigating ethical challenges, encouraging thoughtful analysis and promoting responsible decision-making.",1,,1,,,,
18,Introduction to Social Computing,"Social computing refers to the intersection of social behavior and computational systems, where technology and social interaction converge to create new forms of communication, collaboration, and information sharing. In this rapidly evolving field, the focus is on understanding and leveraging the dynamics of human interactions in online and digital spaces. Social computing encompasses a wide range of activities, from social media platforms and online communities to collaborative tools and virtual environments.At its core, social computing recognizes the importance of collective intelligence, harnessing the power of networks and communities to solve problems, generate ideas, and share knowledge. It involves the study of how individuals and groups engage with technology to connect, communicate, and collaborate, leading to the emergence of novel social structures and behaviors.
The advent of social computing has transformed the way people interact with information and each other. Social media platforms, for instance, facilitate real-time communication and information dissemination on a global scale. Online communities provide spaces for like-minded individuals to connect, share experiences, and collaborate on common interests. Collaborative tools and platforms enable collective problem-solving and innovation, breaking down geographical barriers.
However, social computing also raises important considerations related to privacy, security, and the ethical use of data. The analysis of vast amounts of user-generated content poses challenges and opportunities for understanding human behavior, preferences, and trends.
In essence, social computing represents a paradigm shift in how we perceive and engage with information and each other in the digital age. It continues toshape and redefine our social landscape, influencing the way we communicate, collaborate, and build communities in an increasingly interconnected world.",1,,,1,,,
19,Web2.0 and its tools,"Web 2.0 represents a significant evolution in the landscape of the World Wide Web, marking a transition from static, one-way information delivery to a dynamic, interactive, and collaborative online experience. Coined in the early 2000s, Web 2.0 embodies a shift towards user-generated content, social collaboration, and the seamless sharing of information across the internet.One of the key features of Web 2.0 is the emphasis on user participation and contribution. Unlike the earlier, more passive forms of the web, Web 2.0 platforms encourage users to create, share, and collaborate on content. Social media platforms like Facebook, Twitter, and Instagram exemplify this participatory culture, allowing users to share their thoughts, images, and experiences with a global audience.Blogs and wikis are other manifestations of Web 2.0, where individuals can easily publish their ideas or collaborate on content creation. Wikipedia, a user-edited encyclopedia, is a prime example of the collaborative nature of Web 2.0, demonstrating how collective intelligence can contribute to the creation of a vast knowledge repository.Web 2.0 tools also include social bookmarking sites like Delicious, which enable users to organize, share, and discover web content based on their interests. Additionally, content sharing platforms such as YouTube and Flickr empower users to upload, share, and engage with multimedia content.The rise of Web 2.0 has been instrumental in fostering online communities, connecting people with shared interests and allowing them to engage in discussions, forums, and virtual spaces. The technologies underpinning Web 2.0 have paved the way for a more interconnected, interactive, and collaborative digital landscape, influencing how individuals and businesses communicate, share information, and build online relationships.",1,,,1,,,
20,Computer Networks,"Computer networks form the backbone of modern communication, providing the infrastructure that allows devices and systems to connect and share information seamlessly. At its core, a computer network is a collection of interconnected devices, such as computers, servers, routers, and switches, that are linked to facilitate communication and resource-sharing.These networks can take various forms, from local area networks (LANs) within a single building or campus to wide area networks (WANs) that span across cities, countries, or even continents. The internet itself is the most extensive and globally interconnected computer network, enabling communication and data exchange on a global scale.The fundamental purpose of computer networks is to enable the efficient and reliable transfer of data between connected devices. This data can take various forms, including text, images, videos, and more. To achieve this, computer networks use various communication protocols and technologies, such as Transmission Control Protocol (TCP) and Internet Protocol (IP), which form the basis of the internet's communication architecture.Networking devices like routers and switches play crucial roles in directing and managing the flow of data within a network. Routers, for instance, determine the best path for data to travel from the source to the destination across multiple interconnected networks.Security is a paramount concern in computer networks, given the potential vulnerabilities and threats. Protocols like Secure Sockets Layer (SSL) and Virtual Private Networks (VPNs) are employed to encrypt data, ensuring confidentiality and integrity during transmission.Wireless networks, utilizing technologies like Wi-Fi, have become pervasive, allowing devices to connect without physical cables. This has contributed to the proliferation of mobile devices and the concept of the Internet of Things (IoT), where various physical objects are embedded with sensors and connected to the internet.",1,,,,1,,
21,Types of Computer Networks,"Computer networks come in various types, each serving specific purposes and catering to different scales of connectivity. One prevalent categorization includes Local Area Networks (LANs), Wide Area Networks (WANs), and Metropolitan Area Networks (MANs).
Local Area Networks (LANs) are confined to a small geographic area, typically within a single building or campus. They facilitate high-speed communication between devices, such as computers and printers, allowing for efficient resource sharing and collaboration. LANs commonly use Ethernet or Wi-Fi technologies.
Wide Area Networks (WANs) cover a larger geographical area, connecting LANs that may be located in different cities or even countries. The internet itself is a vast WAN, enabling global communication. WANs utilize various technologies, including leased lines, satellite links, and virtual private networks (VPNs), to connect distant locations.
Metropolitan Area Networks (MANs) fall between LANs and WANs in terms of geographic coverage. They span a city or a large campus and are designed to provide high-speed connectivity within that metropolitan area. MANs are useful for connecting multiple LANs across a city and are commonly used by service providers to offer broadband services. Another network is Enterprise network, an enterprise network is a large-scale computer network that connects various devices and systems within an organization or business. It is designed to facilitate communication, data sharing, and resource access among different departments and personnel. often include Local Area Networks (LANs), Wide Area Networks (WANs), and other interconnected components to support the organization's overall operations.",1,,,,1,,
22,Transaction Processing Systems,"A Transaction Processing System (TPS) is a type of information system designed to manage and process day-to-day transactional data within an organization. It plays a critical role in capturing, storing, processing, and retrieving transactional information generated by business operations. TPS is fundamental for supporting routine activities such as order processing, inventory management, payroll, and other essential functions.The primary goal of a TPS is to ensure the accuracy, consistency, and reliability of transactional data. It typically involves the systematic recording of transactions in a real-time or near-real-time manner, ensuring that data remains up-to-date. TPS is characterized by its high transaction volume, speed, and reliability.Key features of TPS include data integrity, where the system ensures that transactions are processed accurately and completely, and atomicity, ensuring that transactions are treated as indivisible units. TPS is widely used in various industries to streamline operational processes, reduce manual errors, and provide a solid foundation for decision-making based on real-time data.",1,,,,,1,
23,Functional Area Information System,"A Functional Area Information System (FAIS) is a type of information system that focuses on supporting specific business functions or departments within an organization. Unlike enterprise-wide systems, FAIS is designed to cater to the unique needs and requirements of individual functional areas, such as finance, human resources, marketing, and operations.Each functional area within an organization has its own set of tasks, processes, and data needs. A FAIS is tailored to address these specific requirements, providing tools and resources to enhance efficiency and decision-making within each department. For example, a Finance Information System might include modules for accounting, budgeting, and financial reporting, while a Human Resources Information System could handle employee records, payroll, and benefits administration.FAIS aims to improve collaboration, streamline processes, and provide accurate and timely information to support the operations of a particular business area. By aligning with the unique needs of each function, these systems contribute to overall organizational effectiveness. FAIS is an integral part of an organization's overall information system architecture, working in conjunction with other systems to create a comprehensive and cohesive IT infrastructure.",1,,,,,1,
24,What is Tagging,"​A Tag is a keyword which describes a piece of information, for  example, a blog, a picture, an article, or a video clip.​
Tagging  allows users  to  place  information  in multiple, overlapping  associations rather than in rigid categories.​
Tagging is the basis of folksonomies, which are user-generated​classifications that use tags to categorize and retrieve Web pages,  photos, videos, and other Web content.​One specific form of tagging, known as geotagging, where Google  Maps allows users to add pictures and information.​User experience is enriched because they can see pictures of attractions, reviews, and things to do, posted by everyone, and all related to the map location they are viewing.",1,,,1,,,
25,Really Simple Syndication(RSS),"It is a Web 2.0 feature that allows user to receive the customized  information, when required , without having to surf thousands of Web  sites.​

RSS allows anyone to syndicate (publish) his or her blog, or any other  content, to anyone who has an interest in subscribing to it.​

To use RSS, user can utilize a special newsreader that displays RSS  content feeds from the Web sites user selects.",1,,,1,,,
26,Blogs,"Blogs often provide incredibly useful information, often before the  information becomes available in traditional media outlets.​Perhaps the primary value of blogs is their ability to bring current,  breaking news to the public in the fastest time possible.​Unfortunately, in doing so, bloggers sometimes cut corners, and their  blogs can be inaccurate.​However, blogs have transformed the ways in which people gather​and consume information.",1,,,1,,,
27,Wikis,"Organizations use wikis in several ways. In project management, for  example, wikis provide a central repository for capturing constantly  updated product features and specifications, tracking issues, resolving  problems, and maintaining project histories.​
Wikis enable companies to collaborate with customers, suppliers, and  other business partners on projects.​
Wikis are also valuable in knowledge management. For example,  companies use wikis to keep enterprisewide documents, such as  guidelines and frequently asked questions, accurate and current.",1,,,1,,,
28,Microblogging,"It is a form of blogging that allows users to write short messages (or  capture an image or embedded video) and publish them.​
Can be submitted via text messaging from mobile phones, instant  messaging, e-mail, or simply over the Web.​
The content of a microblog differs from that of a blog because of the  limited space per message (usually up to 140 characters).​
A popular microblogging service is Twitter.​
Twitter is becoming a very useful business tool.​
Businesses also use Twitter to gather real-time market intelligence and  customer feedback.",1,,,1,,,
29,Social Networking Websites,"A social network is a social structure composed of individuals,  groups, or organizations linked by values, visions, ideas, financial  exchange, friendship, kinship, conflict, or trade.​
It refers to activities performed using social software tools (e.g.,  blogging) or social networking features (e.g., media sharing).​
It allows convenient connections to those of similar interest.​
It can be described as a map of all relevant links or connections​
among the network’s members.​
It can also be used to determine the social capital of individual  participants. Social capital refers to the number of connections a  person has within and between social networks.",1,,,1,,,
30,Categories of Social Networking Web Sites​,"Social news: Focused on user-posted news stories that are ranked by  popularity based on user voting: Digg, Chime.in, Reddit​
Events: Focused on alerts for relevant events, people you know  nearby, etc.: Eventful, Meetup, Foursquare​
Virtual meeting place: Sites that are essentially three-dimensional  worlds, built and owned by the residents (the users): Second Life",1,,,1,,,
31,Mashups,"A mashup is a Web site that takes different content from a number of  other Web sites and mixes them together to create a new kind of  content.​Google Maps is credited with providing the start for mashups. A user  can take a map from Google, add user data, and then display a map  mashup user Web site, Craigslist developed a dynamic map of all available apartments in theUS (www.housingmaps.com)..​Everyblock.com- integrates content from newspapers, blogs, and  government databases to inform citizens of cities about what is  happening in their neighborhoods.",1,,,1,,,
32,Features of Semantic Web,"Semantic Web – It improves web technologies in order to generate,  share and connect content through search and analysis based on the  ability to understand the meaning of words, rather than on keywords  or numbers.​
Artificial Intelligence - Combining this capability with natural  language processing, in Web 3.0, computers can understand  information like humans in order to provide faster and more relevant  results. They become more intelligent to satisfy the needs of users.​
3D Graphics - is being used extensively in websites and services  Ex: Museum guides, computer games, ecommerce, geospatial  contexts, etc.",1,,,1,,,
33,Fundamentals of Social Computing in Business​,"It refers to the delivery of e-commerce activities and transactions  through social computing.​
It supports social interactions and user contributions, allowing  customers to participate actively in the marketing and selling of  products and services in online marketplaces and communities.​
Individuals can collaborate online, obtain advice from trusted  individuals, and find and purchase goods and services.​
Disney allows people to book tickets on Facebook without leaving the  social network.​
PepsiCo provides a live notification when its customers are close to  physical stores (grocery, restaurants, gas stations) that sell Pepsi  products. The company then uses Foursquare to send them coupons  and discount information.",1,,,1,,,
34,Benefits  of Social Commerce,"Better and faster vendor responses to complaints, because customers  can air their complaints in public (on Twitter, Facebook, YouTube)​
Customers can assist other customers (e.g., in online forums).​
Customers’ expectations can be met more fully and quickly.​
Customers can easily search, link, chat, and buy while staying on a  social network’s page.",1,,,1,,,
35,Benefits to Businesses,"Can test new products and ideas quickly and inexpensively​
Learn a lot about their customers​
Identify problems quickly and alleviate customer anger​
Learn about customers’ experiences via rapid feedback​
Increase sales when customers discuss products positively on social  networking sites​
Create more effective marketing campaigns and brand awareness​
Use low-cost user-generated content, for example,  campaigns​
Obtain free advertising through viral marketing​Identify and reward influential brand advocates",1,,,1,,,
36,"​

Risks of social computing","Information security concerns​
Invasion of privacy​
Violation of intellectual property and copyright​
Employees’ reluctance to participate​
Data leakage of personal information ",1,,,1,,,
37,Collaborative Consumption​,"Having access to goods and services is more important than owning  them.​
Transforming Social, Economic, and Environmental practices.​
Includes Collaborative Production, Crowdfunding, Peer-to-Peer  lending, and others.​
Collaborative consumption is a very old concept.​
On the Web, the peer-to-peer model started with eBay in 1995. Then  Craigslist began in the late 1990s, followed by Zipcar in 2000 and  Airbnb in 2007.",1,,,1,,,
38,Database Approach,"The database approach is a systematic and structured method for efficiently managing digital data. It involves the use of database systems, which are software applications facilitating the creation, maintenance, and retrieval of data. In this approach, data is organized into tables representing entities, with each row corresponding to a record. Relationships between tables establish connections between related data, enhancing data integrity and reducing redundancy. Normalization, a key concept, optimizes database structure by minimizing redundancy and enhancing data consistency. Structured Query Language (SQL) is utilized for data manipulation, allowing users to define, query, and modify data. The principles of data integrity, security, and concurrency control are integral to database systems, ensuring accuracy, protecting against unauthorized access, and managing simultaneous data transactions. The database approach also encompasses scalability, performance optimization, and adherence to various data models, with the relational model being widely adopted. It provides a robust foundation for the storage, retrieval, and management of large datasets in diverse applications, making it a fundamental element in information systems.",1,1,,,,,
39,Database Approach-Data Hierarchy,"Data hierarchy refers to the systematic organization of data in a structured manner, typically arranged in levels of increasing complexity and abstraction. At the base of the hierarchy are raw data, which are individual facts or observations. As we move up the hierarchy, data is grouped into fields, representing specific attributes, and records, which are collections of related fields. The next level is the file, a logical grouping of records sharing a common structure. Databases come next, allowing for the organization and retrieval of data from multiple files. The topmost level often includes data warehouses and data marts, facilitating the analysis of large volumes of integrated data from various sources.
This hierarchical organization enables efficient data management, retrieval, and analysis. Each level of the hierarchy builds upon the one below, providing a structured framework for handling information within information systems. This approach supports data integrity, facilitates data maintenance, and enhances the overall efficiency of data processing in diverse applications. The data hierarchy is fundamental to the design and implementation of information systems, ensuring that data is organized, accessible, and meaningful for decision-making processes.",1,1,,,,,
40,Database Approach-Designing the Database,"Designing a database is a multifaceted process integral to information system development, focused on ensuring efficient organization and retrieval of data. The initial step involves Requirement Analysis, where information needs are identified, including data types, relationships, and user requirements. Following this, the Conceptual Design phase creates a conceptual data model using tools like Entity-Relationship Diagrams. Normalization is crucial for minimizing redundancy and enhancing integrity. Logical Design transforms the conceptual model into a form suitable for a specific Database Management System (DBMS), determining tables, keys, and structures. The Physical Design considers storage structures, file organization, and indexing for optimal performance. Data Integrity and Security measures are implemented, including access control and user permissions. Decisions about Data Storage and Indexing impact efficiency in querying. Continuous Normalization Review ensures an ongoing balance between normalization and retrieval efficiency. Comprehensive documentation, including data models and schema diagrams, is vital. Testing and refinement involve evaluating the design with sample data and making adjustments based on real-world usage. The Implementation phase creates the physical database and loads initial data. Maintenance and Evolution address ongoing performance and changing requirements, ensuring the database aligns with evolving system needs. A well-designed database forms a robust foundation for effective data management and retrieval in an information system.",1,1,,,,,
41,Database Approach-Relational Database Model,"The Relational Database Model is a foundational approach to organizing and managing data in a systematic manner. It is built on the principles of a relational model, which structures data into tables, each consisting of rows and columns. Tables represent entities, and attributes of those entities are stored in the columns. Relationships between tables are established based on common attributes, fostering data integrity and reducing redundancy.Key components of the Relational Database Model include tables, which store data in a structured format, and each table has a primary key that uniquely identifies each record. Foreign keys are used to establish relationships between tables. The model adheres to the principles of normalization, aiming to minimize data redundancy and dependency issues. Normalization involves breaking down tables into smaller, more manageable structures, ensuring data consistency and integrity.SQL (Structured Query Language) is employed to interact with relational databases. It provides a standardized way to define, manipulate, and query data. Queries can be used to retrieve specific information, update records, or perform complex operations across multiple tables.One of the strengths of the Relational Database Model lies in its flexibility and scalability. It accommodates a wide range of data types and supports complex queries, making it suitable for various applications. Additionally, the model provides a clear and understandable structure, facilitating effective data management and analysis.Despite its advantages, the Relational Database Model is not without challenges. Scaling large databases can be resource-intensive, and designing complex relationships may require careful consideration. Nevertheless, the model's widespread use and continued relevance highlight its significance in modern data management systems.",1,1,,,,,
42,Big Data,"Big data refers to the massive volume, variety, and velocity of data generated and collected by organizations on a daily basis. This term encompasses datasets that are too large and complex for traditional data processing applications to handle efficiently. Big data is characterized by the three Vs: volume, representing the sheer size of the data; variety, reflecting the diverse types of data, including structured, semi-structured, and unstructured; and velocity, indicating the speed at which data is generated and processed.The sources of big data are numerous, including social media interactions, sensor data, transaction records, and more. The challenge lies not only in managing the vast amount of data but also in extracting meaningful insights from it. Traditional databases and data processing tools often struggle to handle big data due to its scale and diversity.To address the complexities of big data, specialized technologies and frameworks have emerged. Distributed computing frameworks like Apache Hadoop enable the processing of large datasets across clusters of computers, providing scalability and fault tolerance. Apache Spark, another powerful tool, facilitates real-time data processing and analytics. NoSQL databases, such as MongoDB and Cassandra, are designed to handle unstructured and semi-structured data more efficiently than traditional relational databases.The significance of big data lies in its potential to uncover valuable insights, patterns, and trends that can inform decision-making processes. Organizations across various industries leverage big data analytics to gain a competitive edge, improve operational efficiency, and enhance customer experiences. The evolving field of data science plays a crucial role in extracting actionable intelligence from big data, utilizing techniques like machine learning and predictive analytics.",1,1,,,,,
43,Characteristics of Big Data,"The characteristics of big data are defined by the three Vs: volume, variety, and velocity. First, volume refers to the sheer scale of data generated and collected by organizations, often exceeding the capacity of traditional data processing systems. The massive volume of data requires scalable and distributed computing frameworks to handle it effectively.
Second, variety signifies the diverse types of data that make up big data. This includes structured data (like traditional databases), semi-structured data (such as XML or JSON files), and unstructured data (like text documents, social media posts, and multimedia content). Dealing with this variety demands flexible data storage and processing solutions capable of handling different formats.
The third characteristic, velocity, refers to the speed at which data is generated, processed, and analyzed. In today's fast-paced digital environment, data is produced in real-time or near-real-time, requiring systems that can quickly capture, process, and derive insights from streaming data. This velocity is crucial for organizations aiming to make timely and informed decisions based on the latest information.
In addition to the three primary characteristics, big data is often expanded to include other Vs, such as veracity (the accuracy and reliability of the data), volatility (the temporal nature of data), and value (the importance and usefulness of the insights derived from the data). These additional dimensions further highlight the complexity of managing and extracting meaningful information from large and diverse datasets.",1,1,,,,,
44,Benifits of Big Data,"Big data offers a myriad of benefits to organizations across various industries. One key advantage is the ability to gain valuable insights from vast and diverse datasets. By analyzing large volumes of data, organizations can uncover patterns, trends, and correlations that provide a deeper understanding of customer behavior, market dynamics, and operational processes. This, in turn, facilitates data-driven decision-making, leading to more informed and strategic choices.
Another significant benefit is improved operational efficiency. Big data technologies enable organizations to process and manage large datasets more efficiently, reducing the time and resources required for tasks like data storage, retrieval, and analysis. This efficiency translates into cost savings and allows for a more streamlined and agile operation.
Big data also plays a crucial role in enhancing customer experiences. Through advanced analytics, organizations can personalize their interactions with customers, offering tailored products, services, and recommendations. This not only strengthens customer satisfaction but also contributes to increased customer loyalty and retention.
Furthermore, big data analytics empowers organizations to proactively identify and address potential issues or risks. By monitoring and analyzing data in real-time, businesses can detect anomalies, security threats, or operational challenges early on, allowing for prompt intervention and mitigation.
In the realm of research and development, big data facilitates innovation. The analysis of large datasets can uncover new opportunities, support research initiatives, and drive innovation in products and services. This innovation, fueled by insights derived from big data, can give organizations a competitive edge in the market.
In summary, the benefits of big data encompass improved decision-making, enhanced operational efficiency, personalized customer experiences, proactive issue detection, and support for innovation and research. As organizations continue to harness the power of big data, these advantages become integral to achieving success in today's data-driven landscape.",1,1,,,,,
45,Issues with Big Data,"While big data offers numerous advantages, it also presents several challenges and issues that organizations must address to fully harness its potential. One major concern is the sheer volume of data generated, often referred to as the ""data deluge."" Managing and processing such massive amounts of information can strain existing IT infrastructures and lead to increased storage and computational demands.
Data security and privacy are critical issues associated with big data. With the vast amount of sensitive information being collected and analyzed, there is a heightened risk of data breaches, unauthorized access, and privacy infringements. Ensuring robust security measures and compliance with data protection regulations is essential to mitigate these risks.
The complexity of integrating diverse data sources is another challenge. Big data often comes from various platforms, formats, and systems. Integrating this heterogeneous data into a cohesive and meaningful structure requires advanced data integration and interoperability solutions. Inconsistencies and discrepancies in data quality can also hinder accurate analysis and decision-making.
Maintaining data quality and accuracy is a persistent concern in the realm of big data. Large datasets may contain errors, duplications, or outdated information, impacting the reliability of analytical outcomes. Implementing effective data governance practices, quality assurance protocols, and data cleaning processes is crucial for ensuring the integrity of big data analytics.
Scalability poses a significant challenge, especially as data volumes continue to grow. Organizations must design their big data infrastructure to scale seamlessly, accommodating increasing data loads without compromising performance. This requires a strategic approach to hardware, software, and architecture to meet evolving business needs.
Furthermore, the shortage of skilled professionals in the field of big data analytics is a pressing issue. The demand for data scientists, analysts, and engineers far exceeds the available talent pool, creating a skills gap. Organizations must invest in training and development initiatives to build a workforce capable of leveraging the full potential of big data.
In conclusion, addressing issues related to the volume, security, integration, quality, scalability, and skills associated with big data is crucial for organizations seeking to unlock the transformative power of large-scale data analytics. Proactive strategies and robust solutions are essential to navigate these challenges successfully.",1,1,,,,,
46,Business Intelligence ,"Business Intelligence (BI) is a comprehensive framework that encompasses technologies, processes, and tools for gathering, storing, analyzing, and presenting business data to support informed decision-making. The primary objective of BI is to convert raw data into actionable insights, aiding organizations in strategic planning and operational improvements. BI systems aggregate data from various sources, transforming it into meaningful reports, dashboards, and visualizations. These tools enable stakeholders to interpret complex data sets, identify trends, and make data-driven decisions. Business Intelligence plays a pivotal role in enhancing organizational agility, fostering a data-driven culture, and gaining a competitive edge in the dynamic business landscape. By providing real-time analytics, forecasting capabilities, and performance metrics, BI empowers businesses to adapt swiftly to market changes and optimize their operations for sustained success.",1,1,,,,,
47,Datawarehouse,"A data warehouse is a centralized repository that systematically gathers, integrates, and manages large volumes of data from various sources within an organization. It is designed to support business intelligence and analytics activities by providing a unified and historical view of data. Unlike transactional databases, data warehouses are optimized for query and analysis rather than transaction processing.
In a data warehouse, data from disparate sources, such as operational databases, spreadsheets, and external systems, are extracted, transformed, and loaded (ETL) into a structured format for analytical purposes. This transformation process involves cleaning, aggregating, and organizing data to ensure consistency and relevance.",1,1,,,,,
48,Features of Datawarehouse,"The main features of a data warehouse include:
Subject-Oriented: Data is organized around specific subjects or business areas, allowing for a focused analysis of particular aspects of the organization.
Integrated: Data from different sources is integrated to provide a consolidated view, resolving inconsistencies and discrepancies.
Time-Variant: The data warehouse includes historical data, enabling the analysis of trends and changes over time.
Non-Volatile: Once data is stored in the warehouse, it remains unchanged and serves as a stable source for analysis.
Data warehouses play a crucial role in supporting decision-making processes by providing a reliable and efficient platform for business intelligence, reporting, and data analytics. They are essential for organizations seeking to extract valuable insights from their data to drive strategic initiatives and improve overall performance.",1,1,,,,,
49,Characteristics of Datawarehouse,"The characteristics of a data warehouse are defined by key features that distinguish it from other types of databases. These characteristics are essential for supporting effective business intelligence and analytics. Here are the main characteristics of a data warehouse:
1. Subject-Oriented: Focus on Business Subjects: Data warehouses are organized around specific business subjects, such as sales, marketing, or finance. This subject-oriented approach facilitates targeted analysis and reporting.
2. Integrated:Data Integration: Data from various sources, including operational databases, spreadsheets, and external systems, is integrated into a unified view. This integration resolves inconsistencies and provides a cohesive and accurate representation of the data.
3. Time-Variant:Historical Data: Data warehouses maintain historical information, allowing users to analyze trends, changes, and performance over time. This time-variant characteristic supports trend analysis and decision-making based on historical context.
4. Non-Volatile:Stability of Stored Data: Once data is loaded into the data warehouse, it becomes non-volatile. This means that historical records are preserved and do not change, providing a stable and reliable source for analysis.
5. Scalability:Support for Growth: Data warehouses are designed to handle large volumes of data efficiently. They can scale to accommodate increasing data storage and analytical processing requirements as an organization's data needs grow.
6. Optimized for Query and Analysis:Query Performance: The structure and indexing of data in a data warehouse are optimized for query performance and analytical processing. This ensures that complex queries and reports can be executed efficiently.
7. Decision Support:Business Intelligence Focus: Data warehouses are primarily built to support decision-making processes through business intelligence and analytics. They provide a platform for users to extract insights and make informed decisions.
Understanding and leveraging these characteristics are crucial for organizations looking to harness the full potential of their data for strategic decision-making and business insights.",1,1,,,,,
50,Datamart,"A data mart is a specialized subset of a data warehouse that is designed to serve the data needs of a specific business unit, department, or functional area within an organization. Unlike a comprehensive data warehouse, which covers a wide range of business subjects and processes, a data mart is focused on providing targeted and domain-specific information for a particular group of users.",1,1,,,,,
51,Datamart-Characteristics,"A data mart is a specialized subset of a data warehouse that is designed to serve the data needs of a specific business unit, department, or functional area within an organization. Unlike a comprehensive data warehouse, which covers a wide range of business subjects and processes, a data mart is focused on providing targeted and domain-specific information for a particular group of users.
The key characteristics of a data mart include:
Focused Scope: A data mart is designed to address the specific needs of a particular business segment. It focuses on a subset of data that is relevant to a specific group of users, such as marketing, finance, or sales.
Subset of Data Warehouse: While a data warehouse contains integrated and consolidated data from various sources, a data mart is a smaller and more specialized component derived from the larger data warehouse. It represents a tailored view of data for a specific business area.
Improved Accessibility: Data marts are created to enhance accessibility for end-users by providing a more streamlined and user-friendly interface. This makes it easier for individuals within a department to access the information they need without navigating through the entire data warehouse.
Quicker Implementation: Compared to building a comprehensive data warehouse, creating a data mart is generally quicker and requires fewer resources. This agility allows organizations to respond rapidly to the specific data requirements of individual departments.
Domain-Specific Data Models: Data marts often utilize domain-specific data models that align closely with the business processes and terminology of the targeted department. This customization enhances the relevance and usability of the data for the end-users.
Enhanced Performance: Since data marts store a subset of data relevant to a specific business area, queries and reports generated within the data mart can often be executed more quickly and efficiently than in a larger data warehouse.
Autonomy for Business Units: Data marts provide a level of autonomy for different business units or departments. They can tailor the data to meet their unique requirements, fostering a sense of ownership and ensuring that the information aligns with their specific business objectives.
In summary, a data mart is a focused and specialized component of a data warehouse, catering to the distinct data needs of specific business units or functional areas within an organization. It plays a crucial role in providing targeted and actionable insights to support decision-making processes at the departmental level.",1,1,,,,,
52,Datamart-Features ,"A data mart possesses distinctive features that contribute to its effectiveness in meeting the specific information needs of a particular business unit or functional area within an organization. One key feature is its focused scope, as a data mart concentrates on a subset of data that is relevant to the targeted users, offering a tailored view of information. Unlike a comprehensive data warehouse, a data mart allows for quicker implementation and improved accessibility for end-users by providing a more streamlined interface.
Data marts are designed with domain-specific data models, aligning closely with the business processes and terminology of the specific department. This customization enhances the relevance and usability of the data, contributing to the autonomy of business units. The autonomy is further facilitated by the enhanced performance of data marts, as they store a subset of data relevant to a specific business area, resulting in quicker query execution.
Additionally, the creation of data marts requires fewer resources compared to building an entire data warehouse. This agility enables organizations to respond rapidly to the unique data requirements of individual departments, fostering a sense of ownership among users. In summary, the features of a data mart include focused scope, improved accessibility, quicker implementation, domain-specific data models, enhanced performance, and autonomy for business units, collectively making it a valuable component in fulfilling departmental data needs.",1,1,,,,,
53,Architecture of a datawarehouse and datamart in an organization ,"The architecture of a data warehouse and data mart in an organization is a sophisticated framework designed to handle the entire data lifecycle, from extraction to loading, providing a structured environment for efficient data analysis and reporting. The process involves sequential stages: data extraction, data cleaning, data transformation, data loading, and periodic refreshing.Data extraction is the initial step, where data is gathered from various sources, including operational systems, external databases, and other repositories. This raw data is then moved to the data warehouse for further processing.Subsequently, the data cleaning phase ensues, aiming to rectify inconsistencies, errors, or discrepancies in the extracted data. Cleaning involves tasks such as handling missing values, standardizing formats, and eliminating duplicate entries, ensuring that the data is accurate and reliable.Following data cleaning, the information undergoes data transformation. This stage involves restructuring the data to adhere to a standardized format suitable for analytical processing. Transformation includes activities like aggregation, normalization, and the creation of derived attributes, enhancing the overall quality and usefulness of the data.Once transformed, the data is loaded into the data warehouse or data mart, forming the storage layer. Loading involves populating the database tables with the cleaned and transformed data, organizing it based on a dimensional model for optimal querying and reporting.Periodic refreshing is a crucial aspect of the architecture, ensuring that the data warehouse and data mart stay up-to-date. Regular updates, typically scheduled at predefined intervals, involve repeating the extraction, cleaning, transformation, and loading processes to incorporate the latest data.This architecture allows organizations to maintain a centralized repository of clean and transformed data, ready for analysis. It facilitates efficient querying, reporting, and business intelligence activities, providing stakeholders with valuable insights for informed decision-making. The cyclical nature of the process ensures that the data remains relevant and reflects the current state of the organization's operations.",1,1,,,,,
54,Different types of datawarehouse application,"Data warehousing applications serve diverse purposes in catering to the analytical needs of organizations. One prevalent type is the Enterprise Data Warehouse (EDW), designed to support the entire enterprise and integrate data from various sources, providing a comprehensive view for strategic decision-making. EDWs often utilize a centralized architecture.
Another type is the Data Mart, a smaller and more focused subset of an enterprise data warehouse, concentrating on specific business functions or departments. Data marts are more agile, catering to the analytical requirements of particular user groups within the organization.
In addition, organizations may deploy Operational Data Stores (ODS), acting as a temporary storage layer between operational systems and data warehouses. ODS facilitates real-time data integration, enabling organizations to make swift operational decisions.
Further, Analytical Data Processing (ADP) systems leverage data warehousing capabilities to enhance analytical processing. These systems facilitate complex analyses, such as data mining, trend analysis, and predictive modeling, empowering organizations to extract valuable insights from their data.
Moreover, the concept of Real-Time Data Warehousing has gained prominence, allowing organizations to process and analyze data as it arrives, enabling quicker decision-making based on the most recent information.
Each type of data warehousing application serves distinct purposes, aligning with organizational needs and objectives. The flexibility and scalability of these applications empower businesses to adapt to evolving analytical requirements and gain valuable insights from their data.",1,1,,,,,
55,Benifits of Data Warehousing ,"Data warehousing offers a myriad of advantages to organizations aiming to harness the full potential of their data for informed decision-making. Firstly, data warehousing facilitates efficient data integration by consolidating information from various sources into a centralized repository. This unified view of data enables organizations to gain comprehensive insights into their operations, customers, and market trends.
Secondly, data warehousing enhances data quality and consistency. Through data cleaning and transformation processes, inconsistencies and errors are rectified, ensuring that the stored data is accurate and reliable. 
Additionally, data warehousing supports complex analytical processing. With the ability to handle large volumes of data, organizations can conduct in-depth analyses, uncover patterns, and derive meaningful correlations. 
Moreover, data warehousing improves query performance and response times. By pre-aggregating and indexing data, queries are executed more swiftly, providing users with rapid access to the information they need. 
Furthermore, data warehousing fosters a user-friendly environment for reporting and visualization. 
In terms of scalability, data warehousing systems can adapt to the growing volume of data generated by organizations. This scalability ensures that the data warehouse remains a valuable asset as business data expands over time.
Overall, the benefits of data warehousing extend from improved data quality and analytical capabilities to enhanced query performance and scalability. By investing in a data warehouse, organizations can establish a solid foundation for data-driven decision-making and gain a competitive edge in today's dynamic business landscape.",1,1,,,,,
56,Drawbacks of Data warehousing,"Despite its numerous advantages, data warehousing is not without its challenges and drawbacks. One significant limitation is the considerable investment required in terms of time, resources, and finances. The initial setup and implementation of a data warehouse demand substantial investment in hardware, software, and skilled personnel.
Another drawback is the complexity associated with data integration. Consolidating data from diverse sources often involves intricate mapping and transformation processes. Ensuring consistency and accuracy during this integration can be challenging, leading to potential errors and discrepancies in the warehouse.
Data warehousing projects are also notorious for their lengthy development cycles. Building a comprehensive and effective data warehouse involves multiple phases, including requirements gathering, data modeling, ETL (extract, transform, load) processes, and testing. The extended duration of these projects can result in delays in realizing the expected benefits.
Maintenance and evolution of a data warehouse pose ongoing challenges. As business requirements evolve, the data warehouse needs to adapt accordingly. This requires continuous efforts for updates, expansions, and adjustments to meet changing business needs. Failure to keep the data warehouse aligned with evolving requirements may lead to its obsolescence.
Scalability is another concern. While data warehouses are designed to handle large volumes of data, rapid business growth or a sudden surge in data can strain the system. Ensuring scalability to accommodate future data growth becomes a critical consideration.
Data security and privacy concerns also surround data warehousing. As a centralized repository, a data warehouse becomes a lucrative target for security breaches. Protecting sensitive information and ensuring compliance with data privacy regulations are ongoing challenges.
Lastly, user adoption and training are essential but challenging aspects. Implementing a data warehouse necessitates training users to effectively leverage the system. Resistance to change or insufficient training can hinder the realization of the full potential of the data warehouse.",1,1,,,,,
57,Knowledge Management Systems,"Knowledge Management Systems (KMS) refer to integrated software and processes designed to capture, organize, store, and retrieve an organization's collective knowledge. These systems aim to facilitate the creation, sharing, and utilization of knowledge among employees, ultimately enhancing decision-making, problem-solving, and overall organizational performance. KMS typically include tools for content management, collaboration, and knowledge retrieval, creating a structured framework for managing both explicit and tacit knowledge within an organization. The goal is to create a dynamic and accessible repository of information that contributes to organizational learning and innovation.",1,1,,,,,
58,Factors leading to the development of Knowledge Management Systems,"The development of Knowledge Management Systems (KMS) is driven by several factors that recognize the critical importance of managing organizational knowledge. First and foremost, the increasing complexity of business operations and the rapid pace of technological advancements make it imperative for organizations to effectively capture, store, and disseminate knowledge. The global nature of many businesses, along with the rise of remote work, further underscores the need for systems that enable seamless knowledge sharing across geographic boundaries.The recognition of knowledge as a valuable strategic asset has prompted organizations to invest in KMS. The ability to leverage both explicit and tacit knowledge can lead to enhanced innovation, better decision-making, and improved problem-solving capabilities. Additionally, the competitive landscape, marked by rapidly changing markets and evolving customer expectations, necessitates a proactive approach to knowledge management for staying ahead in the industry.Collaboration and knowledge sharing among employees have become critical for fostering a learning culture within organizations. KMS serves as a catalyst for this cultural shift by providing tools and platforms that facilitate communication and knowledge exchange. Furthermore, compliance requirements and the need for efficient training and development programs contribute to the growing demand for robust Knowledge Management Systems.In summary, the development of KMS is propelled by the need to navigate the complexities of the modern business environment, capitalize on organizational knowledge assets, foster innovation, and maintain a competitive edge in an ever-evolving market.",1,1,,,,,
59,KMS Cycle ,"The Knowledge Management Systems (KMS) cycle, often articulated through the stages of Knowledge, Create, Capture, Refine, Store, Manage, and Disseminate, embodies a comprehensive framework for effective knowledge handling within an organization. At its core, the cycle begins with the identification and definition of valuable knowledge, spanning insights, expertise, and relevant information. The subsequent creation phase involves the generation of new knowledge or the transformation of tacit knowledge into explicit formsFollowing creation, the Capture phase comes into play, wherein the newly formed knowledge is systematically collected and converted into tangible assets. Refinement marks the next stage, emphasizing the need to organize, structure, and enhance the quality of captured knowledge. The Storage phase is integral, providing a secure and accessible repository for the organized knowledge, ensuring its availability for future use.Effective Management is essential throughout the cycle, involving the oversight of knowledge resources, updating information, and aligning the stored knowledge with organizational goals. Dissemination, the final phase, revolves around making the refined and managed knowledge accessible to relevant stakeholders, promoting widespread sharing and application.This cyclical process is not a linear progression but rather an iterative and continuous loop. As knowledge is disseminated and applied, feedback and insights are garnered, initiating refinements to the existing knowledge base. This iterative nature ensures that the knowledge within the system remains dynamic, adaptive, and aligned with the evolving needs of the organization, fostering a culture of continuous learning and improvement. The KMS cycle, thus, serves as a strategic guide for organizations seeking to harness, refine, and leverage their collective knowledge for sustained growth and innovation.",1,1,,,,,
60,Examples of Knowledge Management Systems,"Examples of Knowledge Management Systems (KMS) include platforms like Microsoft SharePoint, which enables organizations to create, share, and manage content and applications for seamless collaboration. Another example is Confluence by Atlassian, offering a collaborative space for creating and sharing documents, ideas, and project plans. Salesforce Knowledge provides a KMS solution integrated with customer relationship management (CRM) tools, allowing businesses to centralize and share information.
Moreover, Intranet-based systems like Jive or Enterprise Social Networks such as Yammer serve as KMS by facilitating real-time communication, knowledge sharing, and collaboration among employees. Additionally, Open Source solutions like MediaWiki, the platform behind Wikipedia, showcase the effectiveness of KMS in enabling collaborative content creation and curation within a large user community. These examples demonstrate how KMS can vary in scope and functionality while ultimately serving the purpose of capturing, organizing, and disseminating knowledge within an organization.",1,1,,,,,
61,Managers and Management ,"Management is the process of planning, organizing, directing, and controlling resources to achieve organizational goals effectively and efficiently. It involves coordinating the efforts of people to attain specific objectives and ensuring that resources are used wisely.
Managers are individuals responsible for overseeing and coordinating the activities of an organization to achieve its goals. They plan, organize, lead, and control resources, making strategic decisions and ensuring the effective implementation of plans. Managers operate at different levels within an organization, such as top-level executives, middle managers, and frontline supervisors, each with specific responsibilities in the management hierarchy.",1,1,,,,,
62,Decision and Decision Making process,"A decision is a choice made between alternative courses of action in a situation of uncertainty. It involves selecting a particular course of action from multiple available optionsThe decision-making process is the systematic approach taken to make choices or reach conclusions. It typically involves the following steps:
1. Identifying the Problem or Opportunity: Clearly understanding the issue that requires a decision or recognizing an opportunity for improvement.
2. Gathering Information: Collecting relevant data and information to understand the context and implications of the decision.
3. Identifying Alternatives: Generating possible solutions or courses of action that could address the problem or opportunity.
4. Evaluating Alternatives: Assessing the pros and cons of each alternative based on criteria such as feasibility, effectiveness, and potential outcomes.
5. Making the Decision: Selecting the best alternative or course of action based on the evaluation.
6. Implementing the Decision: Putting the chosen decision into action.
7. Monitoring and Evaluating: Assessing the outcomes of the decision to ensure it is achieving the desired results and making adjustments if necessary.
The decision-making process is crucial in personal and organizational contexts, guiding individuals and groups in choosing the most appropriate actions to achieve their goals.",1,1,,,,,
63,Impact of Business Intelligence on decision making,"Business Intelligence (BI) significantly impacts decision-making processes within organizations. It involves the use of advanced technologies and tools to collect, process, and analyze large sets of data, providing valuable insights to support strategic decision-making. BI enables decision-makers to access accurate and real-time information, allowing them to make informed choices that drive business success. By transforming raw data into meaningful insights, BI facilitates a deeper understanding of market trends, customer behavior, and internal operations.In essence, BI enhances decision-making by providing a comprehensive view of the business landscape. It allows decision-makers to identify patterns, forecast future trends, and evaluate the impact of various scenarios. BI tools enable the creation of intuitive dashboards and reports, presenting information in a visually compelling manner. This accessibility and visualization of data empower decision-makers at all levels of an organization to make timely and well-informed choices.Furthermore, BI fosters a data-driven culture within organizations, where decisions are based on evidence rather than intuition alone. The continuous monitoring and analysis of key performance indicators (KPIs) enable organizations to adapt quickly to changing circumstances and seize new opportunities. Ultimately, the integration of BI into decision-making processes contributes to increased efficiency, agility, and competitiveness in today's dynamic business environment.",1,1,,,,,
64,Applications of Business Intelligence,"Business Intelligence (BI) finds applications across various domains, revolutionizing the way organizations gather, analyze, and leverage data for strategic decision-making. In the realm of sales and marketing, BI tools assist in identifying customer preferences, analyzing market trends, and optimizing promotional strategies. By providing insights into consumer behavior and preferences, BI enhances targeted marketing efforts, ultimately driving revenue growth.
In finance, BI facilitates real-time monitoring of financial metrics, risk assessment, and performance analysis. It aids financial professionals in making informed investment decisions, managing budgets effectively, and ensuring compliance with regulatory requirements. Additionally, BI applications in supply chain management optimize inventory levels, streamline logistics, and enhance overall operational efficiency.
Human resources benefit from BI by enabling data-driven workforce management. BI tools help HR professionals analyze employee performance, assess training needs, and streamline recruitment processes. This leads to more effective talent management and organizational development.
Furthermore, BI plays a crucial role in enhancing customer service. By analyzing customer interactions and feedback, organizations can identify areas for improvement, personalize services, and address issues proactively. This results in improved customer satisfaction and loyalty.
Overall, the applications of BI are diverse, spanning across functions such as operations, logistics, finance, marketing, and human resources. The integration of BI into these areas empowers organizations to make data-driven decisions, gain a competitive edge, and adapt to the ever-changing business landscape.",1,1,,,,,
65,Business Intelligence Applications for Data analysis,"Business Intelligence (BI) applications for data analysis encompass various sophisticated components such as Online Analytical Processing (OLAP), Data Mining, and Decision Support Systems (DSS). OLAP serves as a powerful tool for multidimensional analysis, enabling users to explore and analyze complex datasets from multiple perspectives. It facilitates interactive querying, drilling down into specific data points, and gaining insights into various dimensions of business information.
Data Mining, another integral aspect of BI applications, involves extracting patterns and valuable knowledge from large datasets. It employs advanced statistical and machine learning techniques to identify trends, correlations, and anomalies within the data. This process aids organizations in making informed decisions based on a comprehensive understanding of their data.
Decision Support Systems (DSS) form the backbone of BI applications, providing a framework for managerial decision-making. DSS integrates data analysis tools, models, and information retrieval capabilities to assist decision-makers in evaluating alternatives, exploring scenarios, and selecting optimal courses of action. It acts as a facilitator for strategic planning and enhances the overall decision-making process.",1,1,,,,,
66,Data Mining ,"Data Mining involves the systematic exploration and analysis of large datasets to discover meaningful patterns, correlations, and trends that may be hidden within the data. It encompasses various techniques from statistics, machine learning, and database management to uncover valuable insights and knowledge. The process typically involves data preprocessing, where raw data is cleaned and transformed, followed by the application of algorithms for pattern discovery.By leveraging algorithms such as clustering, classification, association rule mining, and regression analysis, data mining can reveal complex relationships and dependencies in the data. This, in turn, enables organizations to make informed decisions, predict future trends, and identify potential areas for improvement.Data Mining finds applications across diverse domains, including business, finance, healthcare, and marketing. In business, it aids in customer segmentation, market basket analysis, and fraud detection. In healthcare, it contributes to disease prediction and patient outcome analysis. Overall, Data Mining plays a crucial role in extracting actionable insights from vast datasets, supporting decision-making processes and fostering innovation.",1,1,,,,,
67,Decision Support Systems,"Decision Support Systems (DSS) are computer-based tools designed to assist individuals and organizations in making informed decisions. These systems integrate data, analytical models, and user interfaces to provide decision-makers with comprehensive insights. DSS facilitates the exploration of various scenarios and helps in evaluating possible outcomes based on different parameters.Key components of Decision Support Systems include databases, model management systems, and user interfaces. DSS can support a wide range of decision-making activities, from strategic planning to operational decisions. The systems often utilize advanced analytics, data visualization, and interactive features to enhance the decision-making process.Decision Support Systems (DSS) employ various analytical tools and techniques to enhance decision-making processes. One crucial aspect is the integration of models and data, where mathematical models and real-world data are combined to generate insights. Sensitivity analysis is utilized to assess how changes in input variables impact the output of a model, allowing decision-makers to understand the system's sensitivity to different factors.
What-if analysis is another key component, enabling users to explore different scenarios and assess potential outcomes based on changes in input parameters. This dynamic approach allows decision-makers to evaluate the consequences of various decisions and make informed choices. Goal-seeking analysis focuses on determining the inputs necessary to achieve a specific outcome, helping decision-makers set and attain desired objectives.
Together, these elements contribute to the robust functionality of Decision Support Systems, providing a comprehensive toolkit for decision-makers to navigate complex scenarios and make well-informed choices.",1,1,,,,,
68,Business Intelligence Applications for Presenting results,"Business Intelligence (BI) applications play a pivotal role in presenting results effectively, employing various tools for enhanced visualization and comprehension. Dashboards are interactive interfaces that provide a consolidated view of key performance indicators (KPIs) and other relevant metrics. They offer a real-time snapshot of business data, facilitating quick decision-making.
Data Visualization Technologies, such as Geographic Information Systems (GIS), enable the representation of data geographically. GIS allows businesses to analyze and interpret spatial data, offering valuable insights into geographical patterns and trends. This capability is particularly useful for industries with geographic dependencies, such as logistics or retail.
Reality Mining is a BI application that leverages data from mobile devices and sensors to analyze human behavior. By collecting and interpreting data from daily activities, businesses can gain insights into consumer behavior, employee productivity, and other valuable information.In summary, these BI applications contribute to a comprehensive and visually intuitive presentation of results, empowering businesses to derive actionable insights and make informed decisions.",1,1,,,,,
69,Dashboards in BI ,"Dashboards are a critical component of Business Intelligence (BI) systems, serving as visual interfaces that consolidate and present key performance indicators (KPIs) and relevant data in a comprehensible format. These interactive displays offer a real-time snapshot of an organization's performance, facilitating quick and informed decision-making. Dashboards typically include various widgets, charts, graphs, and other visual elements that allow users to monitor trends, track metrics, and gain insights into different aspects of their business. By providing a centralized and accessible view of critical information, dashboards empower users at various levels within an organization to assess performance, identify patterns, and make data-driven decisions.",1,1,,,,,
70,Data Visualization and its Technologies ,"Data visualization is the graphical representation of data to provide insights and facilitate understanding. It involves the use of various technologies and tools to transform raw data into visual formats like charts, graphs, and interactive dashboards. These visualizations help individuals and organizations comprehend complex datasets, identify patterns, and extract meaningful insights. Technologies employed in data visualization include advanced charting libraries, business intelligence tools, and programming languages such as Python and R. Geographic Information Systems (GIS) and Reality Mining are also integral parts of data visualization, enabling the representation of spatial and location-based information. As a crucial aspect of Business Intelligence, data visualization enhances communication, aids decision-making, and enables more effective storytelling with data.",1,1,,,,,
71,Network Protocols,"Network protocols are sets of rules or standards that govern the communication and interaction between devices within a network. They define the procedures for transmitting and receiving data across networks, ensuring that data can be transferred reliably and efficiently. These protocols encompass various aspects of network communication, including how data is formatted, transmitted, received, acknowledged, and interpreted.
Network protocols operate at different levels within the OSI (Open Systems Interconnection) model or the TCP/IP (Transmission Control Protocol/Internet Protocol) model, each addressing specific functions and responsibilities in the process of transmitting information between devices. These protocols facilitate seamless communication between devices regardless of their make, model, or operating system, enabling interoperability and the exchange of data across diverse network environments.",1,,,,1,,
72,Network Topologies,"Bus Topology: In this topology, all devices are connected to a central cable, known as the bus. Data is transmitted in both directions, but it can be prone to interruptions if the main cable fails.
Star Topology: Devices are connected to a central hub or switch individually. If one connection fails, it doesn't affect the rest of the network, making it reliable.
Ring Topology: Devices are connected in a circular manner, where data travels in one direction. Failure of one device can disrupt the entire network.
Mesh Topology: Every device is interconnected with every other device in the network, offering redundancy and multiple paths for data transmission, ensuring reliability.
Tree Topology: This combines aspects of bus and star topologies. Devices are arranged hierarchically, with groups of star-configured networks connected to a linear bus backbone.",1,,,,1,,
73,Network Components,"Hardware Components:
Network Devices: These include routers, switches, hubs, modems, access points, and network interface cards (NICs). Routers and switches help in directing and managing traffic, while hubs and modems facilitate connections between devices and networks.
Cables and Connectors: Ethernet cables, fiber optic cables, and connectors like RJ45 are used to physically connect devices within a network.
Network Interface Cards (NICs): These are hardware components installed in computers or devices that enable them to connect to a network. They provide the physical interface for data transmission.
Servers: These powerful computers manage network resources and provide services like file storage, email, printing, and access to applications.
Firewalls and Security Appliances: Hardware-based security devices like firewalls, intrusion detection systems (IDS), and VPN concentrators 
help protect networks from unauthorized access and cyber threats.

Software Components:
Operating Systems: Network-capable operating systems like Windows, macOS, Linux, and Unix provide built-in networking functionalities.
Network Protocols: These are sets of rules governing communication between devices. Protocols like TCP/IP, HTTP, FTP, and SMTP facilitate data transfer and communication across networks.
Network Management Software: Tools and applications designed for monitoring, managing, and optimizing network performance. Examples include network monitoring tools, configuration management software, and traffic analysis applications.
Security Software: Antivirus programs, firewalls, intrusion detection/prevention systems, and encryption tools protect networks from cyber threats and unauthorized access.",1,,,,1,,
74,Wired Network,"-Wired network consists of physical connection between two or more devices using physical cables.They are also called as Ethernet networks and mostly used in local area networks (LAN).
- Ethernet is the fastest wired network protocol, with connection speeds of 10 megabits per second (Mbps) to 100 Mbps or higher (now upto 10000Mbps/10 Gbps).
- The most commonly used wired network topologies are bus, ring, star, mesh.
- The benefit of a wired network is that bandwidth is very high and that interference is very limited due to direct connections and hence is safer.
-But the only disadvantage is that they need a lot of rewiring every time they are moved.
- Normally the range of wired networks is somewhere within a 2,000-foot-radius. Beyond these distances the data transmission may become slow or even nonexistent.",1,,,,1,,
75,Types of wired media ,"a) Twisted-Pair Wire
- It consists of strands of copper wires twisted in pairs.
- The twisting of wires reduces noise on the wires by cancelling the electromagnetic interference from the environment during transmission.
- It is a very common form of wiring and used in almost all business telephone wiring.
- These types of wires are widely available and quiet inexpensive but relatively slow in transmission.
b) Coaxial Cable
- Coaxial Cables consist of insulated copper wires.
- There is a cylindrical wire between an insulating sheath. Surrounding the insulating   sheath is a conductive sheath, acting simultaneously as a shield and a return path for the signal.
- Due to this shielding most of the electromagnetic energy remains inside the surrounding conductive sheath and as a result avoids interference of noise.
- These cables are even more noise resistant than twisted pair cables.
- Coaxial cables can carry large amounts of data and therefore used for high speed transmission as in Cable TV network.
(c) Fiber Optics
- Fiber Optics cables consist of numerous thin fibres of glass through which the light pulses are sent to transmit information.
- The fiber-optic cable is surrounded by cladding which is a coating that prevents the light from leaking out of the fiber.
- They are the lightest of all types of wired cables, provide the greatest security from any kind of external interference as well as can transmit large amounts of data.
- They provide extremely fast data transfer rates.",1,,,,1,,
76,Wireless Network,"- In wireless networks, there are no physical wires instead electromagnetic waves are used for transmission of information.
- This increases mobility but surely affects the range of transmission.
- These networks provide anytime, anywhere access to information by devices such as personal computers, smart phones, laptops, iPads etc.
- Wireless networks can be stationary as in microwave towers or they can be mobile as in Mili devices. Wireless technologies enable individuals and organizations to conduct mobile computing, mobile commerce, and pervasive computing.
- The frequency used for wireless communication is from 3 KHz to 900THz.
- Wireless networks are reliable as long as they are not interfered by devices operating at same radio frequencies. Wireless networks can easily be installed without physically destructing the existing wired network.
- Wireless technologies include both wireless devices, such as smart phones, which are small, easily portable and affordable and secondly, wireless transmission media, such as microwave, satellite, and radio.",1,,,,1,,
77,Types of Wireless devices,"- People are finding wireless technologies more convenient to use due to anywhere and anytime access. As a result of which people can make best utilization of their time for work while travelling.
- The wireless devices such as smart phones, laptops, and ipads are easily portable and give flexibility in managing the working hours.
- The devices have achieved high computational capability and all this is available at a reasonably affordable cost. With the help of these devices we can connect to the Internet wirelessly.
- Modern smart phones provide capabilities that include cellular telephony, Bluetooth, Wi-Fi, digital cameras for capturing images and video, global positioning system (GPS), an organizer, a scheduler, an address book, a calculator, access to e-mail, instant messaging, text messaging, music player, a video player, Internet access etc.
- Also there are small portable wireless devices such as MiFi, that provide Wi-Fi hotspot to upto 5 devices to connect at the same time anywhere you go. The range is upto 30 feet or around
10 meters. Thus wireless technologies and devices are making the work easy, convenient and
faster.",1,,,,1,,
78,Types of Wireless Transmission Media,"(a) Microwave
- Microwave transmission systems transmit data via electromagnetic waves. The signals which have transmitting frequency ranging from 1GHz to 300GHz are called microwaves.
- These types of transmissions are used when very large amounts of data need to be transmitted and need to cover finitely long distances. The transmitters and receivers need to be in line-of-sight to each other for efficient communication without interference.
- But as the Earth's surface is curved and not flat, the microwave towers should be spaced after
every 30 miles to satisfy the line-of-sight constraint.
- For very long distances microwave systems provide very limited support. Also, disturbances in climatic conditions like heavy storms or rains can affect the transmission.
(b) Satellite
- Satellite transmission systems make use of communication satellites.
- The three satellites orbiting around the Earth are :
(i) Geostationary-earth-orbit(GEO),
(ii) Medium-earth-orbit(MEO), and
(iii) Low-earth-orbit(LEO).
- GEO is the farthest from earth and LEO is the closest. The three communication satellites along with their features and distance from earth's surface. Like in microwave there is constraint of line of sight, similarly in satellite communication there is something called footprint.
- The footprint is dependent on distance. The farther the satellite from the Earth's surface, highest is its footprint and the nearer the satellite, shorter is the footprint.",1,,,,1,,
79,Pervasive Computing,"- Pervasive Computing also called as Ubiquitous Computing is anywhere, anytime computing.
- According to this technology, every object around us such as mobile phones, clothes, washing machines, books etc. can be made to have computational capabilty.
- The processing power is embedded into these objects with the help of microprocessor chips.
- Ubiquitous computing has evolved after mobile computing and includes various technologies such as wireless communication and networking, mobile devices, embedded systems, wearable devices, radio frequency ID (RFID) tags, software agents, Internet capabilities, voice recognition and artificial intelligence (AI).",1,,,,1,,
80,Cloud Computing,"- Cloud computing is a type of computing that lets customers easily access resources such as servers, storage, applications etc. over the Internet.
- These resources are shared by multiple computing devices over the network and can be acquired as and when needed and released when the work is done.
- Cloud computing refers to offering computing services from servers in a network.
- Typical features of cloud services are :
• available on demand
• can be accessed over a network
• share resources between multiple applications and tenants scale elastically based on dynamic computing needs provide measured service
• pay-per-use(utility computing)",1,,,,1,,
81,Cloud Computing Deployment Models based on Infrastructure Ownership,#NAME?,1,,,,1,,
82,Cloud Computing Models based on Services provided,#NAME?,1,,,,1,,
83,Advantages of Cloud Computing,"1. Users can easily access or store information from anywhere, anytime, from any device connected to the internet.
2. ⁠As the data is stored on the cloud, there is always a backup and recovery for your data.
3. It enhances an organization's productivity and efficiency by ensuring that the data is all the time available and that too on a click of a second.
4. Cloud computing reduces both hardware and software maintenance costs for organizations as they do not hold or buy anything physically but rent it from the cloud service provider. The cloud service provider takes care of the maintenance job.
5. The users pay only for the services they use from the cloud i.e. pay-per-usage.
6. Cloud offers huge amount of virtual storage which is physically not possible on our personal computer systems. Cloud storage can be used to store all our personal data, photos, documents etc.
7. Cloud service provider guarantees the security of our confidential information by implementing various security mechanisms.",1,,,,1,,
84,Disadvantages of Cloud Computing,"1. You need to be always connected to the internet to access cloud services.
2. Organizations may face problems when transferring their services from one vendor to another, if the platforms provided by the vendors are different.
3. The cloud users have limited control over the services within a cloud infrastructure as everything is controlled by the cloud service provider.
4. Although cloud service providers take full responsibility of security of your confidential data on the cloud but still there could be chances that as we are sending our sensitive information to a third party, it could be hacked.",1,,,,1,,
85,,,,,,,,,
86,Reconstructing Subject-Specific Effect Maps,"Predictive models allow subject-specific inference when analyzing disease
 related alterations in neuroimaging data. Given a subject's data, inference can
 be made at two levels: global, i.e. identifiying condition presence for the
 subject, and local, i.e. detecting condition effect on each individual
 measurement extracted from the subject's data. While global inference is widely
 used, local inference, which can be used to form subject-specific effect maps,
 is rarely used because existing models often yield noisy detections composed of
 dispersed isolated islands. In this article, we propose a reconstruction
 method, named RSM, to improve subject-specific detections of predictive
 modeling approaches and in particular, binary classifiers. RSM specifically
 aims to reduce noise due to sampling error associated with using a finite
 sample of examples to train classifiers. The proposed method is a wrapper-type
 algorithm that can be used with different binary classifiers in a diagnostic
 manner, i.e. without information on condition presence. Reconstruction is posed
 as a Maximum-A-Posteriori problem with a prior model whose parameters are
 estimated from training data in a classifier-specific fashion. Experimental
 evaluation is performed on synthetically generated data and data from the
 Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on
 synthetic data demonstrate that using RSM yields higher detection accuracy
 compared to using models directly or with bootstrap averaging. Analyses on the
 ADNI dataset show that RSM can also improve correlation between
 subject-specific detections in cortical thickness data and non-imaging markers
 of Alzheimer's Disease (AD), such as the Mini Mental State Examination Score
 and Cerebrospinal Fluid amyloid-$\beta$ levels. Further reliability studies on
 the longitudinal ADNI dataset show improvement on detection reliability when
 RSM is used.",1,0,0,0,0,0,
87,Rotation Invariance Neural Network,"Rotation invariance and translation invariance have great values in image
 recognition tasks. In this paper, we bring a new architecture in convolutional
 neural network (CNN) named cyclic convolutional layer to achieve rotation
 invariance in 2-D symbol recognition. We can also get the position and
 orientation of the 2-D symbol by the network to achieve detection purpose for
 multiple non-overlap target. Last but not least, this architecture can achieve
 one-shot learning in some cases using those invariance.",1,0,0,0,0,0,
88,Spherical polyharmonics and Poisson kernels for polyharmonic functions,"We introduce and develop the notion of spherical polyharmonics, which are a
 natural generalisation of spherical harmonics. In particular we study the
 theory of zonal polyharmonics, which allows us, analogously to zonal harmonics,
 to construct Poisson kernels for polyharmonic functions on the union of rotated
 balls. We find the representation of Poisson kernels and zonal polyharmonics in
 terms of the Gegenbauer polynomials. We show the connection between the
 classical Poisson kernel for harmonic functions on the ball, Poisson kernels
 for polyharmonic functions on the union of rotated balls, and the Cauchy-Hua
 kernel for holomorphic functions on the Lie ball.",0,0,1,0,0,0,
89,A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system,"The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the
 Maxwell equations (the so called stochastic MLLG system) describes the creation
 of domain walls and vortices (fundamental objects for the novel nanostructured
 magnetic memories). We first reformulate the stochastic LLG equation into an
 equation with time-differentiable solutions. We then propose a convergent
 $\theta$-linear scheme to approximate the solutions of the reformulated system.
 As a consequence, we prove convergence of the approximate solutions, with no or
 minor conditions on time and space steps (depending on the value of $\theta$).
 Hence, we prove the existence of weak martingale solutions of the stochastic
 MLLG system. Numerical results are presented to show applicability of the
 method.",0,0,1,0,0,0,
90,Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants,"Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species
 were used to explore the influence of preprocessing and feature extraction on
 efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and
 Discrete Wavelet Transforms (DWT) were compared as feature extraction
 techniques for FTIR data of medicinal plants. Various combinations of signal
 processing steps showed different behavior when applied to classification and
 clustering tasks. Best results for WTT and DWT found through grid search were
 similar, significantly improving quality of clustering as well as
 classification accuracy for tuned logistic regression in comparison to original
 spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a
 more versatile and easier to use as a data processing tool in various signal
 processing applications.",1,0,0,1,0,0,
91,On maximizing the fundamental frequency of the complement of an obstacle,"Let $\Omega \subset \mathbb{R}^n$ be a bounded domain satisfying a
 Hayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain
 referred to as ""obstacle"". We are interested in the behaviour of the first
 Dirichlet eigenvalue $ \lambda_1(\Omega \setminus (x+D)) $. First, we prove an
 upper bound on $ \lambda_1(\Omega \setminus (x+D)) $ in terms of the distance
 of the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet
 ground state $ \phi_{\lambda_1} > 0 $ of $ \Omega $. In short, a direct
 corollary is that if \begin{equation} \mu_\Omega := \max_{x}\lambda_1(\Omega
 \setminus (x+D)) \end{equation} is large enough in terms of $ \lambda_1(\Omega)
 $, then all maximizer sets $ x+D $ of $ \mu_\Omega $ are close to each maximum
 point $ x_0 $ of $ \phi_{\lambda_1} $.
 Second, we discuss the distribution of $ \phi_{\lambda_1(\Omega)} $ and the
 possibility to inscribe wavelength balls at a given point in $ \Omega $.
 Finally, we specify our observations to convex obstacles $ D $ and show that
 if $ \mu_\Omega $ is sufficiently large with respect to $ \lambda_1(\Omega) $,
 then all maximizers $ x+D $ of $ \mu_\Omega $ contain all maximum points $ x_0
 $ of $ \phi_{\lambda_1(\Omega)} $.",0,0,1,0,0,0,
92,On the rotation period and shape of the hyperbolic asteroid 1I/`Oumuamua (2017) U1 from its lightcurve,"We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017
 U1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel
 Telescope. From these observations, we derived a partial lightcurve with
 peak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out
 rotation periods less than 3 hr and suggests that the period is at least 5 hr.
 On the assumption that the variability is due to a changing cross section, the
 axial ratio is at least 3:1. We saw no evidence for a coma or tail in either
 individual images or in a stacked image having an equivalent exposure time of
 9000 s.",0,1,0,0,0,0,
93,Adverse effects of polymer coating on heat transport at solid-liquid interface,"The ability of metallic nanoparticles to supply heat to a liquid environment
 under exposure to an external optical field has attracted growing interest for
 biomedical applications. Controlling the thermal transport properties at a
 solid-liquid interface then appears to be particularly relevant. In this work,
 we address the thermal transport between water and a gold surface coated by a
 polymer layer. Using molecular dynamics simulations, we demonstrate that
 increasing the polymer density displaces the domain resisting to the heat flow,
 while it doesn't affect the final amount of thermal energy released in the
 liquid. This unexpected behavior results from a trade-off established by the
 increasing polymer density which couples more efficiently with the solid but
 initiates a counterbalancing resistance with the liquid.",0,1,0,0,0,0,
94,"SPH calculations of Mars-scale collisions: the role of the Equation of State, material rheologies, and numerical effects","We model large-scale ($\approx$2000km) impacts on a Mars-like planet using a
 Smoothed Particle Hydrodynamics code. The effects of material strength and of
 using different Equations of State on the post-impact material and temperature
 distributions are investigated. The properties of the ejected material in terms
 of escaping and disc mass are analysed as well. We also study potential
 numerical effects in the context of density discontinuities and rigid body
 rotation. We find that in the large-scale collision regime considered here
 (with impact velocities of 4km/s), the effect of material strength is
 substantial for the post-impact distribution of the temperature and the
 impactor material, while the influence of the Equation of State is more subtle
 and present only at very high temperatures.",0,1,0,0,0,0,
95,$\mathcal{R}_{0}$ fails to predict the outbreak potential in the presence of natural-boosting immunity,"Time varying susceptibility of host at individual level due to waning and
 boosting immunity is known to induce rich long-term behavior of disease
 transmission dynamics. Meanwhile, the impact of the time varying heterogeneity
 of host susceptibility on the shot-term behavior of epidemics is not
 well-studied, even though the large amount of the available epidemiological
 data are the short-term epidemics. Here we constructed a parsimonious
 mathematical model describing the short-term transmission dynamics taking into
 account natural-boosting immunity by reinfection, and obtained the explicit
 solution for our model. We found that our system show ""the delayed epidemic"",
 the epidemic takes off after negative slope of the epidemic curve at the
 initial phase of epidemic, in addition to the common classification in the
 standard SIR model, i.e., ""no epidemic"" as $\mathcal{R}_{0}\leq1$ or normal
 epidemic as $\mathcal{R}_{0}>1$. Employing the explicit solution we derived the
 condition for each classification.",0,0,0,0,1,0,
96,A global sensitivity analysis and reduced order models for hydraulically-fractured horizontal wells,"We present a systematic global sensitivity analysis using the Sobol method
 which can be utilized to rank the variables that affect two quantity of
 interests -- pore pressure depletion and stress change -- around a
 hydraulically-fractured horizontal well based on their degree of importance.
 These variables include rock properties and stimulation design variables. A
 fully-coupled poroelastic hydraulic fracture model is used to account for pore
 pressure and stress changes due to production. To ease the computational cost
 of a simulator, we also provide reduced order models (ROMs), which can be used
 to replace the complex numerical model with a rather simple analytical model,
 for calculating the pore pressure and stresses at different locations around
 hydraulic fractures. The main findings of this research are: (i) mobility,
 production pressure, and fracture half-length are the main contributors to the
 changes in the quantities of interest. The percentage of the contribution of
 each parameter depends on the location with respect to pre-existing hydraulic
 fractures and the quantity of interest. (ii) As the time progresses, the effect
 of mobility decreases and the effect of production pressure increases. (iii)
 These two variables are also dominant for horizontal stresses at large
 distances from hydraulic fractures. (iv) At zones close to hydraulic fracture
 tips or inside the spacing area, other parameters such as fracture spacing and
 half-length are the dominant factors that affect the minimum horizontal stress.
 The results of this study will provide useful guidelines for the stimulation
 design of legacy wells and secondary operations such as refracturing and infill
 drilling.",1,0,0,0,0,0,
97,Role-separating ordering in social dilemmas controlled by topological frustration,"""Three is a crowd"" is an old proverb that applies as much to social
 interactions, as it does to frustrated configurations in statistical physics
 models. Accordingly, social relations within a triangle deserve special
 attention. With this motivation, we explore the impact of topological
 frustration on the evolutionary dynamics of the snowdrift game on a triangular
 lattice. This topology provides an irreconcilable frustration, which prevents
 anti-coordination of competing strategies that would be needed for an optimal
 outcome of the game. By using different strategy updating protocols, we observe
 complex spatial patterns in dependence on payoff values that are reminiscent to
 a honeycomb-like organization, which helps to minimize the negative consequence
 of the topological frustration. We relate the emergence of these patterns to
 the microscopic dynamics of the evolutionary process, both by means of
 mean-field approximations and Monte Carlo simulations. For comparison, we also
 consider the same evolutionary dynamics on the square lattice, where of course
 the topological frustration is absent. However, with the deletion of diagonal
 links of the triangular lattice, we can gradually bridge the gap to the square
 lattice. Interestingly, in this case the level of cooperation in the system is
 a direct indicator of the level of topological frustration, thus providing a
 method to determine frustration levels in an arbitrary interaction network.",0,1,0,0,0,0,
98,Dynamics of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells: the effect of self-localization,"We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se
 diluted-magnetic-semiconductor quantum wells using time-resolved
 photoluminescence (PL). The magnetic field and temperature dependencies of this
 dynamics allow us to separate the non-magnetic and magnetic contributions to
 the exciton localization. We deduce the EMP energy of 14 meV, which is in
 agreement with time-integrated measurements based on selective excitation and
 the magnetic field dependence of the PL circular polarization degree. The
 polaron formation time of 500 ps is significantly longer than the corresponding
 values reported earlier. We propose that this behavior is related to strong
 self-localization of the EMP, accompanied with a squeezing of the heavy-hole
 envelope wavefunction. This conclusion is also supported by the decrease of the
 exciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and
 temperature.",0,1,0,0,0,0,
99,On Varieties of Ordered Automata,"The classical Eilenberg correspondence, based on the concept of the syntactic
 monoid, relates varieties of regular languages with pseudovarieties of finite
 monoids. Various modifications of this correspondence appeared, with more
 general classes of regular languages on one hand and classes of more complex
 algebraic structures on the other hand. For example, classes of languages need
 not be closed under complementation or all preimages under homomorphisms, while
 monoids can be equipped with a compatible order or they can have a
 distinguished set of generators. Such generalized varieties and pseudovarieties
 also have natural counterparts formed by classes of finite (ordered) automata.
 In this paper the previous approaches are combined. The notion of positive
 $\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final
 states are specified) is introduced and their correspondence with positive
 $\mathcal C$-varieties of languages is proved.",1,0,0,0,0,0,
100,Direct Evidence of Spontaneous Abrikosov Vortex State in Ferromagnetic Superconductor EuFe$_2$(As$_{1-x}$P$_x$)$_2$ with $x=0.21$,"Using low-temperature Magnetic Force Microscopy (MFM) we provide direct
 experimental evidence for spontaneous vortex phase (SVP) formation in
 EuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting
 $T^{\rm 0}_{\rm SC}=23.6$~K and ferromagnetic $T_{\rm FM}\sim17.7$~K transition
 temperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the
 vicinity of $T_{\rm FM}$. Also, upon cooling cycle near $T_{\rm FM}$ we observe
 the first-order transition from the short period domain structure, which
 appears in the Meissner state, into the long period domain structure with
 spontaneous vortices. It is the first experimental observation of this scenario
 in the ferromagnetic superconductors. Low-temperature phase is characterized by
 much larger domains in V-AV state and peculiar branched striped structures at
 the surface, which are typical for uniaxial ferromagnets with perpendicular
 magnetic anisotropy (PMA). The domain wall parameters at various temperatures
 are estimated.",0,1,0,0,0,0,
101,A rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ with 432 symmetries,"The recent discovery that the exponent of matrix multiplication is determined
 by the rank of the symmetrized matrix multiplication tensor has invigorated
 interest in better understanding symmetrized matrix multiplication. I present
 an explicit rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ and
 describe its symmetry group.",0,0,1,0,0,0,
102,The PdBI Arcsecond Whirlpool Survey (PAWS). The Role of Spiral Arms in Cloud and Star Formation,"The process that leads to the formation of the bright star forming sites
 observed along prominent spiral arms remains elusive. We present results of a
 multi-wavelength study of a spiral arm segment in the nearby grand-design
 spiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas
 spurs. The combined observations of the(ionized, atomic, molecular, dusty)
 interstellar medium (ISM) with star formation tracers (HII regions, young
 <10Myr stellar clusters) suggest (1) no variation in giant molecular cloud
 (GMC) properties between arm and gas spurs, (2) gas spurs and extinction
 feathers arising from the same structure with a close spatial relation between
 gas spurs and ongoing/recent star formation (despite higher gas surface
 densities in the spiral arm), (3) no trend in star formation age either along
 the arm or along a spur, (4) evidence for strong star formation feedback in gas
 spurs: (5) tentative evidence for star formation triggered by stellar feedback
 for one spur, and (6) GMC associations (GMAs) being no special entities but the
 result of blending of gas arm/spur cross-sections in lower resolution
 observations. We conclude that there is no evidence for a coherent star
 formation onset mechanism that can be solely associated to the presence of the
 spiral density wave. This suggests that other (more localized) mechanisms are
 important to delay star formation such that it occurs in spurs. The evidence of
 star formation proceeding over several million years within individual spurs
 implies that the mechanism that leads to star formation acts or is sustained
 over a longer time-scale.",0,1,0,0,0,0,
103,Higher structure in the unstable Adams spectral sequence,"We describe a variant construction of the unstable Adams spectral the
 sequence for a space $Y$, associated to any free simplicial resolution of
 $H^*(Y;R)$ for $R=\mathbb{F}_p$ or $\mathbb{Q}$. We use this construction to
 describe the differentials and filtration in the spectral sequence in terms of
 appropriate systems of higher cohomology operations.",0,0,1,0,0,0,
104,Comparing Covariate Prioritization via Matching to Machine Learning Methods for Causal Inference using Five Empirical Applications,"When investigators seek to estimate causal effects, they often assume that
 selection into treatment is based only on observed covariates. Under this
 identification strategy, analysts must adjust for observed confounders. While
 basic regression models have long been the dominant method of statistical
 adjustment, more robust methods based on matching or weighting have become more
 common. Of late, even more flexible methods based on machine learning methods
 have been developed for statistical adjustment. These machine learning methods
 are designed to be black box methods with little input from the researcher.
 Recent research used a data competition to evaluate various methods of
 statistical adjustment and found that black box methods out performed all other
 methods of statistical adjustment. Matching methods with covariate
 prioritization are designed for direct input from substantive investigators in
 direct contrast to black methods. In this article, we use a different research
 design to compare matching with covariate prioritization to black box methods.
 We use black box methods to replicate results from five studies where matching
 with covariate prioritization was used to customize the statistical adjustment
 in direct response to substantive expertise. We find little difference across
 the methods. We conclude with advice for investigators.",0,0,0,1,0,0,
105,Acoustic Impedance Calculation via Numerical Solution of the Inverse Helmholtz Problem,"Assigning homogeneous boundary conditions, such as acoustic impedance, to the
 thermoviscous wave equations (TWE) derived by transforming the linearized
 Navier-Stokes equations (LNSE) to the frequency domain yields a so-called
 Helmholtz solver, whose output is a discrete set of complex eigenfunction and
 eigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --
 reverses such procedure by returning the value of acoustic impedance at one or
 more unknown impedance boundaries (IBs) of a given domain via spatial
 integration of the TWE for a given real-valued frequency with assigned
 conditions on other boundaries. The iHS procedure is applied to a second-order
 spatial discretization of the TWEs derived on an unstructured grid with
 staggered grid arrangement. The momentum equation only is extended to the
 center of each IB face where pressure and velocity components are co-located
 and treated as unknowns. One closure condition considered for the iHS is the
 assignment of the surface gradient of pressure phase over the IBs,
 corresponding to assigning the shape of the acoustic waveform at the IB. The
 iHS procedure is carried out independently for each frequency in order to
 return the complete broadband complex impedance distribution at the IBs in any
 desired frequency range. The iHS approach is first validated against Rott's
 theory for both inviscid and viscous, rectangular and circular ducts. The
 impedance of a geometrically complex toy cavity is then reconstructed and
 verified against companion full compressible unstructured Navier-Stokes
 simulations resolving the cavity geometry and one-dimensional impedance test
 tube calculations based on time-domain impedance boundary conditions (TDIBC).
 The iHS methodology is also shown to capture thermoacoustic effects, with
 reconstructed impedance values quantitatively in agreement with thermoacoustic
 growth rates.",0,1,0,0,0,0,
106,Deciphering noise amplification and reduction in open chemical reaction networks,"The impact of random fluctuations on the dynamical behavior a complex
 biological systems is a longstanding issue, whose understanding would shed
 light on the evolutionary pressure that nature imposes on the intrinsic noise
 levels and would allow rationally designing synthetic networks with controlled
 noise. Using the It stochastic differential equation formalism, we performed
 both analytic and numerical analyses of several model systems containing
 different molecular species in contact with the environment and interacting
 with each other through mass-action kinetics. These systems represent for
 example biomolecular oligomerization processes, complex-breakage reactions,
 signaling cascades or metabolic networks. For chemical reaction networks with
 zero deficiency values, which admit a detailed- or complex-balanced steady
 state, all molecular species are uncorrelated. The number of molecules of each
 species follow a Poisson distribution and their Fano factors, which measure the
 intrinsic noise, are equal to one. Systems with deficiency one have an
 unbalanced non-equilibrium steady state and a non-zero S-flux, defined as the
 flux flowing between the complexes multiplied by an adequate stoichiometric
 coefficient. In this case, the noise on each species is reduced if the flux
 flows from the species of lowest to highest complexity, and is amplified is the
 flux goes in the opposite direction. These results are generalized to systems
 of deficiency two, which possess two independent non-vanishing S-fluxes, and we
 conjecture that a similar relation holds for higher deficiency systems.",0,0,0,0,1,0,
107,Many-Body Localization: Stability and Instability,"Rare regions with weak disorder (Griffiths regions) have the potential to
 spoil localization. We describe a non-perturbative construction of local
 integrals of motion (LIOMs) for a weakly interacting spin chain in one
 dimension, under a physically reasonable assumption on the statistics of
 eigenvalues. We discuss ideas about the situation in higher dimensions, where
 one can no longer ensure that interactions involving the Griffiths regions are
 much smaller than the typical energy-level spacing for such regions. We argue
 that ergodicity is restored in dimension d > 1, although equilibration should
 be extremely slow, similar to the dynamics of glasses.",0,1,1,0,0,0,
108,Fault Detection and Isolation Tools (FDITOOLS) User's Guide,"The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB
 functions for the analysis and solution of fault detection and model detection
 problems. The implemented functions are based on the computational procedures
 described in the Chapters 5, 6 and 7 of the book: ""A. Varga, Solving Fault
 Diagnosis Problems - Linear Synthesis Techniques, Springer, 2017"". This
 document is the User's Guide for the version V1.0 of FDITOOLS. First, we
 present the mathematical background for solving several basic exact and
 approximate synthesis problems of fault detection filters and model detection
 filters. Then, we give in-depth information on the command syntax of the main
 analysis and synthesis functions. Several examples illustrate the use of the
 main functions of FDITOOLS.",1,0,0,0,0,0,
109,Complexity of Deciding Detectability in Discrete Event Systems,"Detectability of discrete event systems (DESs) is a question whether the
 current and subsequent states can be determined based on observations. Shu and
 Lin designed a polynomial-time algorithm to check strong (periodic)
 detectability and an exponential-time (polynomial-space) algorithm to check
 weak (periodic) detectability. Zhang showed that checking weak (periodic)
 detectability is PSpace-complete. This intractable complexity opens a question
 whether there are structurally simpler DESs for which the problem is tractable.
 In this paper, we show that it is not the case by considering DESs represented
 as deterministic finite automata without non-trivial cycles, which are
 structurally the simplest deadlock-free DESs. We show that even for such very
 simple DESs, checking weak (periodic) detectability remains intractable. On the
 contrary, we show that strong (periodic) detectability of DESs can be
 efficiently verified on a parallel computer.",1,0,0,0,0,0,
110,The Knaster-Tarski theorem versus monotone nonexpansive mappings,"Let $X$ be a partially ordered set with the property that each family of
 order intervals of the form $[a,b],[a,\rightarrow )$ with the finite
 intersection property has a nonempty intersection. We show that every directed
 subset of $X$ has a supremum. Then we apply the above result to prove that if
 $X$ is a topological space with a partial order $\preceq $ for which the order
 intervals are compact, $\mathcal{F}$ a nonempty commutative family of monotone
 maps from $X$ into $X$ and there exists $c\in X$ such that $c\preceq Tc$ for
 every $T\in \mathcal{F}$, then the set of common fixed points of $\mathcal{F}$
 is nonempty and has a maximal element. The result, specialized to the case of
 Banach spaces gives a general fixed point theorem that drops almost all
 assumptions from the recent results in this area. An application to the theory
 of integral equations of Urysohn's type is also given.",0,0,1,0,0,0,
111,Efficient methods for computing integrals in electronic structure calculations,"Efficient methods are proposed, for computing integrals appeaing in
 electronic structure calculations. The methods consist of two parts: the first
 part is to represent the integrals as contour integrals and the second one is
 to evaluate the contour integrals by the Clenshaw-Curtis quadrature. The
 efficiency of the proposed methods is demonstrated through numerical
 experiments.",0,1,0,0,0,0,
112,Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source,"We present a novel sound localization algorithm for a non-line-of-sight
 (NLOS) sound source in indoor environments. Our approach exploits the
 diffraction properties of sound waves as they bend around a barrier or an
 obstacle in the scene. We combine a ray tracing based sound propagation
 algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate
 bending effects by placing a virtual sound source on a wedge in the
 environment. We precompute the wedges of a reconstructed mesh of an indoor
 scene and use them to generate diffraction acoustic rays to localize the 3D
 position of the source. Our method identifies the convergence region of those
 generated acoustic rays as the estimated source position based on a particle
 filter. We have evaluated our algorithm in multiple scenarios consisting of a
 static and dynamic NLOS sound source. In our tested cases, our approach can
 localize a source position with an average accuracy error, 0.7m, measured by
 the L2 distance between estimated and actual source locations in a 7m*7m*3m
 room. Furthermore, we observe 37% to 130% improvement in accuracy over a
 state-of-the-art localization method that does not model diffraction effects,
 especially when a sound source is not visible to the robot.",1,0,0,0,0,0,
113,"Jacob's ladders, crossbreeding in the set of $??$-factorization formulas and selection of families of $??$-kindred real continuous functions","In this paper we introduce the notion of $\zeta$-crossbreeding in a set of
 $\zeta$-factorization formulas and also the notion of complete hybrid formula
 as the final result of that crossbreeding. The last formula is used as a
 criterion for selection of families of $\zeta$-kindred elements in class of
 real continuous functions.
 Dedicated to recalling of Gregory Mendel's pea-crossbreeding.",0,0,1,0,0,0,
114,Minimax Estimation of the $L_1$ Distance,"We consider the problem of estimating the $L_1$ distance between two discrete
 probability measures $P$ and $Q$ from empirical data in a nonasymptotic and
 large alphabet setting. When $Q$ is known and one obtains $n$ samples from $P$,
 we show that for every $Q$, the minimax rate-optimal estimator with $n$ samples
 achieves performance comparable to that of the maximum likelihood estimator
 (MLE) with $n\ln n$ samples. When both $P$ and $Q$ are unknown, we construct
 minimax rate-optimal estimators whose worst case performance is essentially
 that of the known $Q$ case with $Q$ being uniform, implying that $Q$ being
 uniform is essentially the most difficult case. The \emph{effective sample size
 enlargement} phenomenon, identified in Jiao \emph{et al.} (2015), holds both in
 the known $Q$ case for every $Q$ and the $Q$ unknown case. However, the
 construction of optimal estimators for $\|P-Q\|_1$ requires new techniques and
 insights beyond the approximation-based method of functional estimation in Jiao
 \emph{et al.} (2015).",0,0,1,1,0,0,
115,Density large deviations for multidimensional stochastic hyperbolic conservation laws,"We investigate the density large deviation function for a multidimensional
 conservation law in the vanishing viscosity limit, when the probability
 concentrates on weak solutions of a hyperbolic conservation law conservation
 law. When the conductivity and dif-fusivity matrices are proportional, i.e. an
 Einstein-like relation is satisfied, the problem has been solved in [4]. When
 this proportionality does not hold, we compute explicitly the large deviation
 function for a step-like density profile, and we show that the associated
 optimal current has a non trivial structure. We also derive a lower bound for
 the large deviation function, valid for a general weak solution, and leave the
 general large deviation function upper bound as a conjecture.",0,1,1,0,0,0,
116,mixup: Beyond Empirical Risk Minimization,"Large deep neural networks are powerful, but exhibit undesirable behaviors
 such as memorization and sensitivity to adversarial examples. In this work, we
 propose mixup, a simple learning principle to alleviate these issues. In
 essence, mixup trains a neural network on convex combinations of pairs of
 examples and their labels. By doing so, mixup regularizes the neural network to
 favor simple linear behavior in-between training examples. Our experiments on
 the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
 that mixup improves the generalization of state-of-the-art neural network
 architectures. We also find that mixup reduces the memorization of corrupt
 labels, increases the robustness to adversarial examples, and stabilizes the
 training of generative adversarial networks.",1,0,0,1,0,0,
117,Equality of the usual definitions of Brakke flow,"In 1978 Brakke introduced the mean curvature flow in the setting of geometric
 measure theory. There exist multiple variants of the original definition. Here
 we prove that most of them are indeed equal. One central point is to correct
 the proof of Brakke's ??3.5, where he develops an estimate for the evolution
 of the measure of time-dependent test functions.",0,0,1,0,0,0,
118,Dynamic Base Station Repositioning to Improve Spectral Efficiency of Drone Small Cells,"With recent advancements in drone technology, researchers are now considering
 the possibility of deploying small cells served by base stations mounted on
 flying drones. A major advantage of such drone small cells is that the
 operators can quickly provide cellular services in areas of urgent demand
 without having to pre-install any infrastructure. Since the base station is
 attached to the drone, technically it is feasible for the base station to
 dynamic reposition itself in response to the changing locations of users for
 reducing the communication distance, decreasing the probability of signal
 blocking, and ultimately increasing the spectral efficiency. In this paper, we
 first propose distributed algorithms for autonomous control of drone movements,
 and then model and analyse the spectral efficiency performance of a drone small
 cell to shed new light on the fundamental benefits of dynamic repositioning. We
 show that, with dynamic repositioning, the spectral efficiency of drone small
 cells can be increased by nearly 100\% for realistic drone speed, height, and
 user traffic model and without incurring any major increase in drone energy
 consumption.",1,0,0,0,0,0,
119,An Unsupervised Homogenization Pipeline for Clustering Similar Patients using Electronic Health Record Data,"Electronic health records (EHR) contain a large variety of information on the
 clinical history of patients such as vital signs, demographics, diagnostic
 codes and imaging data. The enormous potential for discovery in this rich
 dataset is hampered by its complexity and heterogeneity.
 We present the first study to assess unsupervised homogenization pipelines
 designed for EHR clustering. To identify the optimal pipeline, we tested
 accuracy on simulated data with varying amounts of redundancy, heterogeneity,
 and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
 Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
 Z-scoring, and Deep Autoencoders.",0,0,0,0,1,0,
120,Deep Neural Network Optimized to Resistive Memory with Nonlinear Current-Voltage Characteristics,"Artificial Neural Network computation relies on intensive vector-matrix
 multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array
 showed a feasibility of implementing such operations with high energy
 efficiency, thus there are many works on efficiently utilizing emerging NVM
 crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V
 characteristics restrain critical design parameters, such as the read voltage
 and weight range, resulting in substantial accuracy loss. In this paper,
 instead of optimizing hardware parameters to a given neural network, we propose
 a methodology of reconstructing a neural network itself optimized to resistive
 memory crossbar arrays. To verify the validity of the proposed method, we
 simulated various neural network with MNIST and CIFAR-10 dataset using two
 different specific Resistive Random Access Memory (RRAM) model. Simulation
 results show that our proposed neural network produces significantly higher
 inference accuracies than conventional neural network when the synapse devices
 have nonlinear I-V characteristics.",1,0,0,0,0,0,
121,Rate-Distortion Region of a Gray-Wyner Model with Side Information,"In this work, we establish a full single-letter characterization of the
 rate-distortion region of an instance of the Gray-Wyner model with side
 information at the decoders. Specifically, in this model an encoder observes a
 pair of memoryless, arbitrarily correlated, sources $(S^n_1,S^n_2)$ and
 communicates with two receivers over an error-free rate-limited link of
 capacity $R_0$, as well as error-free rate-limited individual links of
 capacities $R_1$ to the first receiver and $R_2$ to the second receiver. Both
 receivers reproduce the source component $S^n_2$ losslessly; and Receiver $1$
 also reproduces the source component $S^n_1$ lossily, to within some prescribed
 fidelity level $D_1$. Also, Receiver $1$ and Receiver $2$ are equipped
 respectively with memoryless side information sequences $Y^n_1$ and $Y^n_2$.
 Important in this setup, the side information sequences are arbitrarily
 correlated among them, and with the source pair $(S^n_1,S^n_2)$; and are not
 assumed to exhibit any particular ordering. Furthermore, by specializing the
 main result to two Heegard-Berger models with successive refinement and
 scalable coding, we shed light on the roles of the common and private
 descriptions that the encoder should produce and what they should carry
 optimally. We develop intuitions by analyzing the developed single-letter
 optimal rate-distortion regions of these models, and discuss some insightful
 binary examples.",1,0,1,0,0,0,
122,Fourier-based numerical approximation of the Weertman equation for moving dislocations,"This work discusses the numerical approximation of a nonlinear
 reaction-advection-diffusion equation, which is a dimensionless form of the
 Weertman equation. This equation models steadily-moving dislocations in
 materials science. It reduces to the celebrated Peierls-Nabarro equation when
 its advection term is set to zero. The approach rests on considering a
 time-dependent formulation, which admits the equation under study as its
 long-time limit. Introducing a Preconditioned Collocation Scheme based on
 Fourier transforms, the iterative numerical method presented solves the
 time-dependent problem, delivering at convergence the desired numerical
 solution to the Weertman equation. Although it rests on an explicit
 time-evolution scheme, the method allows for large time steps, and captures the
 solution in a robust manner. Numerical results illustrate the efficiency of the
 approach for several types of nonlinearities.",0,1,0,0,0,0,
123,Design Decisions for Weave: A Real-Time Web-based Collaborative Visualization Framework,"There are many web-based visualization systems available to date, each having
 its strengths and limitations. The goals these systems set out to accomplish
 influence design decisions and determine how reusable and scalable they are.
 Weave is a new web-based visualization platform with the broad goal of enabling
 visualization of any available data by anyone for any purpose. Our open source
 framework supports highly interactive linked visualizations for users of
 varying skill levels. What sets Weave apart from other systems is its
 consideration for real-time remote collaboration with session history. We
 provide a detailed account of the various framework designs we considered with
 comparisons to existing state-of-the-art systems.",1,0,0,0,0,0,
124,Suzaku Analysis of the Supernova Remnant G306.3-0.9 and the Gamma-ray View of Its Neighborhood,"We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using
 archival multi-wavelength data. The Suzaku spectra are well described by
 two-component thermal plasma models: The soft component is in ionization
 equilibrium and has a temperature $\sim$0.59 keV, while the hard component has
 temperature $\sim$3.2 keV and ionization time-scale $\sim$$2.6\times10^{10}$
 cm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\sim$6.5 keV
 from this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the
 X-ray emission has an ejecta origin. The centroid energy of the Fe-K line
 supports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than
 a core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its
 surrounding were analyzed using about 6 years of Fermi data. We report about
 the non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray
 source in the south-west of G306.3$-$0.9 with a significance of
 $\sim$13$\sigma$. We discuss several scenarios for these results with the help
 of data from other wavebands to understand the SNR and its neighborhood.",0,1,0,0,0,0,
125,Japanese Sentiment Classification using a Tree-Structured Long Short-Term Memory with Attention,"Previous approaches to training syntax-based sentiment classification models
 required phrase-level annotated corpora, which are not readily available in
 many languages other than English. Thus, we propose the use of tree-structured
 Long Short-Term Memory with an attention mechanism that pays attention to each
 subtree of the parse tree. Experimental results indicate that our model
 achieves the state-of-the-art performance in a Japanese sentiment
 classification task.",1,0,0,0,0,0,
126,"Covariances, Robustness, and Variational Bayes","Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
 inference technique that is increasingly popular due to its fast runtimes on
 large-scale datasets. However, even when MFVB provides accurate posterior means
 for certain parameters, it often mis-estimates variances and covariances.
 Furthermore, prior robustness measures have remained undeveloped for MFVB. By
 deriving a simple formula for the effect of infinitesimal model perturbations
 on MFVB posterior means, we provide both improved covariance estimates and
 local robustness measures for MFVB, thus greatly expanding the practical
 usefulness of MFVB posterior approximations. The estimates for MFVB posterior
 covariances rely on a result from the classical Bayesian robustness literature
 relating derivatives of posterior expectations to posterior covariances and
 include the Laplace approximation as a special case. Our key condition is that
 the MFVB approximation provides good estimates of a select subset of posterior
 means---an assumption that has been shown to hold in many practical settings.
 In our experiments, we demonstrate that our methods are simple, general, and
 fast, providing accurate posterior uncertainty estimates and robustness
 measures with runtimes that can be an order of magnitude faster than MCMC.",0,0,0,1,0,0,
127,Are multi-factor Gaussian term structure models still useful? An empirical analysis on Italian BTPs,"In this paper, we empirically study models for pricing Italian sovereign
 bonds under a reduced form framework, by assuming different dynamics for the
 short-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek
 multi-factor models, with a focus on optimization algorithms applied in the
 calibration exercise. The Kalman filter algorithm together with a maximum
 likelihood estimation method are considered to fit the Italian term-structure
 over a 12-year horizon, including the global financial crisis and the euro area
 sovereign debt crisis. Analytic formulas for the gradient vector and the
 Hessian matrix of the likelihood function are provided.",0,0,0,0,0,1,
128,Probing valley filtering effect by Andreev reflection in zigzag graphene nanoribbon,"Ballistic point contact (BPC) with zigzag edges in graphene is a main
 candidate of a valley filter, in which the polarization of the valley degree of
 freedom can be selected by using a local gate voltage. Here, we propose to
 detect the valley filtering effect by Andreev reflection. Because electrons in
 the lowest conduction band and the highest valence band of the BPC possess
 opposite chirality, the inter-band Andreev reflection is strongly suppressed,
 after multiple scattering and interference. We draw this conclusion by both the
 scattering matrix analysis and the numerical simulation. The Andreev reflection
 as a function of the incident energy of electrons and the local gate voltage at
 the BPC is obtained, by which the parameter region for a perfect valley filter
 and the direction of valley polarization can be determined. The Andreev
 reflection exhibits an oscillatory decay with the length of the BPC, indicating
 a negative correlation to valley polarization.",0,1,0,0,0,0,
129,Generalized Approximate Message-Passing Decoder for Universal Sparse Superposition Codes,"Sparse superposition (SS) codes were originally proposed as a
 capacity-achieving communication scheme over the additive white Gaussian noise
 channel (AWGNC) [1]. Very recently, it was discovered that these codes are
 universal, in the sense that they achieve capacity over any memoryless channel
 under generalized approximate message-passing (GAMP) decoding [2], although
 this decoder has never been stated for SS codes. In this contribution we
 introduce the GAMP decoder for SS codes, we confirm empirically the
 universality of this communication scheme through its study on various channels
 and we provide the main analysis tools: state evolution and potential. We also
 compare the performance of GAMP with the Bayes-optimal MMSE decoder. We
 empirically illustrate that despite the presence of a phase transition
 preventing GAMP to reach the optimal performance, spatial coupling allows to
 boost the performance that eventually tends to capacity in a proper limit. We
 also prove that, in contrast with the AWGNC case, SS codes for binary input
 channels have a vanishing error floor in the limit of large codewords.
 Moreover, the performance of Hadamard-based encoders is assessed for practical
 implementations.",1,0,1,0,0,0,
130,LAAIR: A Layered Architecture for Autonomous Interactive Robots,"When developing general purpose robots, the overarching software architecture
 can greatly affect the ease of accomplishing various tasks. Initial efforts to
 create unified robot systems in the 1990s led to hybrid architectures,
 emphasizing a hierarchy in which deliberative plans direct the use of reactive
 skills. However, since that time there has been significant progress in the
 low-level skills available to robots, including manipulation and perception,
 making it newly feasible to accomplish many more tasks in real-world domains.
 There is thus renewed optimism that robots will be able to perform a wide array
 of tasks while maintaining responsiveness to human operators. However, the top
 layer in traditional hybrid architectures, designed to achieve long-term goals,
 can make it difficult to react quickly to human interactions during goal-driven
 execution. To mitigate this difficulty, we propose a novel architecture that
 supports such transitions by adding a top-level reactive module which has
 flexible access to both reactive skills and a deliberative control module. To
 validate this architecture, we present a case study of its application on a
 domestic service robot platform.",1,0,0,0,0,0,
131,3D Human Pose Estimation in RGBD Images for Robotic Task Learning,"We propose an approach to estimate 3D human pose in real world units from a
 single RGBD image and show that it exceeds performance of monocular 3D pose
 estimation approaches from color as well as pose estimation exclusively from
 depth. Our approach builds on robust human keypoint detectors for color images
 and incorporates depth for lifting into 3D. We combine the system with our
 learning from demonstration framework to instruct a service robot without the
 need of markers. Experiments in real world settings demonstrate that our
 approach enables a PR2 robot to imitate manipulation actions observed from a
 human teacher.",1,0,0,0,0,0,
132,Simultaneous non-vanishing for Dirichlet L-functions,"We extend the work of Fouvry, Kowalski and Michel on correlation between
 Hecke eigenvalues of modular forms and algebraic trace functions in order to
 establish an asymptotic formula for a generalized cubic moment of modular
 L-functions at the central point s = 1/2 and for prime moduli q. As an
 application, we exploit our recent result on the mollification of the fourth
 moment of Dirichlet L-functions to derive that for any pair
 $(\omega_1,\omega_2)$ of multiplicative characters modulo q, there is a
 positive proportion of $\chi$ (mod q) such that $L(\chi, 1/2 ), L(\chi\omega_1,
 1/2 )$ and $L(\chi\omega_2, 1/2)$ are simultaneously not too small.",0,0,1,0,0,0,
133,Wehrl Entropy Based Quantification of Nonclassicality for Single Mode Quantum Optical States,"Nonclassical states of a quantized light are described in terms of
 Glauber-Sudarshan P distribution which is not a genuine classical probability
 distribution. Despite several attempts, defining a uniform measure of
 nonclassicality (NC) for the single mode quantum states of light is yet an open
 task. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that
 the existing well-known measures fail to quantify the NC of single mode states
 that are generated under multiple NC-inducing operations. Recently, Ivan et.
 al. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of
 non-Gaussian character of quantum optical states in terms of Wehrl entropy.
 Here, we adopt this concept in the context of single mode NC. In this paper, we
 propose a new quantification of NC for the single mode quantum states of light
 as the difference between the total Wehrl entropy of the state and the maximum
 Wehrl entropy arising due to its classical characteristics. This we achieve by
 subtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any
 classical state that has same randomness as measured in terms of von-Neumann
 entropy. We obtain analytic expressions of NC for most of the states, in
 particular, all pure states and Gaussian mixed states. However, the evaluation
 of NC for the non-Gaussian mixed states is subject to extensive numerical
 computation that lies beyond the scope of the current work. We show that, along
 with the states generated under single NC-inducing operations, also for the
 broader class of states that are generated under multiple NC-inducing
 operations, our quantification enumerates the NC consistently.",1,1,0,0,0,0,
134,Attention-based Natural Language Person Retrieval,"Following the recent progress in image classification and captioning using
 deep learning, we develop a novel natural language person retrieval system
 based on an attention mechanism. More specifically, given the description of a
 person, the goal is to localize the person in an image. To this end, we first
 construct a benchmark dataset for natural language person retrieval. To do so,
 we generate bounding boxes for persons in a public image dataset from the
 segmentation masks, which are then annotated with descriptions and attributes
 using the Amazon Mechanical Turk. We then adopt a region proposal network in
 Faster R-CNN as a candidate region generator. The cropped images based on the
 region proposals as well as the whole images with attention weights are fed
 into Convolutional Neural Networks for visual feature extraction, while the
 natural language expression and attributes are input to Bidirectional Long
 Short- Term Memory (BLSTM) models for text feature extraction. The visual and
 text features are integrated to score region proposals, and the one with the
 highest score is retrieved as the output of our system. The experimental
 results show significant improvement over the state-of-the-art method for
 generic object retrieval and this line of research promises to benefit search
 in surveillance video footage.",1,0,0,0,0,0,
135,Large Scale Automated Forecasting for Monitoring Network Safety and Security,"Real time large scale streaming data pose major challenges to forecasting, in
 particular defying the presence of human experts to perform the corresponding
 analysis. We present here a class of models and methods used to develop an
 automated, scalable and versatile system for large scale forecasting oriented
 towards safety and security monitoring. Our system provides short and long term
 forecasts and uses them to detect safety and security issues in relation with
 multiple internet connected devices well in advance they might take place.",0,0,0,1,0,0,
136,Contextual Regression: An Accurate and Conveniently Interpretable Nonlinear Model for Mining Discovery from Scientific Data,"Machine learning algorithms such as linear regression, SVM and neural network
 have played an increasingly important role in the process of scientific
 discovery. However, none of them is both interpretable and accurate on
 nonlinear datasets. Here we present contextual regression, a method that joins
 these two desirable properties together using a hybrid architecture of neural
 network embedding and dot product layer. We demonstrate its high prediction
 accuracy and sensitivity through the task of predictive feature selection on a
 simulated dataset and the application of predicting open chromatin sites in the
 human genome. On the simulated data, our method achieved high fidelity recovery
 of feature contributions under random noise levels up to 200%. On the open
 chromatin dataset, the application of our method not only outperformed the
 state of the art method in terms of accuracy, but also unveiled two previously
 unfound open chromatin related histone marks. Our method can fill the blank of
 accurate and interpretable nonlinear modeling in scientific data mining tasks.",1,0,0,1,0,0,
137,Multi-time correlators in continuous measurement of qubit observables,"We consider multi-time correlators for output signals from linear detectors,
 continuously measuring several qubit observables at the same time. Using the
 quantum Bayesian formalism, we show that for unital (symmetric) evolution in
 the absence of phase backaction, an $N$-time correlator can be expressed as a
 product of two-time correlators when $N$ is even. For odd $N$, there is a
 similar factorization, which also includes a single-time average. Theoretical
 predictions agree well with experimental results for two detectors, which
 simultaneously measure non-commuting qubit observables.",0,1,0,0,0,0,
138,"Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey","Constraint Handling Rules is an effective concurrent declarative programming
 language and a versatile computational logic formalism. CHR programs consist of
 guarded reactive rules that transform multisets of constraints. One of the main
 features of CHR is its inherent concurrency. Intuitively, rules can be applied
 to parts of a multiset in parallel. In this comprehensive survey, we give an
 overview of concurrent and parallel as well as distributed CHR semantics,
 standard and more exotic, that have been proposed over the years at various
 levels of refinement. These semantics range from the abstract to the concrete.
 They are related by formal soundness results. Their correctness is established
 as correspondence between parallel and sequential computations. We present
 common concise sample CHR programs that have been widely used in experiments
 and benchmarks. We review parallel CHR implementations in software and
 hardware. The experimental results obtained show a consistent parallel speedup.
 Most implementations are available online. The CHR formalism can also be used
 to implement and reason with models for concurrency. To this end, the Software
 Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus
 have been faithfully encoded in CHR. Under consideration in Theory and Practice
 of Logic Programming (TPLP).",1,0,0,0,0,0,
139,Robustness against the channel effect in pathological voice detection,"Many people are suffering from voice disorders, which can adversely affect
 the quality of their lives. In response, some researchers have proposed
 algorithms for automatic assessment of these disorders, based on voice signals.
 However, these signals can be sensitive to the recording devices. Indeed, the
 channel effect is a pervasive problem in machine learning for healthcare. In
 this study, we propose a detection system for pathological voice, which is
 robust against the channel effect. This system is based on a bidirectional LSTM
 network. To increase the performance robustness against channel mismatch, we
 integrate domain adversarial training (DAT) to eliminate the differences
 between the devices. When we train on data recorded on a high-quality
 microphone and evaluate on smartphone data without labels, our robust detection
 system increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target
 sample labels). To the best of our knowledge, this is the first study applying
 unsupervised domain adaptation to pathological voice detection. Notably, our
 system does not need target device sample labels, which allows for
 generalization to many new devices.",1,0,0,0,0,0,
140,An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers,"Computing a basis for the exponent lattice of algebraic numbers is a basic
 problem in the field of computational number theory with applications to many
 other areas. The main cost of a well-known algorithm
 \cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
 computing the primitive element of the extended field generated by the given
 algebraic numbers. When the extended field is of large degree, the problem
 seems intractable by the tool implementing the algorithm. In this paper, a
 special kind of exponent lattice basis is introduced. An important feature of
 the basis is that it can be inductively constructed, which allows us to deal
 with the given algebraic numbers one by one when computing the basis. Based on
 this, an effective framework for constructing exponent lattice basis is
 proposed. Through computing a so-called pre-basis first and then solving some
 linear Diophantine equations, the basis can be efficiently constructed. A new
 certificate for multiplicative independence and some techniques for decreasing
 degrees of algebraic numbers are provided to speed up the computation. The new
 algorithm has been implemented with Mathematica and its effectiveness is
 verified by testing various examples. Moreover, the algorithm is applied to
 program verification for finding invariants of linear loops.",1,0,0,0,0,0,
141,Competing evolutionary paths in growing populations with applications to multidrug resistance,"Investigating the emergence of a particular cell type is a recurring theme in
 models of growing cellular populations. The evolution of resistance to therapy
 is a classic example. Common questions are: when does the cell type first
 occur, and via which sequence of steps is it most likely to emerge? For growing
 populations, these questions can be formulated in a general framework of
 branching processes spreading through a graph from a root to a target vertex.
 Cells have a particular fitness value on each vertex and can transition along
 edges at specific rates. Vertices represents cell states, say \mic{genotypes
 }or physical locations, while possible transitions are acquiring a mutation or
 cell migration. We focus on the setting where cells at the root vertex have the
 highest fitness and transition rates are small. Simple formulas are derived for
 the time to reach the target vertex and for the probability that it is reached
 along a given path in the graph. We demonstrate our results on \mic{several
 scenarios relevant to the emergence of drug resistance}, including: the
 orderings of resistance-conferring mutations in bacteria and the impact of
 imperfect drug penetration in cancer.",0,0,0,0,1,0,
142,Transient flows in active porous media,"Stimuli-responsive materials that modify their shape in response to changes
 in environmental conditions -- such as solute concentration, temperature, pH,
 and stress -- are widespread in nature and technology. Applications include
 micro- and nanoporous materials used in filtration and flow control. The
 physiochemical mechanisms that induce internal volume modifications have been
 widely studies. The coupling between induced volume changes and solute
 transport through porous materials, however, is not well understood. Here, we
 consider advective and diffusive transport through a small channel linking two
 large reservoirs. A section of stimulus-responsive material regulates the
 channel permeability, which is a function of the local solute concentration. We
 derive an exact solution to the coupled transport problem and demonstrate the
 existence of a flow regime in which the steady state is reached via a damped
 oscillation around the equilibrium concentration value. Finally, the
 feasibility of an experimental observation of the phenomena is discussed.
 Please note that this version of the paper has not been formally peer reviewed,
 revised or accepted by a journal.",0,1,0,0,0,0,
143,An information model for modular robots: the Hardware Robot Information Model (HRIM),"Today's landscape of robotics is dominated by vertical integration where
 single vendors develop the final product leading to slow progress, expensive
 products and customer lock-in. Opposite to this, an horizontal integration
 would result in a rapid development of cost-effective mass-market products with
 an additional consumer empowerment. The transition of an industry from vertical
 integration to horizontal integration is typically catalysed by de facto
 industry standards that enable a simplified and seamless integration of
 products. However, in robotics there is currently no leading candidate for a
 global plug-and-play standard.
 This paper tackles the problem of incompatibility between robot components
 that hinder the reconfigurability and flexibility demanded by the robotics
 industry. Particularly, it presents a model to create plug-and-play robot
 hardware components. Rather than iteratively evolving previous ontologies, our
 proposed model answers the needs identified by the industry while facilitating
 interoperability, measurability and comparability of robotics technology. Our
 approach differs significantly with the ones presented before as it is
 hardware-oriented and establishes a clear set of actions towards the
 integration of this model in real environments and with real manufacturers.",1,0,0,0,0,0,
144,Detecting Adversarial Samples Using Density Ratio Estimates,"Machine learning models, especially based on deep architectures are used in
 everyday applications ranging from self driving cars to medical diagnostics. It
 has been shown that such models are dangerously susceptible to adversarial
 samples, indistinguishable from real samples to human eye, adversarial samples
 lead to incorrect classifications with high confidence. Impact of adversarial
 samples is far-reaching and their efficient detection remains an open problem.
 We propose to use direct density ratio estimation as an efficient model
 agnostic measure to detect adversarial samples. Our proposed method works
 equally well with single and multi-channel samples, and with different
 adversarial sample generation methods. We also propose a method to use density
 ratio estimates for generating adversarial samples with an added constraint of
 preserving density ratio.",1,0,0,1,0,0,
145,The Query Complexity of Cake Cutting,"We study the query complexity of cake cutting and give lower and upper bounds
 for computing approximately envy-free, perfect, and equitable allocations with
 the minimum number of cuts. The lower bounds are tight for computing connected
 envy-free allocations among n=3 players and for computing perfect and equitable
 allocations with minimum number of cuts between n=2 players.
 We also formalize moving knife procedures and show that a large subclass of
 this family, which captures all the known moving knife procedures, can be
 simulated efficiently with arbitrarily small error in the Robertson-Webb query
 model.",1,0,0,0,0,0,
146,Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition,"This paper studies the emotion recognition from musical tracks in the
 2-dimensional valence-arousal (V-A) emotional space. We propose a method based
 on convolutional (CNN) and recurrent neural networks (RNN), having
 significantly fewer parameters compared with the state-of-the-art method for
 the same task. We utilize one CNN layer followed by two branches of RNNs
 trained separately for arousal and valence. The method was evaluated using the
 'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for
 arousal and 0.268 for valence, which is the best result reported on this
 dataset.",1,0,0,0,0,0,
147,Timed Automata with Polynomial Delay and their Expressiveness,"We consider previous models of Timed, Probabilistic and Stochastic Timed
 Automata, we introduce our model of Timed Automata with Polynomial Delay and we
 characterize the expressiveness of these models relative to each other.",1,0,0,0,0,0,
148,Superconducting properties of Cu intercalated Bi$_2$Se$_3$ studied by Muon Spin Spectroscopy,"We present muon spin rotation measurements on superconducting Cu intercalated
 Bi$_2$Se$_3$, which was suggested as a realization of a topological
 superconductor. We observe a clear evidence of the superconducting transition
 below 4 K, where the width of magnetic field distribution increases as the
 temperature is decreased. The measured broadening at mK temperatures suggests a
 large London penetration depth in the $ab$ plane ($\lambda_{\mathrm{eff}}\sim
 1.6$ $\mathrm{\mu}$m). We show that the temperature dependence of this
 broadening follows the BCS prediction, but could be consistent with several gap
 symmetries.",0,1,0,0,0,0,
149,Time-domain THz spectroscopy reveals coupled protein-hydration dielectric response in solutions of native and fibrils of human lyso-zyme,"Here we reveal details of the interaction between human lysozyme proteins,
 both native and fibrils, and their water environment by intense terahertz time
 domain spectroscopy. With the aid of a rigorous dielectric model, we determine
 the amplitude and phase of the oscillating dipole induced by the THz field in
 the volume containing the protein and its hydration water. At low
 concentrations, the amplitude of this induced dipolar response decreases with
 increasing concentration. Beyond a certain threshold, marking the onset of the
 interactions between the extended hydration shells, the amplitude remains fixed
 but the phase of the induced dipolar response, which is initially in phase with
 the applied THz field, begins to change. The changes observed in the THz
 response reveal protein-protein interactions me-diated by extended hydration
 layers, which may control fibril formation and may have an important role in
 chemical recognition phenomena.",0,1,0,0,0,0,
150,Inversion of Qubit Energy Levels in Qubit-Oscillator Circuits in the Deep-Strong-Coupling Regime,"We report on experimentally measured light shifts of superconducting flux
 qubits deep-strongly coupled to LC oscillators, where the coupling constants
 are comparable to the qubit and oscillator resonance frequencies. By using
 two-tone spectroscopy, the energies of the six lowest levels of each circuit
 are determined. We find huge Lamb shifts that exceed 90% of the bare qubit
 frequencies and inversions of the qubits' ground and excited states when there
 are a finite number of photons in the oscillator. Our experimental results
 agree with theoretical predictions based on the quantum Rabi model.",0,1,0,0,0,0,
151,Deep Multiple Instance Feature Learning via Variational Autoencoder,"We describe a novel weakly supervised deep learning framework that combines
 both the discriminative and generative models to learn meaningful
 representation in the multiple instance learning (MIL) setting. MIL is a weakly
 supervised learning problem where labels are associated with groups of
 instances (referred as bags) instead of individual instances. To address the
 essential challenge in MIL problems raised from the uncertainty of positive
 instances label, we use a discriminative model regularized by variational
 autoencoders (VAEs) to maximize the differences between latent representations
 of all instances and negative instances. As a result, the hidden layer of the
 variational autoencoder learns meaningful representation. This representation
 can effectively be used for MIL problems as illustrated by better performance
 on the standard benchmark datasets comparing to the state-of-the-art
 approaches. More importantly, unlike most related studies, the proposed
 framework can be easily scaled to large dataset problems, as illustrated by the
 audio event detection and segmentation task. Visualization also confirms the
 effectiveness of the latent representation in discriminating positive and
 negative classes.",0,0,0,1,0,0,
152,Regularity of envelopes in K??hler classes,"We establish the C^{1,1} regularity of quasi-psh envelopes in a Kahler class,
 confirming a conjecture of Berman.",0,0,1,0,0,0,
153,$S^1$-equivariant Index theorems and Morse inequalities on complex manifolds with boundary,"Let $M$ be a complex manifold of dimension $n$ with smooth connected boundary
 $X$. Assume that $\overline M$ admits a holomorphic $S^1$-action preserving the
 boundary $X$ and the $S^1$-action is transversal and CR on $X$. We show that
 the $\overline\partial$-Neumann Laplacian on $M$ is transversally elliptic and
 as a consequence, the $m$-th Fourier component of the $q$-th Dolbeault
 cohomology group $H^q_m(\overline M)$ is finite dimensional, for every
 $m\in\mathbb Z$ and every $q=0,1,\ldots,n$. This enables us to define
 $\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ the $m$-th Fourier
 component of the Euler characteristic on $M$ and to study large $m$-behavior of
 $H^q_m(\overline M)$. In this paper, we establish an index formula for
 $\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ and Morse inequalities for
 $H^q_m(\overline M)$.",0,0,1,0,0,0,
154,Internal Model from Observations for Reward Shaping,"Reinforcement learning methods require careful design involving a reward
 function to obtain the desired action policy for a given task. In the absence
 of hand-crafted reward functions, prior work on the topic has proposed several
 methods for reward estimation by using expert state trajectories and action
 pairs. However, there are cases where complete or good action information
 cannot be obtained from expert demonstrations. We propose a novel reinforcement
 learning method in which the agent learns an internal model of observation on
 the basis of expert-demonstrated state trajectories to estimate rewards without
 completely learning the dynamics of the external environment from state-action
 pairs. The internal model is obtained in the form of a predictive model for the
 given expert state distribution. During reinforcement learning, the agent
 predicts the reward as a function of the difference between the actual state
 and the state predicted by the internal model. We conducted multiple
 experiments in environments of varying complexity, including the Super Mario
 Bros and Flappy Bird games. We show our method successfully trains good
 policies directly from expert game-play videos.",1,0,0,1,0,0,
155,Characterizations of quasitrivial symmetric nondecreasing associative operations,"In this paper we are interested in the class of n-ary operations on an
 arbitrary chain that are quasitrivial, symmetric, nondecreasing, and
 associative. We first provide a description of these operations. We then prove
 that associativity can be replaced with bisymmetry in the definition of this
 class. Finally we investigate the special situation where the chain is finite.",0,0,1,0,0,0,
156,Multivariate Dependency Measure based on Copula and Gaussian Kernel,"We propose a new multivariate dependency measure. It is obtained by
 considering a Gaussian kernel based distance between the copula transform of
 the given d-dimensional distribution and the uniform copula and then
 appropriately normalizing it. The resulting measure is shown to satisfy a
 number of desirable properties. A nonparametric estimate is proposed for this
 dependency measure and its properties (finite sample as well as asymptotic) are
 derived. Some comparative studies of the proposed dependency measure estimate
 with some widely used dependency measure estimates on artificial datasets are
 included. A non-parametric test of independence between two or more random
 variables based on this measure is proposed. A comparison of the proposed test
 with some existing nonparametric multivariate test for independence is
 presented.",0,0,1,1,0,0,
157,The nature of the tensor order in Cd2Re2O7,"The pyrochlore metal Cd2Re2O7 has been recently investigated by
 second-harmonic generation (SHG) reflectivity. In this paper, we develop a
 general formalism that allows for the identification of the relevant tensor
 components of the SHG from azimuthal scans. We demonstrate that the secondary
 order parameter identified by SHG at the structural phase transition is the
 x2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2
 symmetry of the atomic displacements associated with the I-4m2 crystal
 structure that was previously thought to be its origin. Within the same
 formalism, we suggest that the primary order parameter detected in the SHG
 experiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the
 general mechanism driving the phase transition in our proposed framework, and
 suggest experiments, particularly resonant X-ray scattering ones, that could
 clarify this issue.",0,1,0,0,0,0,
158,Efficient and consistent inference of ancestral sequences in an evolutionary model with insertions and deletions under dense taxon sampling,"In evolutionary biology, the speciation history of living organisms is
 represented graphically by a phylogeny, that is, a rooted tree whose leaves
 correspond to current species and branchings indicate past speciation events.
 Phylogenies are commonly estimated from molecular sequences, such as DNA
 sequences, collected from the species of interest. At a high level, the idea
 behind this inference is simple: the further apart in the Tree of Life are two
 species, the greater is the number of mutations to have accumulated in their
 genomes since their most recent common ancestor. In order to obtain accurate
 estimates in phylogenetic analyses, it is standard practice to employ
 statistical approaches based on stochastic models of sequence evolution on a
 tree. For tractability, such models necessarily make simplifying assumptions
 about the evolutionary mechanisms involved. In particular, commonly omitted are
 insertions and deletions of nucleotides -- also known as indels.
 Properly accounting for indels in statistical phylogenetic analyses remains a
 major challenge in computational evolutionary biology. Here we consider the
 problem of reconstructing ancestral sequences on a known phylogeny in a model
 of sequence evolution incorporating nucleotide substitutions, insertions and
 deletions, specifically the classical TKF91 process. We focus on the case of
 dense phylogenies of bounded height, which we refer to as the taxon-rich
 setting, where statistical consistency is achievable. We give the first
 polynomial-time ancestral reconstruction algorithm with provable guarantees
 under constant rates of mutation. Our algorithm succeeds when the phylogeny
 satisfies the ""big bang"" condition, a necessary and sufficient condition for
 statistical consistency in this context.",1,0,1,1,0,0,
159,Flow Characteristics and Cores of Complex Network and Multiplex Type Systems,"Subject of research is complex networks and network systems. The network
 system is defined as a complex network in which flows are moved. Classification
 of flows in the network is carried out on the basis of ordering and continuity.
 It is shown that complex networks with different types of flows generate
 various network systems. Flow analogues of the basic concepts of the theory of
 complex networks are introduced and the main problems of this theory in terms
 of flow characteristics are formulated. Local and global flow characteristics
 of networks bring closer the theory of complex networks to the systems theory
 and systems analysis. Concept of flow core of network system is introduced and
 defined how it simplifies the process of its investigation. Concepts of kernel
 and flow core of multiplex are determined. Features of operation of multiplex
 type systems are analyzed.",1,1,0,0,0,0,
160,Pattern-forming fronts in a Swift-Hohenberg equation with directional quenching - parallel and oblique stripes,"We study the effect of domain growth on the orientation of striped phases in
 a Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter
 dependence that allows stripe formation in a half plane, and suppresses
 patterns in the complement, while the boundary of the pattern-forming region is
 propagating with fixed normal velocity. We construct front solutions that leave
 behind stripes in the pattern-forming region that are parallel to or at a small
 oblique angle to the boundary.
 Technically, the construction of stripe formation parallel to the boundary
 relies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at
 a small oblique angle are constructed using a functional-analytic, perturbative
 approach. Here, the main difficulties are the presence of continuous spectrum
 and the fact that small oblique angles appear as a singular perturbation in a
 traveling-wave problem. We resolve the former difficulty using a farfield-core
 decomposition and Fredholm theory in weighted spaces. The singular perturbation
 problem is resolved using preconditioners and boot-strapping.",0,1,0,0,0,0,
161,Generalized Minimum Distance Estimators in Linear Regression with Dependent Errors,"This paper discusses minimum distance estimation method in the linear
 regression model with dependent errors which are strongly mixing. The
 regression parameters are estimated through the minimum distance estimation
 method, and asymptotic distributional properties of the estimators are
 discussed. A simulation study compares the performance of the minimum distance
 estimator with other well celebrated estimator. This simulation study shows the
 superiority of the minimum distance estimator over another estimator. KoulMde
 (R package) which was used for the simulation study is available online. See
 section 4 for the detail.",0,0,1,1,0,0,
162,Live Service Migration in Mobile Edge Clouds,"Mobile edge clouds (MECs) bring the benefits of the cloud closer to the user,
 by installing small cloud infrastructures at the network edge. This enables a
 new breed of real-time applications, such as instantaneous object recognition
 and safety assistance in intelligent transportation systems, that require very
 low latency. One key issue that comes with proximity is how to ensure that
 users always receive good performance as they move across different locations.
 Migrating services between MECs is seen as the means to achieve this. This
 article presents a layered framework for migrating active service applications
 that are encapsulated either in virtual machines (VMs) or containers. This
 layering approach allows a substantial reduction in service downtime. The
 framework is easy to implement using readily available technologies, and one of
 its key advantages is that it supports containers, which is a promising
 emerging technology that offers tangible benefits over VMs. The migration
 performance of various real applications is evaluated by experiments under the
 presented framework. Insights drawn from the experimentation results are
 discussed.",1,0,0,0,0,0,
163,Induced density correlations in a sonic black hole condensate,"Analog black/white hole pairs, consisting of a region of supersonic flow,
 have been achieved in a recent experiment by J. Steinhauer using an elongated
 Bose-Einstein condensate. A growing standing density wave, and a checkerboard
 feature in the density-density correlation function, were observed in the
 supersonic region. We model the density-density correlation function, taking
 into account both quantum fluctuations and the shot-to-shot variation of atom
 number normally present in ultracold-atom experiments. We find that quantum
 fluctuations alone produce some, but not all, of the features of the
 correlation function, whereas atom-number fluctuation alone can produce all the
 observed features, and agreement is best when both are included. In both cases,
 the density-density correlation is not intrinsic to the fluctuations, but
 rather is induced by modulation of the standing wave caused by the
 fluctuations.",0,1,0,0,0,0,
164,Genus growth in $\mathbb{Z}_p$-towers of function fields,"Let $K$ be a function field over a finite field $k$ of characteristic $p$ and
 let $K_{\infty}/K$ be a geometric extension with Galois group $\mathbb{Z}_p$.
 Let $K_n$ be the corresponding subextension with Galois group
 $\mathbb{Z}/p^n\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple
 explicit formula $g_n$ in terms of an explicit Witt vector construction of the
 $\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which
 is quadratic in $p^n$. Furthermore, we determine all $\mathbb{Z}_p$-towers for
 which the genus sequence is stable, in the sense that there are $a,b,c \in
 \mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus
 stable towers are expected to have strong stable arithmetic properties for
 their zeta functions. A key technical contribution of this work is a new
 simplified formula for the Schmid-Witt symbol coming from local class field
 theory.",0,0,1,0,0,0,
165,Topological Phases emerging from Spin-Orbital Physics,"We study the evolution of spin-orbital correlations in an inhomogeneous
 quantum system with an impurity replacing a doublon by a holon orbital degree
 of freedom. Spin-orbital entanglement is large when spin correlations are
 antiferromagnetic, while for a ferromagnetic host we obtain a pure orbital
 description. In this regime the orbital model can be mapped on spinless
 fermions and we uncover topological phases with zero energy modes at the edge
 or at the domain between magnetically inequivalent regions.",0,1,0,0,0,0,
166,"Accurate and Diverse Sampling of Sequences based on a ""Best of Many"" Sample Objective","For autonomous agents to successfully operate in the real world, anticipation
 of future events and states of their environment is a key competence. This
 problem has been formalized as a sequence extrapolation problem, where a number
 of observations are used to predict the sequence into the future. Real-world
 scenarios demand a model of uncertainty of such predictions, as predictions
 become increasingly uncertain -- in particular on long time horizons. While
 impressive results have been shown on point estimates, scenarios that induce
 multi-modal distributions over future sequences remain challenging. Our work
 addresses these challenges in a Gaussian Latent Variable model for sequence
 prediction. Our core contribution is a ""Best of Many"" sample objective that
 leads to more accurate and more diverse predictions that better capture the
 true variations in real-world sequence data. Beyond our analysis of improved
 model fit, our models also empirically outperform prior work on three diverse
 tasks ranging from traffic scenes to weather data.",0,0,0,1,0,0,
167,Exploring RNN-Transducer for Chinese Speech Recognition,"End-to-end approaches have drawn much attention recently for significantly
 simplifying the construction of an automatic speech recognition (ASR) system.
 RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous
 studies have shown that RNN-T is difficult to train and a very complex training
 process is needed for a reasonable performance. In this paper, we explore RNN-T
 for a Chinese large vocabulary continuous speech recognition (LVCSR) task and
 aim to simplify the training process while maintaining performance. First, a
 new strategy of learning rate decay is proposed to accelerate the model
 convergence. Second, we find that adding convolutional layers at the beginning
 of the network and using ordered data can discard the pre-training process of
 the encoder without loss of performance. Besides, we design experiments to find
 a balance among the usage of GPU memory, training circle and model performance.
 Finally, we achieve 16.9% character error rate (CER) on our test set which is
 2% absolute improvement from a strong BLSTM CE system with language model
 trained on the same text corpus.",1,0,0,0,0,0,
168,A Debt-Aware Learning Approach for Resource Adaptations in Cloud Elasticity Management,"Elasticity is a cloud property that enables applications and its execution
 systems to dynamically acquire and release shared computational resources on
 demand. Moreover, it unfolds the advantage of economies of scale in the cloud
 through a drop in the average costs of these shared resources. However, it is
 still an open challenge to achieve a perfect match between resource demand and
 provision in autonomous elasticity management. Resource adaptation decisions
 essentially involve a trade-off between economics and performance, which
 produces a gap between the ideal and actual resource provisioning. This gap, if
 not properly managed, can negatively impact the aggregate utility of a cloud
 customer in the long run. To address this limitation, we propose a technical
 debt-aware learning approach for autonomous elasticity management based on a
 reinforcement learning of elasticity debts in resource provisioning; the
 adaptation pursues strategic decisions that trades off economics against
 performance. We extend CloudSim and Burlap to evaluate our approach. The
 evaluation shows that a reinforcement learning of technical debts in elasticity
 obtains a higher utility for a cloud customer, while conforming expected levels
 of performance.",1,0,0,0,0,0,
169,Semi-simplicial spaces,"This is an exposition of homotopical results on the geometric realization of
 semi-simplicial spaces. We then use these to derive basic foundational results
 about classifying spaces of topological categories, possibly without units. The
 topics considered include: fibrancy conditions on topological categories; the
 effect on classifying spaces of freely adjoining units; approximate notions of
 units; Quillen's Theorems A and B for non-unital topological categories; the
 effect on classifying spaces of changing the topology on the space of objects;
 the Group-Completion Theorem.",0,0,1,0,0,0,
170,"Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis","Answer Set Programming (ASP) is a well-established declarative paradigm. One
 of the successes of ASP is the availability of efficient systems.
 State-of-the-art systems are based on the ground+solve approach. In some
 applications this approach is infeasible because the grounding of one or few
 constraints is expensive. In this paper, we systematically compare alternative
 strategies to avoid the instantiation of problematic constraints, that are
 based on custom extensions of the solver. Results on real and synthetic
 benchmarks highlight some strengths and weaknesses of the different strategies.
 (Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)",1,0,0,0,0,0,
171,A Unified Approach to Nonlinear Transformation Materials,"The advances in geometric approaches to optical devices due to transformation
 optics has led to the development of cloaks, concentrators, and other devices.
 It has also been shown that transformation optics can be used to gravitational
 fields from general relativity. However, the technique is currently constrained
 to linear devices, as a consistent approach to nonlinearity (including both the
 case of a nonlinear background medium and a nonlinear transformation) remains
 an open question. Here we show that nonlinearity can be incorporated into
 transformation optics in a consistent way. We use this to illustrate a number
 of novel effects, including cloaking an optical soliton, modeling nonlinear
 solutions to Einstein's field equations, controlling transport in a Debye
 solid, and developing a set of constitutive to relations for relativistic
 cloaks in arbitrary nonlinear backgrounds.",0,1,0,0,0,0,
172,Stationary crack propagation in a two-dimensional visco-elastic network model,"We investigate crack propagation in a simple two-dimensional visco-elastic
 model and find a scaling regime in the relation between the propagation
 velocity and energy release rate or fracture energy, together with lower and
 upper bounds of the scaling regime. On the basis of our result, the existence
 of the lower and upper bounds is expected to be universal or model-independent:
 the present simple simulation model provides generic insight into the physics
 of crack propagation, and the model will be a first step towards the
 development of a more refined coarse-grained model. Relatively abrupt changes
 of velocity are predicted near the lower and upper bounds for the scaling
 regime and the positions of the bounds could be good markers for the
 development of tough polymers, for which we provide simple views that could be
 useful as guiding principles for toughening polymer-based materials.",0,1,0,0,0,0,
173,A note on the fundamental group of Kodaira fibrations,"The fundamental group $\pi$ of a Kodaira fibration is, by definition, the
 extension of a surface group $\Pi_b$ by another surface group $\Pi_g$, i.e. \[
 1 \rightarrow \Pi_g \rightarrow \pi \rightarrow \Pi_b \rightarrow 1. \]
 Conversely, we can inquire about what conditions need to be satisfied by a
 group of that sort in order to be the fundamental group of a Kodaira fibration.
 In this short note we collect some restriction on the image of the classifying
 map $m \colon \Pi_b \to \Gamma_g$ in terms of the coinvariant homology of
 $\Pi_g$. In particular, we observe that if $\pi$ is the fundamental group of a
 Kodaira fibration with relative irregularity $g-s$, then $g \leq 1+ 6s$, and we
 show that this effectively constrains the possible choices for $\pi$, namely
 that there are group extensions as above that fail to satisfy this bound, hence
 cannot be the fundamental group of a Kodaira fibration. In particular this
 provides examples of symplectic $4$--manifolds that fail to admit a K??hler
 structure for reasons that eschew the usual obstructions.",0,0,1,0,0,0,
174,Photo-Chemically Directed Self-Assembly of Carbon Nanotubes on Surfaces,"Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel
 material are used in a variety of electronics applications. However, a
 competitive CNT-based technology requires the precise placement of CNTs at
 predefined locations of a substrate. One promising placement approach is to use
 chemical recognition to bind CNTs from solution at the desired locations on a
 surface. Producing the chemical pattern on the substrate is challenging. Here
 we describe a one-step patterning approach based on a highly photosensitive
 surface monolayer. The monolayer contains chromophopric group as light
 sensitive body with heteroatoms as high quantum yield photolysis center. As
 deposited, the layer will bind CNTs from solution. However, when exposed to
 ultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for
 conventional photoresists, the monolayer cleaves and no longer binds CNTs.
 These features allow standard, wafer-scale UV lithography processes to be used
 to form a patterned chemical monolayer without the need for complex substrate
 patterning or monolayer stamping.",0,1,0,0,0,0,
175,Split-and-augmented Gibbs sampler - Application to large-scale inference problems,"This paper derives two new optimization-driven Monte Carlo algorithms
 inspired from variable splitting and data augmentation. In particular, the
 formulation of one of the proposed approaches is closely related to the
 alternating direction method of multipliers (ADMM) main steps. The proposed
 framework enables to derive faster and more efficient sampling schemes than the
 current state-of-the-art methods and can embed the latter. By sampling
 efficiently the parameter to infer as well as the hyperparameters of the
 problem, the generated samples can be used to approximate Bayesian estimators
 of the parameters to infer. Additionally, the proposed approach brings
 confidence intervals at a low cost contrary to optimization methods.
 Simulations on two often-studied signal processing problems illustrate the
 performance of the two proposed samplers. All results are compared to those
 obtained by recent state-of-the-art optimization and MCMC algorithms used to
 solve these problems.",0,0,0,1,0,0,
176,Does a generalized Chaplygin gas correctly describe the cosmological dark sector?,"Yes, but only for a parameter value that makes it almost coincide with the
 standard model. We reconsider the cosmological dynamics of a generalized
 Chaplygin gas (gCg) which is split into a cold dark matter (CDM) part and a
 dark energy (DE) component with constant equation of state. This model, which
 implies a specific interaction between CDM and DE, has a $\Lambda$CDM limit and
 provides the basis for studying deviations from the latter. Including matter
 and radiation, we use the (modified) CLASS code \cite{class} to construct the
 CMB and matter power spectra in order to search for a gCg-based concordance
 model that is in agreement with the SNIa data from the JLA sample and with
 recent Planck data. The results reveal that the gCg parameter $\alpha$ is
 restricted to $|\alpha|\lesssim 0.05$, i.e., to values very close to the
 $\Lambda$CDM limit $\alpha =0$. This excludes, in particular, models in which
 DE decays linearly with the Hubble rate.",0,1,0,0,0,0,
177,The effects of subdiffusion on the NTA size measurements of extracellular vesicles in biological samples,"The interest in the extracellular vesicles (EVs) is rapidly growing as they
 became reliable biomarkers for many diseases. For this reason, fast and
 accurate techniques of EVs size characterization are the matter of utmost
 importance. One increasingly popular technique is the Nanoparticle Tracking
 Analysis (NTA), in which the diameters of EVs are calculated from their
 diffusion constants. The crucial assumption here is that the diffusion in NTA
 follows the Stokes-Einstein relation, i.e. that the Mean Square Displacement
 (MSD) of a particle grows linearly in time (MSD $\propto t$). However, we show
 that NTA violates this assumption in both artificial and biological samples,
 i.e. a large population of particles show a strongly sub-diffusive behaviour
 (MSD $\propto t^\alpha$, $0<\alpha<1$). To support this observation we present
 a range of experimental results for both polystyrene beads and EVs. This is
 also related to another problem: for the same samples there exists a huge
 discrepancy (by the factor of 2-4) between the sizes measured with NTA and with
 the direct imaging methods, such as AFM. This can be remedied by e.g. the
 Finite Track Length Adjustment (FTLA) method in NTA, but its applicability is
 limited in the biological and poly-disperse samples. On the other hand, the
 models of sub-diffusion rarely provide the direct relation between the size of
 a particle and the generalized diffusion constant. However, we solve this last
 problem by introducing the logarithmic model of sub-diffusion, aimed at
 retrieving the size data. In result, we propose a novel protocol of NTA data
 analysis. The accuracy of our method is on par with FTLA for small
 ($\simeq$200nm) particles. We apply our method to study the EVs samples and
 corroborate the results with AFM.",0,1,0,0,0,0,
178,Empirical regression quantile process with possible application to risk analysis,"The processes of the averaged regression quantiles and of their modifications
 provide useful tools in the regression models when the covariates are not fully
 under our control. As an application we mention the probabilistic risk
 assessment in the situation when the return depends on some exogenous
 variables. The processes enable to evaluate the expected $\alpha$-shortfall
 ($0\leq\alpha\leq 1$) and other measures of the risk, recently generally
 accepted in the financial literature, but also help to measure the risk in
 environment analysis and elsewhere.",0,0,1,1,0,0,
179,Primordial perturbations from inflation with a hyperbolic field-space,"We study primordial perturbations from hyperinflation, proposed recently and
 based on a hyperbolic field-space. In the previous work, it was shown that the
 field-space angular momentum supported by the negative curvature modifies the
 background dynamics and enhances fluctuations of the scalar fields
 qualitatively, assuming that the inflationary background is almost de Sitter.
 In this work, we confirm and extend the analysis based on the standard approach
 of cosmological perturbation in multi-field inflation. At the background level,
 to quantify the deviation from de Sitter, we introduce the slow-varying
 parameters and show that steep potentials, which usually can not drive
 inflation, can drive inflation. At the linear perturbation level, we obtain the
 power spectrum of primordial curvature perturbation and express the spectral
 tilt and running in terms of the slow-varying parameters. We show that
 hyperinflation with power-law type potentials has already been excluded by the
 recent Planck observations, while exponential-type potential with the exponent
 of order unity can be made consistent with observations as far as the power
 spectrum is concerned. We also argue that, in the context of a simple $D$-brane
 inflation, the hyperinflation requires exponentially large hyperbolic extra
 dimensions but that masses of Kaluza-Klein gravitons can be kept relatively
 heavy.",0,1,0,0,0,0,
180,Role of Vanadyl Oxygen in Understanding Metallic Behavior of V2O5(001) Nanorods,"Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,
 exhibits interesting semiconductor to metal transition in the temperature range
 of 530-560 K. The metallic behavior originates because of the reduction of V2O5
 through oxygen vacancies. In the present report, V2O5 nanorods in the
 orthorhombic phase with crystal orientation of (001) are grown using vapor
 transport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal
 formula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the
 formation of metallic phase above the transition temperature is established
 from the temperature-dependent Raman spectroscopic studies. The origin of the
 metallic behavior of V2O5 is also understood due to the breakdown of pdpi bond
 between OI and nearest V atom instigated by the formation of vanadyl OI
 vacancy, confirmed from the downward shift of the bottom most split-off
 conduction bands in the material with increasing temperature.",0,1,0,0,0,0,
181,Graph Convolution: A High-Order and Adaptive Approach,"In this paper, we presented a novel convolutional neural network framework
 for graph modeling, with the introduction of two new modules specially designed
 for graph-structured data: the $k$-th order convolution operator and the
 adaptive filtering module. Importantly, our framework of High-order and
 Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed
 architecture that fits various applications on both node and graph centrics, as
 well as graph generative models. We conducted extensive experiments on
 demonstrating the advantages of our framework. Particularly, our HA-GCN
 outperforms the state-of-the-art models on node classification and molecule
 property prediction tasks. It also generates 32% more real molecules on the
 molecule generation task, both of which will significantly benefit real-world
 applications such as material design and drug screening.",1,0,0,1,0,0,
182,Learning Sparse Representations in Reinforcement Learning with Sparse Coding,"A variety of representation learning approaches have been investigated for
 reinforcement learning; much less attention, however, has been given to
 investigating the utility of sparse coding. Outside of reinforcement learning,
 sparse coding representations have been widely used, with non-convex objectives
 that result in discriminative representations. In this work, we develop a
 supervised sparse coding objective for policy evaluation. Despite the
 non-convexity of this objective, we prove that all local minima are global
 minima, making the approach amenable to simple optimization strategies. We
 empirically show that it is key to use a supervised objective, rather than the
 more straightforward unsupervised sparse coding approach. We compare the
 learned representations to a canonical fixed sparse representation, called
 tile-coding, demonstrating that the sparse coding representation outperforms a
 wide variety of tilecoding representations.",1,0,0,1,0,0,
183,Almost euclidean Isoperimetric Inequalities in spaces satisfying local Ricci curvature lower bounds,"Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove
 that if a Riemannian manifold has Ricci curvature bounded below in a metric
 ball which moreover has almost maximal volume, then in a smaller ball (in a
 quantified sense) it holds an almost-euclidean isoperimetric inequality. The
 result is actually established in the more general framework of non-smooth
 spaces satisfying local Ricci curvature lower bounds in a synthetic sense via
 optimal transportation.",0,0,1,0,0,0,
184,Exponential Sums and Riesz energies,"We bound an exponential sum that appears in the study of irregularities of
 distribution (the low-frequency Fourier energy of the sum of several Dirac
 measures) by geometric quantities: a special case is that for all $\left\{ x_1,
 \dots, x_N\right\} \subset \mathbb{T}^2$, $X \geq 1$ and a universal $c>0$ $$
 \sum_{i,j=1}^{N}{ \frac{X^2}{1 + X^4 \|x_i -x_j\|^4}} \lesssim \sum_{k \in
 \mathbb{Z}^2 \atop \|k\| \leq X}{ \left| \sum_{n=1}^{N}{ e^{2 \pi i
 \left\langle k, x_n \right\rangle}}\right|^2} \lesssim \sum_{i,j=1}^{N}{ X^2
 e^{-c X^2\|x_i -x_j\|^2}}.$$ Since this exponential sum is intimately tied to
 rather subtle distribution properties of the points, we obtain nonlocal
 structural statements for near-minimizers of the Riesz-type energy. In the
 regime $X \gtrsim N^{1/2}$ both upper and lower bound match for
 maximally-separated point sets satisfying $\|x_i -x_j\| \gtrsim N^{-1/2}$.",0,0,1,0,0,0,
185,One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice,"We investigate the effect of dimensional crossover in the ground state of the
 antiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular
 lattice that interpolates between the regime of weakly coupled Haldane chains
 ($J^{\prime}\! \!\ll\!\! J$) and the isotropic triangular lattice
 ($J^{\prime}\!\!=\!\!J$). We use the density-matrix renormalization group
 (DMRG) and Schwinger boson theory performed at the Gaussian correction level
 above the saddle-point solution. Our DMRG results show an abrupt transition
 between decoupled spin chains and the spirally ordered regime at
 $(J^{\prime}/J)_c\sim 0.42$, signaled by the sudden closing of the spin gap.
 Coming from the magnetically ordered side, the computation of the spin
 stiffness within Schwinger boson theory predicts the instability of the spiral
 magnetic order toward a magnetically disordered phase with one-dimensional
 features at $(J^{\prime}/J)_c \sim 0.43$. The agreement of these complementary
 methods, along with the strong difference found between the intra- and the
 interchain DMRG short spin-spin correlations; for sufficiently large values of
 the interchain coupling, suggests that the interplay between the quantum
 fluctuations and the dimensional crossover effects gives rise to the
 one-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.",0,1,0,0,0,0,
186,Memory Aware Synapses: Learning what (not) to forget,"Humans can learn in a continuous manner. Old rarely utilized knowledge can be
 overwritten by new incoming information while important, frequently used
 knowledge is prevented from being erased. In artificial learning systems,
 lifelong learning so far has focused mainly on accumulating knowledge over
 tasks and overcoming catastrophic forgetting. In this paper, we argue that,
 given the limited model capacity and the unlimited new information to be
 learned, knowledge has to be preserved or erased selectively. Inspired by
 neuroplasticity, we propose a novel approach for lifelong learning, coined
 Memory Aware Synapses (MAS). It computes the importance of the parameters of a
 neural network in an unsupervised and online manner. Given a new sample which
 is fed to the network, MAS accumulates an importance measure for each parameter
 of the network, based on how sensitive the predicted output function is to a
 change in this parameter. When learning a new task, changes to important
 parameters can then be penalized, effectively preventing important knowledge
 related to previous tasks from being overwritten. Further, we show an
 interesting connection between a local version of our method and Hebb's
 rule,which is a model for the learning process in the brain. We test our method
 on a sequence of object recognition tasks and on the challenging problem of
 learning an embedding for predicting $<$subject, predicate, object$>$ triplets.
 We show state-of-the-art performance and, for the first time, the ability to
 adapt the importance of the parameters based on unlabeled data towards what the
 network needs (not) to forget, which may vary depending on test conditions.",1,0,0,1,0,0,
187,Uniform Spectral Convergence of the Stochastic Galerkin Method for the Linear Semiconductor Boltzmann Equation with Random Inputs and Diffusive Scalings,"In this paper, we study the generalized polynomial chaos (gPC) based
 stochastic Galerkin method for the linear semiconductor Boltzmann equation
 under diffusive scaling and with random inputs from an anisotropic collision
 kernel and the random initial condition. While the numerical scheme and the
 proof of uniform-in-Knudsen-number regularity of the distribution function in
 the random space has been introduced in [Jin-Liu-16'], the main goal of this
 paper is to first obtain a sharper estimate on the regularity of the
 solution-an exponential decay towards its local equilibrium, which then lead to
 the uniform spectral convergence of the stochastic Galerkin method for the
 problem under study.",0,0,1,0,0,0,
188,On Improving the Capacity of Solving Large-scale Wireless Network Design Problems by Genetic Algorithms,"Over the last decade, wireless networks have experienced an impressive growth
 and now play a main role in many telecommunications systems. As a consequence,
 scarce radio resources, such as frequencies, became congested and the need for
 effective and efficient assignment methods arose. In this work, we present a
 Genetic Algorithm for solving large instances of the Power, Frequency and
 Modulation Assignment Problem, arising in the design of wireless networks. To
 our best knowledge, this is the first Genetic Algorithm that is proposed for
 such problem. Compared to previous works, our approach allows a wider
 exploration of the set of power solutions, while eliminating sources of
 numerical problems. The performance of the algorithm is assessed by tests over
 a set of large realistic instances of a Fixed WiMAX Network.",1,0,1,0,0,0,
189,Quasi two-dimensional Fermi surface topography of the delafossite PdRhO$_2$,"We report on a combined study of the de Haas-van Alphen effect and angle
 resolved photoemission spectroscopy on single crystals of the metallic
 delafossite PdRhO$_2$ rounded off by \textit{ab initio} band structure
 calculations. A high sensitivity torque magnetometry setup with SQUID readout
 and synchrotron-based photoemission with a light spot size of
 $~50\,\mu\mathrm{m}$ enabled high resolution data to be obtained from samples
 as small as $150\times100\times20\,(\mu\mathrm{m})^3$. The Fermi surface shape
 is nearly cylindrical with a rounded hexagonal cross section enclosing a
 Luttinger volume of 1.00(1) electrons per formula unit.",0,1,0,0,0,0,
190,A Variational Characterization of R??nyi Divergences,"Atar, Chowdhary and Dupuis have recently exhibited a variational formula for
 exponential integrals of bounded measurable functions in terms of R??nyi
 divergences. We develop a variational characterization of the R??nyi
 divergences between two probability distributions on a measurable sace in terms
 of relative entropies. When combined with the elementary variational formula
 for exponential integrals of bounded measurable functions in terms of relative
 entropy, this yields the variational formula of Atar, Chowdhary and Dupuis as a
 corollary. We also develop an analogous variational characterization of the
 R??nyi divergence rates between two stationary finite state Markov chains in
 terms of relative entropy rates. When combined with Varadhan's variational
 characterization of the spectral radius of square matrices with nonnegative
 entries in terms of relative entropy, this yields an analog of the variational
 formula of Atar, Chowdary and Dupuis in the framework of finite state Markov
 chains.",1,0,1,1,0,0,
191,Interlayer coupling and gate-tunable excitons in transition metal dichalcogenide heterostructures,"Bilayer van der Waals (vdW) heterostructures such as MoS2/WS2 and MoSe2/WSe2
 have attracted much attention recently, particularly because of their type II
 band alignments and the formation of interlayer exciton as the lowest-energy
 excitonic state. In this work, we calculate the electronic and optical
 properties of such heterostructures with the first-principles GW+Bethe-Salpeter
 Equation (BSE) method and reveal the important role of interlayer coupling in
 deciding the excited-state properties, including the band alignment and
 excitonic properties. Our calculation shows that due to the interlayer
 coupling, the low energy excitons can be widely tunable by a vertical gate
 field. In particular, the dipole oscillator strength and radiative lifetime of
 the lowest energy exciton in these bilayer heterostructures is varied by over
 an order of magnitude within a practical external gate field. We also build a
 simple model that captures the essential physics behind this tunability and
 allows the extension of the ab initio results to a large range of electric
 fields. Our work clarifies the physical picture of interlayer excitons in
 bilayer vdW heterostructures and predicts a wide range of gate-tunable
 excited-state properties of 2D optoelectronic devices.",0,1,0,0,0,0,
192,Enumeration of singular varieties with tangency conditions,"We construct the algebraic cobordism theory of bundles and divisors on
 varieties. It has a simple basis (over Q) from projective spaces and its rank
 is equal to the number of Chern numbers. An application of this algebraic
 cobordism theory is the enumeration of singular subvarieties with give tangent
 conditions with a fixed smooth divisor, where the subvariety is the zero locus
 of a section of a vector bundle. We prove that the generating series of numbers
 of such subvarieties gives a homomorphism from the algebraic cobordism group to
 the power series ring. This implies that the enumeration of singular
 subvarieties with tangency conditions is governed by universal polynomials of
 Chern numbers, when the vector bundle is sufficiently ample. This result
 combines and generalizes the Caporaso-Harris recursive formula, Gottsche's
 conjecture, classical De Jonquiere's Formula and node polynomials from tropical
 geometry.",0,0,1,0,0,0,
193,In-home and remote use of robotic body surrogates by people with profound motor deficits,"People with profound motor deficits could perform useful physical tasks for
 themselves by controlling robots that are comparable to the human body. Whether
 this is possible without invasive interfaces has been unclear, due to the
 robot's complexity and the person's limitations. We developed a novel,
 augmented reality interface and conducted two studies to evaluate the extent to
 which it enabled people with profound motor deficits to control robotic body
 surrogates. 15 novice users achieved meaningful improvements on a clinical
 manipulation assessment when controlling the robot in Atlanta from locations
 across the United States. Also, one expert user performed 59 distinct tasks in
 his own home over seven days, including self-care tasks such as feeding. Our
 results demonstrate that people with profound motor deficits can effectively
 control robotic body surrogates without invasive interfaces.",1,0,0,0,0,0,
194,ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information,"Object detection in wide area motion imagery (WAMI) has drawn the attention
 of the computer vision research community for a number of years. WAMI proposes
 a number of unique challenges including extremely small object sizes, both
 sparse and densely-packed objects, and extremely large search spaces (large
 video frames). Nearly all state-of-the-art methods in WAMI object detection
 report that appearance-based classifiers fail in this challenging data and
 instead rely almost entirely on motion information in the form of background
 subtraction or frame-differencing. In this work, we experimentally verify the
 failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a
 heatmap-based fully convolutional neural network (CNN), and propose a novel
 two-stage spatio-temporal CNN which effectively and efficiently combines both
 appearance and motion information to significantly surpass the state-of-the-art
 in WAMI object detection. To reduce the large search space, the first stage
 (ClusterNet) takes in a set of extremely large video frames, combines the
 motion and appearance information within the convolutional architecture, and
 proposes regions of objects of interest (ROOBI). These ROOBI can contain from
 one to clusters of several hundred objects due to the large video frame size
 and varying object density in WAMI. The second stage (FoveaNet) then estimates
 the centroid location of all objects in that given ROOBI simultaneously via
 heatmap estimation. The proposed method exceeds state-of-the-art results on the
 WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped
 objects, as well as being the first proposed method in wide area motion imagery
 to detect completely stationary objects.",1,0,0,0,0,0,
195,Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds,"Monte Carlo Tree Search (MCTS), most famously used in game-play artificial
 intelligence (e.g., the game of Go), is a well-known strategy for constructing
 approximate solutions to sequential decision problems. Its primary innovation
 is the use of a heuristic, known as a default policy, to obtain Monte Carlo
 estimates of downstream values for states in a decision tree. This information
 is used to iteratively expand the tree towards regions of states and actions
 that an optimal policy might visit. However, to guarantee convergence to the
 optimal action, MCTS requires the entire tree to be expanded asymptotically. In
 this paper, we propose a new technique called Primal-Dual MCTS that utilizes
 sampled information relaxation upper bounds on potential actions, creating the
 possibility of ""ignoring"" parts of the tree that stem from highly suboptimal
 choices. This allows us to prove that despite converging to a partial decision
 tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The
 new approach shows significant promise when used to optimize the behavior of a
 single driver navigating a graph while operating on a ride-sharing platform.
 Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest
 that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision
 trees and exhibits a reduced sensitivity to the size of the action space.",1,0,1,0,0,0,
196,Fermi-edge singularity and the functional renormalization group,"We study the Fermi-edge singularity, describing the response of a degenerate
 electron system to optical excitation, in the framework of the functional
 renormalization group (fRG). Results for the (interband) particle-hole
 susceptibility from various implementations of fRG (one- and two-
 particle-irreducible, multi-channel Hubbard-Stratonovich, flowing
 susceptibility) are compared to the summation of all leading logarithmic (log)
 diagrams, achieved by a (first-order) solution of the parquet equations. For
 the (zero-dimensional) special case of the X-ray-edge singularity, we show that
 the leading log formula can be analytically reproduced in a consistent way from
 a truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic
 structure, we show that this derivation relies on fortuitous partial
 cancellations special to the form of and accuracy applied to the X-ray-edge
 singularity and does not generalize.",0,1,0,0,0,0,
197,"Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies","Retrosynthesis is a technique to plan the chemical synthesis of organic
 molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a
 search tree is built by analysing molecules recursively and dissecting them
 into simpler molecular building blocks until one obtains a set of known
 building blocks. The search space is intractably large, and it is difficult to
 determine the value of retrosynthetic positions. Here, we propose to model
 retrosynthesis as a Markov Decision Process. In combination with a Deep Neural
 Network policy learned from essentially the complete published knowledge of
 chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In
 exploratory studies, we demonstrate that MCTS with neural network policies
 outperforms the traditionally used best-first search with hand-coded
 heuristics.",1,1,0,0,0,0,
198,The quasi-Assouad dimension for stochastically self-similar sets,"The class of stochastically self-similar sets contains many famous examples
 of random sets, e.g. Mandelbrot percolation and general fractal percolation.
 Under the assumption of the uniform open set condition and some mild
 assumptions on the iterated function systems used, we show that the
 quasi-Assouad dimension of self-similar random recursive sets is almost surely
 equal to the almost sure Hausdorff dimension of the set. We further comment on
 random homogeneous and $V$-variable sets and the removal of overlap conditions.",0,0,1,0,0,0,
199,Influence of Spin Orbit Coupling in the Iron-Based Superconductors,"We report on the influence of spin-orbit coupling (SOC) in the Fe-based
 superconductors (FeSCs) via application of circularly-polarized spin and
 angle-resolved photoemission spectroscopy. We combine this technique in
 representative members of both the Fe-pnictides and Fe-chalcogenides with ab
 initio density functional theory and tight-binding calculations to establish an
 ubiquitous modification of the electronic structure in these materials imbued
 by SOC. The influence of SOC is found to be concentrated on the hole pockets
 where the superconducting gap is generally found to be largest. This result
 contests descriptions of superconductivity in these materials in terms of pure
 spin-singlet eigenstates, raising questions regarding the possible pairing
 mechanisms and role of SOC therein.",0,1,0,0,0,0,
200,Effect of Meltdown and Spectre Patches on the Performance of HPC Applications,"In this work we examine how the updates addressing Meltdown and Spectre
 vulnerabilities impact the performance of HPC applications. To study this we
 use the application kernel module of XDMoD to test the performance before and
 after the application of the vulnerability patches. We tested the performance
 difference for multiple application and benchmarks including: NWChem, NAMD,
 HPCC, IOR, MDTest and IMB. The results show that although some specific
 functions can have performance decreased by as much as 74%, the majority of
 individual metrics indicates little to no decrease in performance. The
 real-world applications show a 2-3% decrease in performance for single node
 jobs and a 5-11% decrease for parallel multi node jobs.",1,0,0,0,0,0,
201,Gene regulatory network inference: an introductory survey,"Gene regulatory networks are powerful abstractions of biological systems.
 Since the advent of high-throughput measurement technologies in biology in the
 late 90s, reconstructing the structure of such networks has been a central
 computational problem in systems biology. While the problem is certainly not
 solved in its entirety, considerable progress has been made in the last two
 decades, with mature tools now available. This chapter aims to provide an
 introduction to the basic concepts underpinning network inference tools,
 attempting a categorisation which highlights commonalities and relative
 strengths. While the chapter is meant to be self-contained, the material
 presented should provide a useful background to the later, more specialised
 chapters of this book.",0,0,0,0,1,0,
202,Optic Disc and Cup Segmentation Methods for Glaucoma Detection with Modification of U-Net Convolutional Neural Network,"Glaucoma is the second leading cause of blindness all over the world, with
 approximately 60 million cases reported worldwide in 2010. If undiagnosed in
 time, glaucoma causes irreversible damage to the optic nerve leading to
 blindness. The optic nerve head examination, which involves measurement of
 cup-to-disc ratio, is considered one of the most valuable methods of structural
 diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation
 of optic disc and optic cup on eye fundus images and can be performed by modern
 computer vision algorithms. This work presents universal approach for automatic
 optic disc and cup segmentation, which is based on deep learning, namely,
 modification of U-Net convolutional neural network. Our experiments include
 comparison with the best known methods on publicly available databases
 DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,
 our method achieves quality comparable to current state-of-the-art methods,
 outperforming them in terms of the prediction time.",1,0,0,1,0,0,
203,"Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks","The life of the modern world essentially depends on the work of the large
 artificial homogeneous networks, such as wired and wireless communication
 systems, networks of roads and pipelines. The support of their effective
 continuous functioning requires automatic screening and permanent optimization
 with processing of the huge amount of data by high-performance distributed
 systems. We propose new meta-algorithm of large homogeneous network analysis,
 its decomposition into alternative sets of loosely connected subnets, and
 parallel optimization of the most independent elements. This algorithm is based
 on a network-specific correlation function, Simulated Annealing technique, and
 is adapted to work in the computer cluster. On the example of large wireless
 network, we show that proposed algorithm essentially increases speed of
 parallel optimization. The elaborated general approach can be used for analysis
 and optimization of the wide range of networks, including such specific types
 as artificial neural networks or organized in networks physiological systems of
 living organisms.",1,0,1,0,0,0,
204,Robust Contextual Bandit via the Capped-$\ell_{2}$ norm,"This paper considers the actor-critic contextual bandit for the mobile health
 (mHealth) intervention. The state-of-the-art decision-making methods in mHealth
 generally assume that the noise in the dynamic system follows the Gaussian
 distribution. Those methods use the least-square-based algorithm to estimate
 the expected reward, which is prone to the existence of outliers. To deal with
 the issue of outliers, we propose a novel robust actor-critic contextual bandit
 method for the mHealth intervention. In the critic updating, the
 capped-$\ell_{2}$ norm is used to measure the approximation error, which
 prevents outliers from dominating our objective. A set of weights could be
 achieved from the critic updating. Considering them gives a weighted objective
 for the actor updating. It provides the badly noised sample in the critic
 updating with zero weights for the actor updating. As a result, the robustness
 of both actor-critic updating is enhanced. There is a key parameter in the
 capped-$\ell_{2}$ norm. We provide a reliable method to properly set it by
 making use of one of the most fundamental definitions of outliers in
 statistics. Extensive experiment results demonstrate that our method can
 achieve almost identical results compared with the state-of-the-art methods on
 the dataset without outliers and dramatically outperform them on the datasets
 noised by outliers.",1,0,0,1,0,0,
205,Improper posteriors are not improper,"In 1933 Kolmogorov constructed a general theory that defines the modern
 concept of conditional expectation. In 1955 Renyi fomulated a new axiomatic
 theory for probability motivated by the need to include unbounded measures. We
 introduce a general concept of conditional expectation in Renyi spaces. In this
 theory improper priors are allowed, and the resulting posterior can also be
 improper.
 In 1965 Lindley published his classic text on Bayesian statistics using the
 theory of Renyi, but retracted this idea in 1973 due to the appearance of
 marginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes
 are investigated, and the seemingly conflicting results are explained. The
 theory of Renyi can hence be used as an axiomatic basis for statistics that
 allows use of unbounded priors.
 Keywords: Haldane's prior; Poisson intensity; Marginalization paradox;
 Measure theory; conditional probability space; axioms for statistics;
 conditioning on a sigma field; improper prior",0,0,1,1,0,0,
206,Fault Tolerant Consensus Agreement Algorithm,"Recently a new fault tolerant and simple mechanism was designed for solving
 commit consensus problem. It is based on replicated validation of messages sent
 between transaction participants and a special dispatcher validator manager
 node. This paper presents a correctness, safety proofs and performance analysis
 of this algorithm.",1,0,0,0,0,0,
207,Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology,"This work presents a new method to quantify connectivity in transportation
 networks. Inspired by the field of topological data analysis, we propose a
 novel approach to explore the robustness of road network connectivity in the
 presence of congestion on the roadway. The robustness of the pattern is
 summarized in a congestion barcode, which can be constructed directly from
 traffic datasets commonly used for navigation. As an initial demonstration, we
 illustrate the main technique on a publicly available traffic dataset in a
 neighborhood in New York City.",1,0,1,0,0,0,
208,Once in a blue moon: detection of 'bluing' during debris transits in the white dwarf WD1145+017,"The first transiting planetesimal orbiting a white dwarf was recently
 detected in K2 data of WD1145+017 and has been followed up intensively. The
 multiple, long, and variable transits suggest the transiting objects are dust
 clouds, probably produced by a disintegrating asteroid. In addition, the system
 contains circumstellar gas, evident by broad absorption lines, mostly in the
 u'-band, and a dust disc, indicated by an infrared excess. Here we present the
 first detection of a change in colour of WD1145+017 during transits, using
 simultaneous multi-band fast-photometry ULTRACAM measurements over the
 u'g'r'i'-bands. The observations reveal what appears to be 'bluing' during
 transits; transits are deeper in the redder bands, with a u'-r' colour
 difference of up to ~-0.05 mag. We explore various possible explanations for
 the bluing. 'Spectral' photometry obtained by integrating over bandpasses in
 the spectroscopic data in- and out-of-transit, compared to the photometric
 data, shows that the observed colour difference is most likely the result of
 reduced circumstellar absorption in the spectrum during transits. This
 indicates that the transiting objects and the gas share the same line-of-sight,
 and that the gas covers the white dwarf only partially, as would be expected if
 the gas, the transiting debris, and the dust emitting the infrared excess, are
 part of the same general disc structure (although possibly at different radii).
 In addition, we present the results of a week-long monitoring campaign of the
 system.",0,1,0,0,0,0,
209,"Viscous dynamics of drops and bubbles in Hele-Shaw cells: drainage, drag friction, coalescence, and bursting","In this review article, we discuss recent studies on drops and bubbles in
 Hele-Shaw cells, focusing on how scaling laws exhibit crossovers from the
 three-dimensional counterparts and focusing on topics in which viscosity plays
 an important role. By virtue of progresses in analytical theory and high-speed
 imaging, dynamics of drops and bubbles have actively been studied with the aid
 of scaling arguments. However, compared with three dimensional problems,
 studies on the corresponding problems in Hele-Shaw cells are still limited.
 This review demonstrates that the effect of confinement in the Hele-Shaw cell
 introduces new physics allowing different scaling regimes to appear. For this
 purpose, we discuss various examples that are potentially important for
 industrial applications handling drops and bubbles in confined spaces by
 showing agreement between experiments and scaling theories. As a result, this
 review provides a collection of problems in hydrodynamics that may be
 analytically solved or that may be worth studying numerically in the near
 future.",0,1,0,0,0,0,
210,Stacking-based Deep Neural Network: Deep Analytic Network on Convolutional Spectral Histogram Features,"Stacking-based deep neural network (S-DNN), in general, denotes a deep neural
 network (DNN) resemblance in terms of its very deep, feedforward network
 architecture. The typical S-DNN aggregates a variable number of individually
 learnable modules in series to assemble a DNN-alike alternative to the targeted
 object recognition tasks. This work likewise devises an S-DNN instantiation,
 dubbed deep analytic network (DAN), on top of the spectral histogram (SH)
 features. The DAN learning principle relies on ridge regression, and some key
 DNN constituents, specifically, rectified linear unit, fine-tuning, and
 normalization. The DAN aptitude is scrutinized on three repositories of varying
 domains, including FERET (faces), MNIST (handwritten digits), and CIFAR10
 (natural objects). The empirical results unveil that DAN escalates the SH
 baseline performance over a sufficiently deep layer.",1,0,0,0,0,0,
211,Superconductivity and Frozen Electronic States at the (111) LaAlO$_3$/SrTiO$_3$ Interface,"In spite of Anderson's theorem, disorder is known to affect superconductivity
 in conventional s-wave superconductors. In most superconductors, the degree of
 disorder is fixed during sample preparation. Here we report measurements of the
 superconducting properties of the two-dimensional gas that forms at the
 interface between LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) in the (111) crystal
 orientation, a system that permits \emph{in situ} tuning of carrier density and
 disorder by means of a back gate voltage $V_g$. Like the (001) oriented LAO/STO
 interface, superconductivity at the (111) LAO/STO interface can be tuned by
 $V_g$. In contrast to the (001) interface, superconductivity in these (111)
 samples is anisotropic, being different along different interface crystal
 directions, consistent with the strong anisotropy already observed other
 transport properties at the (111) LAO/STO interface. In addition, we find that
 the (111) interface samples ""remember"" the backgate voltage $V_F$ at which they
 are cooled at temperatures near the superconducting transition temperature
 $T_c$, even if $V_g$ is subsequently changed at lower temperatures. The low
 energy scale and other characteristics of this memory effect ($<1$ K)
 distinguish it from charge-trapping effects previously observed in (001)
 interface samples.",0,1,0,0,0,0,
212,Emittance preservation of an electron beam in a loaded quasi-linear plasma wakefield,"We investigate beam loading and emittance preservation for a high-charge
 electron beam being accelerated in quasi-linear plasma wakefields driven by a
 short proton beam. The structure of the studied wakefields are similar to those
 of a long, modulated proton beam, such as the AWAKE proton driver. We show that
 by properly choosing the electron beam parameters and exploiting two well known
 effects, beam loading of the wakefield and full blow out of plasma electrons by
 the accelerated beam, the electron beam can gain large amounts of energy with a
 narrow final energy spread (%-level) and without significant emittance growth.",0,1,0,0,0,0,
213,Detection of Nonlinearly Distorted OFDM Signals via Generalized Approximate Message Passing,"In this paper, we propose a practical receiver for multicarrier signals
 subjected to a strong memoryless nonlinearity. The receiver design is based on
 a generalized approximate message passing (GAMP) framework, and this allows
 real-time algorithm implementation in software or hardware with moderate
 complexity. We demonstrate that the proposed receiver can provide more than a
 2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range
 $10^{-4}\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to
 clipping nonlinearity and the crest-factor of the clipped waveform is only
 1.9dB. Simulation results also demonstrate that the proposed receiver provides
 significant performance gain in frequency-selective multipath channels",1,0,0,0,0,0,
214,Nonlinear fractal meaning of the Hubble constant,"According to astrophysical observations value of recession velocity in a
 certain point is proportional to a distance to this point. The proportionality
 coefficient is the Hubble constant measured with 5% accuracy. It is used in
 many cosmological theories describing dark energy, dark matter, baryons, and
 their relation with the cosmological constant introduced by Einstein.
 In the present work we have determined a limit value of the global Hubble
 constant (in a big distance from a point of observations) theoretically without
 using any empirical constants on the base of our own fractal model used for the
 description a relation between distance to an observed galaxy and coordinate of
 its center. The distance has been defined as a nonlinear fractal measure with
 scale of measurement corresponding to a deviation of the measure from its fixed
 value (zero-gravity radius). We have suggested a model of specific anisotropic
 fractal for simulation a radial Universe expansion. Our theoretical results
 have shown existence of an inverse proportionality between accuracy of
 determination the Hubble constant and accuracy of calculation a coordinates of
 galaxies leading to ambiguity results obtained at cosmological observations.",0,1,0,0,0,0,
215,SEA: String Executability Analysis by Abstract Interpretation,"Dynamic languages often employ reflection primitives to turn dynamically
 generated text into executable code at run-time. These features make standard
 static analysis extremely hard if not impossible because its essential data
 structures, i.e., the control-flow graph and the system of recursive equations
 associated with the program to analyse, are themselves dynamically mutating
 objects. We introduce SEA, an abstract interpreter for automatic sound string
 executability analysis of dynamic languages employing bounded (i.e, finitely
 nested) reflection and dynamic code generation. Strings are statically
 approximated in an abstract domain of finite state automata with basic
 operations implemented as symbolic transducers. SEA combines standard program
 analysis together with string executability analysis. The analysis of a call to
 reflection determines a call to the same abstract interpreter over a code which
 is synthesised directly from the result of the static string executability
 analysis at that program point. The use of regular languages for approximating
 dynamically generated code structures allows SEA to soundly approximate safety
 properties of self modifying programs yet maintaining efficiency. Soundness
 here means that the semantics of the code synthesised by the analyser to
 resolve reflection over-approximates the semantics of the code dynamically
 built at run-rime by the program at that point.",1,0,0,0,0,0,
216,On the trade-off between labels and weights in quantitative bisimulation,"Reductions for transition systems have been recently introduced as a uniform
 and principled method for comparing the expressiveness of system models with
 respect to a range of properties, especially bisimulations. In this paper we
 study the expressiveness (w.r.t. bisimulations) of models for quantitative
 computations such as weighted labelled transition systems (WLTSs), uniform
 labelled transition systems (ULTraSs), and state-to-function transition systems
 (FuTSs). We prove that there is a trade-off between labels and weights: at one
 extreme lays the class of (unlabelled) weighted transition systems where
 information is presented using weights only; at the other lays the class of
 labelled transition systems (LTSs) where information is shifted on labels.
 These categories of systems cannot be further reduced in any significant way
 and subsume all the aforementioned models.",1,0,0,0,0,0,
217,Poynting's theorem in magnetic turbulence,"Poynting's theorem is used to obtain an expression for the turbulent
 power-spectral density as function of frequency and wavenumber in low-frequency
 magnetic turbulence. No reference is made to Elsasser variables as is usually
 done in magnetohydrodynamic turbulence mixing mechanical and electromagnetic
 turbulence. We rather stay with an implicit form of the mechanical part of
 turbulence as suggested by electromagnetic theory in arbitrary media. All of
 mechanics and flows is included into a turbulent response function which by
 appropriate observations can be determined from knowledge of the turbulent
 fluctuation spectra. This approach is not guided by the wish of developing a
 complete theory of turbulence. It aims on the identification of the response
 function from observations as input into a theory which afterwards attempts its
 interpretation. Combination of both the magnetic and electric power spectral
 densities leads to a representation of the turbulent response function, i.e.
 the turbulent conductivity spectrum $\sigma_{\omega k}$ as function of
 frequency $\omega$ and wavenumber $k$. {It is given as the ratio of magnetic to
 electric power spectral densities in frequency space. This knowledge allows for
 formally writing down a turbulent dispersion relation. Power law inertial range
 spectra result in a power law turbulent conductivity spectrum. These can be
 compared with observations in the solar wind. Keywords: MHD turbulence,
 turbulent dispersion relation, turbulent response function, solar wind
 turbulence",0,1,0,0,0,0,
218,Polar factorization of conformal and projective maps of the sphere in the sense of optimal mass transport,"Let M be a compact Riemannian manifold and let $\mu$,d be the associated
 measure and distance on M. Robert McCann obtained, generalizing results for the
 Euclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->
 M pushing forward $\mu$ to a measure $\nu$: each S factors uniquely a.e. into
 the composition S = T \circ U, where U : M -> M is volume preserving and T : M
 -> M is the optimal map transporting $\mu$ to $\nu$ with respect to the cost
 function d^2/2.
 In this article we study the polar factorization of conformal and projective
 maps of the sphere S^n. For conformal maps, which may be identified with
 elements of the identity component of O(1,n+1), we prove that the polar
 factorization in the sense of optimal mass transport coincides with the
 algebraic polar factorization (Cartan decomposition) of this Lie group. For the
 projective case, where the group GL_+(n+1) is involved, we find necessary and
 sufficient conditions for these two factorizations to agree.",0,0,1,0,0,0,
219,Representing numbers as the sum of squares and powers in the ring $\mathbb{Z}_n$,"We examine the representation of numbers as the sum of two squares in
 $\mathbb{Z}_n$ for a general positive integer $n$. Using this information we
 make some comments about the density of positive integers which can be
 represented as the sum of two squares and powers of $2$ in $\mathbb{N}$.",0,0,1,0,0,0,
220,Spatial Regression and the Bayesian Filter,"Regression for spatially dependent outcomes poses many challenges, for
 inference and for computation. Non-spatial models and traditional spatial
 mixed-effects models each have their advantages and disadvantages, making it
 difficult for practitioners to determine how to carry out a spatial regression
 analysis. We discuss the data-generating mechanisms implicitly assumed by
 various popular spatial regression models, and discuss the implications of
 these assumptions. We propose Bayesian spatial filtering as an approximate
 middle way between non-spatial models and traditional spatial mixed models. We
 show by simulation that our Bayesian spatial filtering model has several
 desirable properties and hence may be a useful addition to a spatial
 statistician's toolkit.",0,0,0,1,0,0,
221,Behaviour of electron content in the ionospheric D-region during solar X-ray flares,"One of the most important parameters in ionospheric plasma research also
 having a wide practical application in wireless satellite telecommunications is
 the total electron content (TEC) representing the columnal electron number
 density. The F region with high electron density provides the biggest
 contribution to TEC while the relatively weakly ionized plasma of the D region
 (60 km - 90 km above Earths surface) is often considered as a negligible cause
 of satellite signal disturbances. However, sudden intensive ionization
 processes like those induced by solar X ray flares can cause relative increases
 of electron density that are significantly larger in the D-region than in
 regions at higher altitudes. Therefore, one cannot exclude a priori the D
 region from investigations of ionospheric influences on propagation of
 electromagnetic signals emitted by satellites. We discuss here this problem
 which has not been sufficiently treated in literature so far. The obtained
 results are based on data collected from the D region monitoring by very low
 frequency radio waves and on vertical TEC calculations from the Global
 Navigation Satellite System (GNSS) signal analyses, and they show noticeable
 variations in the D region electron content (TECD) during activity of a solar X
 ray flare (it rises by a factor of 136 in the considered case) when TECD
 contribution to TEC can reach several percent and which cannot be neglected in
 practical applications like global positioning procedures by satellites.",0,1,0,0,0,0,
222,Fractional compound Poisson processes with multiple internal states,"For the particles undergoing the anomalous diffusion with different waiting
 time distributions for different internal states, we derive the Fokker-Planck
 and Feymann-Kac equations, respectively, describing positions of the particles
 and functional distributions of the trajectories of particles; in particular,
 the equations governing the functional distribution of internal states are also
 obtained. The dynamics of the stochastic processes are analyzed and the
 applications, calculating the distribution of the first passage time and the
 distribution of the fraction of the occupation time, of the equations are
 given.",0,0,1,1,0,0,
223,Zero-point spin-fluctuations of single adatoms,"Stabilizing the magnetic signal of single adatoms is a crucial step towards
 their successful usage in widespread technological applications such as
 high-density magnetic data storage devices. The quantum mechanical nature of
 these tiny objects, however, introduces intrinsic zero-point spin-fluctuations
 that tend to destabilize the local magnetic moment of interest by dwindling the
 magnetic anisotropy potential barrier even at absolute zero temperature. Here,
 we elucidate the origins and quantify the effect of the fundamental ingredients
 determining the magnitude of the fluctuations, namely the ($i$) local magnetic
 moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner
 excitations. Based on a systematic first-principles study of 3d and 4d adatoms,
 we demonstrate that the transverse contribution of the fluctuations is
 comparable in size to the magnetic moment itself, leading to a remarkable
 $\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives
 rise to a comprehensible diagram relating the fluctuation magnitude to
 characteristic features of adatoms, providing practical guidelines for
 designing magnetically stable nanomagnets with minimal quantum fluctuations.",0,1,0,0,0,0,
224,Exploration-exploitation tradeoffs dictate the optimal distributions of phenotypes for populations subject to fitness fluctuations,"We study a minimal model for the growth of a phenotypically heterogeneous
 population of cells subject to a fluctuating environment in which they can
 replicate (by exploiting available resources) and modify their phenotype within
 a given landscape (thereby exploring novel configurations). The model displays
 an exploration-exploitation trade-off whose specifics depend on the statistics
 of the environment. Most notably, the phenotypic distribution corresponding to
 maximum population fitness (i.e. growth rate) requires a non-zero exploration
 rate when the magnitude of environmental fluctuations changes randomly over
 time, while a purely exploitative strategy turns out to be optimal in two-state
 environments, independently of the statistics of switching times. We obtain
 analytical insight into the limiting cases of very fast and very slow
 exploration rates by directly linking population growth to the features of the
 environment.",0,0,0,0,1,0,
225,Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms,"Electronic Health Records (EHR) are data generated during routine clinical
 care. EHR offer researchers unprecedented phenotypic breadth and depth and have
 the potential to accelerate the pace of precision medicine at scale. A main EHR
 use-case is creating phenotyping algorithms to define disease status, onset and
 severity. Currently, no common machine-readable standard exists for defining
 phenotyping algorithms which often are stored in human-readable formats. As a
 result, the translation of algorithms to implementation code is challenging and
 sharing across the scientific community is problematic. In this paper, we
 evaluate openEHR, a formal EHR data specification, for computable
 representations of EHR phenotyping algorithms.",1,0,0,0,0,0,
226,Optimizing Mission Critical Data Dissemination in Massive IoT Networks,"Mission critical data dissemination in massive Internet of things (IoT)
 networks imposes constraints on the message transfer delay between devices. Due
 to low power and communication range of IoT devices, data is foreseen to be
 relayed over multiple device-to-device (D2D) links before reaching the
 destination. The coexistence of a massive number of IoT devices poses a
 challenge in maximizing the successful transmission capacity of the overall
 network alongside reducing the multi-hop transmission delay in order to support
 mission critical applications. There is a delicate interplay between the
 carrier sensing threshold of the contention based medium access protocol and
 the choice of packet forwarding strategy selected at each hop by the devices.
 The fundamental problem in optimizing the performance of such networks is to
 balance the tradeoff between conflicting performance objectives such as the
 spatial frequency reuse, transmission quality, and packet progress towards the
 destination. In this paper, we use a stochastic geometry approach to quantify
 the performance of multi-hop massive IoT networks in terms of the spatial
 frequency reuse and the transmission quality under different packet forwarding
 schemes. We also develop a comprehensive performance metric that can be used to
 optimize the system to achieve the best performance. The results can be used to
 select the best forwarding scheme and tune the carrier sensing threshold to
 optimize the performance of the network according to the delay constraints and
 transmission quality requirements.",1,0,0,0,0,0,
227,Interference of two co-directional exclusion processes in the presence of a static bottleneck: a biologically motivated model,"We develope a two-species exclusion process with a distinct pair of entry and
 exit sites for each species of rigid rods. The relatively slower forward
 stepping of the rods in an extended bottleneck region, located in between the
 two entry sites, controls the extent of interference of the co-directional flow
 of the two species of rods. The relative positions of the sites of entry of the
 two species of rods with respect to the location of the bottleneck are
 motivated by a biological phenomenon. However, the primary focus of the study
 here is to explore the effects of the interference of the flow of the two
 species of rods on their spatio-temporal organization and the regulations of
 this interference by the extended bottleneck. By a combination of mean-field
 theory and computer simulation we calculate the flux of both species of rods
 and their density profiles as well as the composite phase diagrams of the
 system. If the bottleneck is sufficiently stringent some of the phases become
 practically unrealizable although not ruled out on the basis of any fundamental
 physical principle. Moreover the extent of suppression of flow of the
 downstream entrants by the flow of the upstream entrants can also be regulated
 by the strength of the bottleneck. We speculate on the possible implications of
 the results in the context of the biological phenomenon that motivated the
 formulation of the theoretical model.",0,1,0,0,0,0,
228,Gaussian fluctuations of Jack-deformed random Young diagrams,"We introduce a large class of random Young diagrams which can be regarded as
 a natural one-parameter deformation of some classical Young diagram ensembles;
 a deformation which is related to Jack polynomials and Jack characters. We show
 that each such a random Young diagram converges asymptotically to some limit
 shape and that the fluctuations around the limit are asymptotically Gaussian.",0,0,1,0,0,0,
229,Revisiting (logarithmic) scaling relations using renormalization group,"We explicitly compute the critical exponents associated with logarithmic
 corrections (the so-called hatted exponents) starting from the renormalization
 group equations and the mean field behavior for a wide class of models at the
 upper critical behavior (for short and long range $\phi^n$-theories) and below
 it. This allows us to check the scaling relations among these critical
 exponents obtained by analysing the complex singularities (Lee-Yang and Fisher
 zeroes) of these models. Moreover, we have obtained an explicit method to
 compute the $\hat{\coppa}$ exponent [defined by $\xi\sim L (\log
 L)^{\hat{\coppa}}$] and, finally, we have found a new derivation of the scaling
 law associated with it.",0,1,0,0,0,0,
230,Concentration of weakly dependent Banach-valued sums and applications to statistical learning methods,"We obtain a Bernstein-type inequality for sums of Banach-valued random
 variables satisfying a weak dependence assumption of general type and under
 certain smoothness assumptions of the underlying Banach norm. We use this
 inequality in order to investigate in the asymptotical regime the error upper
 bounds for the broad family of spectral regularization methods for reproducing
 kernel decision rules, when trained on a sample coming from a $\tau-$mixing
 process.",0,0,1,1,0,0,
231,Evolution of the Kondo lattice electronic structure above the transport coherence temperature,"The temperature-dependent evolution of the Kondo lattice is a long-standing
 topic of theoretical and experimental investigation and yet it lacks a truly
 microscopic description of the relation of the basic $f$-$d$ hybridization
 processes to the fundamental temperature scales of Kondo screening and
 Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$
 hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo
 lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved
 photoemission (ARPES) with sufficient detail to allow direct comparison to
 first principles dynamical mean field theory (DMFT) calculations containing
 full realism of crystalline electric field states. The ARPES results, for two
 orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$
 hybridization scenarios, with additional microscopic insight provided by DMFT,
 reveal $f$ participation in the Fermi surface at temperatures much higher than
 the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be
 the onset for such behavior. The identification of a $T$-dependent crystalline
 electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is
 specifically highlighted.",0,1,0,0,0,0,
232,On A Conjecture Regarding Permutations Which Destroy Arithmetic Progressions,"Hegarty conjectured for $n\neq 2, 3, 5, 7$ that $\mathbb{Z}/n\mathbb{Z}$ has
 a permutation which destroys all arithmetic progressions mod $n$. For $n\ge
 n_0$, Hegarty and Martinsson demonstrated that $\mathbb{Z}/n\mathbb{Z}$ has an
 arithmetic-progression destroying permutation. However $n_0\approx 1.4\times
 10^{14}$ and thus resolving the conjecture in full remained out of reach of any
 computational techniques. However, this paper using constructions modeled after
 those used by Elkies and Swaminathan for the case of $\mathbb{Z}/p\mathbb{Z}$
 with $p$ being prime, establish the conjecture in full. Furthermore our results
 do not rely on the fact that it suffices to study when $n<n_0$ and thus our
 results completely independent of the proof given by Hegarty and Martinsson.",0,0,1,0,0,0,
233,Inverse monoids and immersions of cell complexes,"An immersion $f : {\mathcal D} \rightarrow \mathcal C$ between cell complexes
 is a local homeomorphism onto its image that commutes with the characteristic
 maps of the cell complexes. We study immersions between finite-dimensional
 connected $\Delta$-complexes by replacing the fundamental group of the base
 space by an appropriate inverse monoid. We show how conjugacy classes of the
 closed inverse submonoids of this inverse monoid may be used to classify
 connected immersions into the complex. This extends earlier results of Margolis
 and Meakin for immersions between graphs and of Meakin and Szak?­cs on
 immersions into $2$-dimensional $CW$-complexes.",0,0,1,0,0,0,
234,Not even wrong: The spurious link between biodiversity and ecosystem functioning,"Resolving the relationship between biodiversity and ecosystem functioning has
 been one of the central goals of modern ecology. Early debates about the
 relationship were finally resolved with the advent of a statistical
 partitioning scheme that decomposed the biodiversity effect into a ""selection""
 effect and a ""complementarity"" effect. We prove that both the biodiversity
 effect and its statistical decomposition into selection and complementarity are
 fundamentally flawed because these methods use a na??ve null expectation based
 on neutrality, likely leading to an overestimate of the net biodiversity
 effect, and they fail to account for the nonlinear abundance-ecosystem
 functioning relationships observed in nature. Furthermore, under such
 nonlinearity no statistical scheme can be devised to partition the biodiversity
 effects. We also present an alternative metric providing a more reasonable
 estimate of biodiversity effect. Our results suggest that all studies conducted
 since the early 1990s likely overestimated the positive effects of biodiversity
 on ecosystem functioning.",0,0,0,0,1,0,
235,Evidence of Fraud in Brazil's Electoral Campaigns Via the Benford's Law,"The principle of democracy is that the people govern through elected
 representatives. Therefore, a democracy is healthy as long as the elected
 politicians do represent the people. We have analyzed data from the Brazilian
 electoral court (Tribunal Superior Eleitoral, TSE) concerning money donations
 for the electoral campaigns and the election results. Our work points to two
 disturbing conclusions: money is a determining factor on whether a candidate is
 elected or not (as opposed to representativeness); secondly, the use of
 Benford's Law to analyze the declared donations received by the parties and
 electoral campaigns shows evidence of fraud in the declarations. A better term
 to define Brazil's government system is what we define as chrimatocracy (govern
 by money).",1,0,0,1,0,0,
236,A Berkeley View of Systems Challenges for AI,"With the increasing commoditization of computer vision, speech recognition
 and machine translation systems and the widespread deployment of learning-based
 back-end technologies such as digital advertising and intelligent
 infrastructures, AI (Artificial Intelligence) has moved from research labs to
 production. These changes have been made possible by unprecedented levels of
 data and computation, by methodological advances in machine learning, by
 innovations in systems software and architectures, and by the broad
 accessibility of these technologies.
 The next generation of AI systems promises to accelerate these developments
 and increasingly impact our lives via frequent interactions and making (often
 mission-critical) decisions on our behalf, often in highly personalized
 contexts. Realizing this promise, however, raises daunting challenges. In
 particular, we need AI systems that make timely and safe decisions in
 unpredictable environments, that are robust against sophisticated adversaries,
 and that can process ever increasing amounts of data across organizations and
 individuals without compromising confidentiality. These challenges will be
 exacerbated by the end of the Moore's Law, which will constrain the amount of
 data these technologies can store and process. In this paper, we propose
 several open research directions in systems, architectures, and security that
 can address these challenges and help unlock AI's potential to improve lives
 and society.",1,0,0,0,0,0,
237,"Equivariant infinite loop space theory, I. The space level story","We rework and generalize equivariant infinite loop space theory, which shows
 how to construct G-spectra from G-spaces with suitable structure. There is a
 naive version which gives naive G-spectra for any topological group G, but our
 focus is on the construction of genuine G-spectra when G is finite.
 We give new information about the Segal and operadic equivariant infinite
 loop space machines, supplying many details that are missing from the
 literature, and we prove by direct comparison that the two machines give
 equivalent output when fed equivalent input. The proof of the corresponding
 nonequivariant uniqueness theorem, due to May and Thomason, works for naive
 G-spectra for general G but fails hopelessly for genuine G-spectra when G is
 finite. Even in the nonequivariant case, our comparison theorem is considerably
 more precise, giving a direct point-set level comparison.
 We have taken the opportunity to update this general area, equivariant and
 nonequivariant, giving many new proofs, filling in some gaps, and giving some
 corrections to results in the literature.",0,0,1,0,0,0,
238,Arithmetic purity of strong approximation for homogeneous spaces,"We prove that any open subset $U$ of a semi-simple simply connected
 quasi-split linear algebraic group $G$ with ${codim} (G\setminus U, G)\geq 2$
 over a number field satisfies strong approximation by establishing a fibration
 of $G$ over a toric variety. We also prove a similar result of strong
 approximation with Brauer-Manin obstruction for a partial equivariant smooth
 compactification of a homogeneous space where all invertible functions are
 constant and the semi-simple part of the linear algebraic group is quasi-split.
 Some semi-abelian varieties of any given dimension where the complements of a
 rational point do not satisfy strong approximation with Brauer-Manin
 obstruction are given.",0,0,1,0,0,0,
239,Flatness results for nonlocal minimal cones and subgraphs,"We show that nonlocal minimal cones which are non-singular subgraphs outside
 the origin are necessarily halfspaces.
 The proof is based on classical ideas of~\cite{DG1} and on the computation of
 the linearized nonlocal mean curvature operator, which is proved to satisfy a
 suitable maximum principle.
 With this, we obtain new, and somehow simpler, proofs of the Bernstein-type
 results for nonlocal minimal surfaces which have been recently established
 in~\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type
 result which classifies Lipschitz nonlocal minimal subgraphs outside a ball.",0,0,1,0,0,0,
240,Effective Asymptotic Formulae for Multilinear Averages of Multiplicative Functions,"Let $f_1,\ldots,f_k : \mathbb{N} \rightarrow \mathbb{C}$ be multiplicative
 functions taking values in the closed unit disc. Using an analytic approach in
 the spirit of Hal?­sz' mean value theorem, we compute multidimensional
 averages of the shape $$x^{-l} \sum_{\mathbf{n} \in [x]^l} \prod_{1 \leq j \leq
 k} f_j(L_j(\mathbf{n}))$$ as $x \rightarrow \infty$, where $[x] := [1,x]$ and
 $L_1,\ldots, L_k$ are affine linear forms that satisfy some natural conditions.
 Our approach gives a new proof of a result of Frantzikinakis and Host that is
 distinct from theirs, with \emph{explicit} main and error terms. \\ As an
 application of our formulae, we establish a \emph{local-to-global} principle
 for Gowers norms of multiplicative functions. We also compute the asymptotic
 densities of the sets of integers $n$ such that a given multiplicative function
 $f: \mathbb{N} \rightarrow \{-1, 1\}$ yields a fixed sign pattern of length 3
 or 4 on almost all 3- and 4-term arithmetic progressions, respectively, with
 first term $n$.",0,0,1,0,0,0,
241,On the apparent permeability of porous media in rarefied gas flows,"The apparent gas permeability of the porous medium is an important parameter
 in the prediction of unconventional gas production, which was first
 investigated systematically by Klinkenberg in 1941 and found to increase with
 the reciprocal mean gas pressure (or equivalently, the Knudsen number).
 Although the underlying rarefaction effects are well-known, the reason that the
 correction factor in Klinkenberg's famous equation decreases when the Knudsen
 number increases has not been fully understood. Most of the studies idealize
 the porous medium as a bundle of straight cylindrical tubes, however, according
 to the gas kinetic theory, this only results in an increase of the correction
 factor with the Knudsen number, which clearly contradicts Klinkenberg's
 experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
 in simplified (but not simple) porous media, we identify, for the first time,
 two key factors that can explain Klinkenberg's experimental results: the
 tortuous flow path and the non-unitary tangential momentum accommodation
 coefficient for the gas-surface interaction. Moreover, we find that
 Klinkenberg's results can only be observed when the ratio between the apparent
 and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
 numbers) the correction factor increases with the Knudsen number. Our numerical
 results could also serve as benchmarking cases to assess the accuracy of
 macroscopic models and/or numerical schemes for the modeling/simulation of
 rarefied gas flows in complex geometries over a wide range of gas rarefaction.",0,1,0,0,0,0,
242,Small subgraphs and their extensions in a random distance graph,"In previous papers, threshold probabilities for the properties of a random
 distance graph to contain strictly balanced graphs were found. We extend this
 result to arbitrary graphs and prove that the number of copies of a strictly
 balanced graph has asymptotically Poisson distribution at the threshold.",0,0,1,0,0,0,
243,Increasing the Reusability of Enforcers with Lifecycle Events,"Runtime enforcement can be effectively used to improve the reliability of
 software applications. However, it often requires the definition of ad hoc
 policies and enforcement strategies, which might be expensive to identify and
 implement. This paper discusses how to exploit lifecycle events to obtain
 useful enforcement strategies that can be easily reused across applications,
 thus reducing the cost of adoption of the runtime enforcement technology. The
 paper finally sketches how this idea can be used to define libraries that can
 automatically overcome problems related to applications misusing them.",1,0,0,0,0,0,
244,A Fast Interior Point Method for Atomic Norm Soft Thresholding,"The atomic norm provides a generalization of the $\ell_1$-norm to continuous
 parameter spaces. When applied as a sparse regularizer for line spectral
 estimation the solution can be obtained by solving a convex optimization
 problem. This problem is known as atomic norm soft thresholding (AST). It can
 be cast as a semidefinite program and solved by standard methods. In the
 semidefinite formulation there are $O(N^2)$ dual variables and a standard
 primal-dual interior point method requires at least $O(N^6)$ flops per
 iteration. That has lead researcher to consider alternating direction method of
 multipliers (ADMM) for the solution of AST, but this method is still somewhat
 slow for large problem sizes. To obtain a faster algorithm we reformulate AST
 as a non-symmetric conic program. That has two properties of key importance to
 its numerical solution: the conic formulation has only $O(N)$ dual variables
 and the Toeplitz structure inherent to AST is preserved. Based on it we derive
 FastAST which is a primal-dual interior point method for solving AST. Two
 variants are considered with the fastest one requiring only $O(N^2)$ flops per
 iteration. Extensive numerical experiments demonstrate that FastAST solves AST
 significantly faster than a state-of-the-art solver based on ADMM.",1,0,0,0,0,0,
245,Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments,"We study the problem of causal structure learning over a set of random
 variables when the experimenter is allowed to perform at most $M$ experiments
 in a non-adaptive manner. We consider the optimal learning strategy in terms of
 minimizing the portions of the structure that remains unknown given the limited
 number of experiments in both Bayesian and minimax setting. We characterize the
 theoretical optimal solution and propose an algorithm, which designs the
 experiments efficiently in terms of time complexity. We show that for bounded
 degree graphs, in the minimax case and in the Bayesian case with uniform
 priors, our proposed algorithm is a $\rho$-approximation algorithm, where
 $\rho$ is independent of the order of the underlying graph. Simulations on both
 synthetic and real data show that the performance of our algorithm is very
 close to the optimal solution.",1,0,0,1,0,0,
246,Economically Efficient Combined Plant and Controller Design Using Batch Bayesian Optimization: Mathematical Framework and Airborne Wind Energy Case Study,"We present a novel data-driven nested optimization framework that addresses
 the problem of coupling between plant and controller optimization. This
 optimization strategy is tailored towards instances where a closed-form
 expression for the system dynamic response is unobtainable and simulations or
 experiments are necessary. Specifically, Bayesian Optimization, which is a
 data-driven technique for finding the optimum of an unknown and
 expensive-to-evaluate objective function, is employed to solve a nested
 optimization problem. The underlying objective function is modeled by a
 Gaussian Process (GP); then, Bayesian Optimization utilizes the predictive
 uncertainty information from the GP to determine the best subsequent control or
 plant parameters. The proposed framework differs from the majority of co-design
 literature where there exists a closed-form model of the system dynamics.
 Furthermore, we utilize the idea of Batch Bayesian Optimization at the plant
 optimization level to generate a set of plant designs at each iteration of the
 overall optimization process, recognizing that there will exist economies of
 scale in running multiple experiments in each iteration of the plant design
 process. We validate the proposed framework for a Buoyant Airborne Turbine
 (BAT). We choose the horizontal stabilizer area, longitudinal center of mass
 relative to center of buoyancy (plant parameters), and the pitch angle
 set-point (controller parameter) as our decision variables. Our results
 demonstrate that these plant and control parameters converge to their
 respective optimal values within only a few iterations.",1,0,0,0,0,0,
247,The 10 phases of spin chains with two Ising symmetries,"We explore the topological properties of quantum spin-1/2 chains with two
 Ising symmetries. This class of models does not possess any of the symmetries
 that are required to protect the Haldane phase. Nevertheless, we show that
 there are 4 symmetry-protected topological phases, in addition to 6 phases that
 spontaneously break one or both Ising symmetries. By mapping the model to
 one-dimensional interacting fermions with particle-hole and time-reversal
 symmetry, we obtain integrable parent Hamiltonians for the conventional and
 topological phases of the spin model. We use these Hamiltonians to characterize
 the physical properties of all 10 phases, identify their local and nonlocal
 order parameters, and understand the effects of weak perturbations that respect
 the Ising symmetries. Our study provides the first explicit example of a class
 of spin chains with several topologically non-trivial phases, and binds
 together the topological classifications of interacting bosons and fermions.",0,1,0,0,0,0,
248,Generalized subspace subcodes with application in cryptology,"Most of the codes that have an algebraic decoding algorithm are derived from
 the Reed Solomon codes. They are obtained by taking equivalent codes, for
 example the generalized Reed Solomon codes, or by using the so-called subfield
 subcode method, which leads to Alternant codes and Goppa codes over the
 underlying prime field, or over some intermediate subfield. The main advantages
 of these constructions is to preserve both the minimum distance and the
 decoding algorithm of the underlying Reed Solomon code. In this paper, we
 propose a generalization of the subfield subcode construction by introducing
 the notion of subspace subcodes and a generalization of the equivalence of
 codes which leads to the notion of generalized subspace subcodes. When the
 dimension of the selected subspaces is equal to one, we show that our approach
 gives exactly the family of the codes obtained by equivalence and subfield
 subcode technique. However, our approach highlights the links between the
 subfield subcode of a code defined over an extension field and the operation of
 puncturing the $q$-ary image of this code. When the dimension of the subspaces
 is greater than one, we obtain codes whose alphabet is no longer a finite
 field, but a set of r-uples. We explain why these codes are practically as
 efficient for applications as the codes defined on an extension of degree r. In
 addition, they make it possible to obtain decodable codes over a large alphabet
 having parameters previously inaccessible. As an application, we give some
 examples that can be used in public key cryptosystems such as McEliece.",1,0,0,0,0,0,
249,Lagrangian fibers of Gelfand-Cetlin systems,"Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of
 Gelfand-Cetlin systems over complex partial flag manifolds, we provide a
 complete description of the topology of Gelfand-Cetlin fibers. We prove that
 all fibers are \emph{smooth} isotropic submanifolds and give a complete
 description of the fiber to be Lagrangian in terms of combinatorics of
 Gelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian
 fibers. After a few combinatorial and numercal tests for the displaceability,
 using the bulk-deformation of Floer cohomology by Schubert cycles, we prove
 that every full flag manifold $\mathcal{F}(n)$ ($n \geq 3$) with a monotone
 Kirillov-Kostant-Souriau symplectic form carries a continuum of
 non-displaceable Lagrangian tori which degenerates to a non-torus fiber in the
 Hausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\mathcal{F}(3)$
 is non-displaceable the question of which was raised by Nohara-Ueda who
 computed its Floer cohomology to be vanishing.",0,0,1,0,0,0,
250,A local ensemble transform Kalman particle filter for convective scale data assimilation,"Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)
 are a key component of probabilistic weather forecasting. They represent the
 uncertainty in the initial conditions by an ensemble which incorporates
 information coming from the physical model with the latest observations.
 High-resolution numerical weather prediction models ran at operational centers
 are able to resolve non-linear and non-Gaussian physical phenomena such as
 convection. There is therefore a growing need to develop ensemble assimilation
 algorithms able to deal with non-Gaussianity while staying computationally
 feasible. In the present paper we address some of these needs by proposing a
 new hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully
 formulated in ensemble space and uses a deterministic scheme such that it has
 the ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a
 limiting case. A new criterion for choosing the proportion of particle filter
 and ETKF update is also proposed. The new algorithm is implemented in the COSMO
 framework and numerical experiments in a quasi-operational convective-scale
 setup are conducted. The results show the feasibility of the new algorithm in
 practice and indicate a strong potential for such local hybrid methods, in
 particular for forecasting non-Gaussian variables such as wind and hourly
 precipitation.",0,1,0,1,0,0,
251,Tensor Robust Principal Component Analysis with A New Tensor Nuclear Norm,"In this paper, we consider the Tensor Robust Principal Component Analysis
 (TRPCA) problem, which aims to exactly recover the low-rank and sparse
 components from their sum. Our model is based on the recently proposed
 tensor-tensor product (or t-product) [13]. Induced by the t-product, we first
 rigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor
 average rank, and show that the tensor nuclear norm is the convex envelope of
 the tensor average rank within the unit ball of the tensor spectral norm. These
 definitions, their relationships and properties are consistent with matrix
 cases. Equipped with the new tensor nuclear norm, we then solve the TRPCA
 problem by solving a convex program and provide the theoretical guarantee for
 the exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA
 as a special case. Numerical experiments verify our results, and the
 applications to image recovery and background modeling problems demonstrate the
 effectiveness of our method.",0,0,0,1,0,0,
252,Resolving the age bimodality of galaxy stellar populations on kpc scales,"Galaxies in the local Universe are known to follow bimodal distributions in
 the global stellar populations properties. We analyze the distribution of the
 local average stellar-population ages of 654,053 sub-galactic regions resolved
 on ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the
 CALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.
 We find a bimodal local-age distribution, with an old and a young peak
 primarily due to regions in early-type galaxies and star-forming regions of
 spirals, respectively. Within spiral galaxies, the older ages of bulges and
 inter-arm regions relative to spiral arms support an internal age bimodality.
 Although regions of higher stellar-mass surface-density, mu*, are typically
 older, mu* alone does not determine the stellar population age and a bimodal
 distribution is found at any fixed mu*. We identify an ""old ridge"" of regions
 of age ~9 Gyr, independent of mu*, and a ""young sequence"" of regions with age
 increasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as
 regions containing only old stars, and the latter as regions where the relative
 contamination of old stellar populations by young stars decreases as mu*
 increases. The reason why this bimodal age distribution is not inconsistent
 with the unimodal shape of the cosmic-averaged star-formation history is that
 i) the dominating contribution by young stars biases the age low with respect
 to the average epoch of star formation, and ii) the use of a single average age
 per region is unable to represent the full time-extent of the star-formation
 history of ""young-sequence"" regions.",0,1,0,0,0,0,
253,Hidden long evolutionary memory in a model biochemical network,"We introduce a minimal model for the evolution of functional
 protein-interaction networks using a sequence-based mutational algorithm, and
 apply the model to study neutral drift in networks that yield oscillatory
 dynamics. Starting with a functional core module, random evolutionary drift
 increases network complexity even in the absence of specific selective
 pressures. Surprisingly, we uncover a hidden order in sequence space that gives
 rise to long-term evolutionary memory, implying strong constraints on network
 evolution due to the topology of accessible sequence space.",0,1,0,0,0,0,
254,On Study of the Reliable Fully Convolutional Networks with Tree Arranged Outputs (TAO-FCN) for Handwritten String Recognition,"The handwritten string recognition is still a challengeable task, though the
 powerful deep learning tools were introduced. In this paper, based on TAO-FCN,
 we proposed an end-to-end system for handwritten string recognition. Compared
 with the conventional methods, there is no preprocess nor manually designed
 rules employed. With enough labelled data, it is easy to apply the proposed
 method to different applications. Although the performance of the proposed
 method may not be comparable with the state-of-the-art approaches, it's
 usability and robustness are more meaningful for practical applications.",1,0,0,0,0,0,
255,Marcel Riesz on N??rlund Means,"We note that the necessary and sufficient conditions established by Marcel
 Riesz for the inclusion of regular N??rlund summation methods are in fact
 applicable quite generally.",0,0,1,0,0,0,
256,Mathematics of Isogeny Based Cryptography,"These lectures notes were written for a summer school on Mathematics for
 post-quantum cryptography in Thi??s, Senegal. They try to provide a guide for
 Masters' students to get through the vast literature on elliptic curves,
 without getting lost on their way to learning isogeny based cryptography. They
 are by no means a reference text on the theory of elliptic curves, nor on
 cryptography; students are encouraged to complement these notes with some of
 the books recommended in the bibliography.
 The presentation is divided in three parts, roughly corresponding to the
 three lectures given. In an effort to keep the reader interested, each part
 alternates between the fundamental theory of elliptic curves, and applications
 in cryptography. We often prefer to have the main ideas flow smoothly, rather
 than having a rigorous presentation as one would have in a more classical book.
 The reader will excuse us for the inaccuracies and the omissions.",1,0,0,0,0,0,
257,Modeling of drug diffusion in a solid tumor leading to tumor cell death,"It has been shown recently that changing the fluidic properties of a drug can
 improve its efficacy in ablating solid tumors. We develop a modeling framework
 for tumor ablation, and present the simplest possible model for drug diffusion
 in a spherical tumor with leaky boundaries and assuming cell death eventually
 leads to ablation of that cell effectively making the two quantities
 numerically equivalent. The death of a cell after a given exposure time depends
 on both the concentration of the drug and the amount of oxygen available to the
 cell. Higher oxygen availability leads to cell death at lower drug
 concentrations. It can be assumed that a minimum concentration is required for
 a cell to die, effectively connecting diffusion with efficacy. The
 concentration threshold decreases as exposure time increases, which allows us
 to compute dose-response curves. Furthermore, these curves can be plotted at
 much finer time intervals compared to that of experiments, which is used to
 produce a dose-threshold-response surface giving an observer a complete picture
 of the drug's efficacy for an individual. In addition, since the diffusion,
 leak coefficients, and the availability of oxygen is different for different
 individuals and tumors, we produce artificial replication data through
 bootstrapping to simulate error. While the usual data-driven model with
 Sigmoidal curves use 12 free parameters, our mechanistic model only has two
 free parameters, allowing it to be open to scrutiny rather than forcing
 agreement with data. Even so, the simplest model in our framework, derived
 here, shows close agreement with the bootstrapped curves, and reproduces well
 established relations, such as Haber's rule.",0,0,0,0,1,0,
258,Sensitivity analysis for inverse probability weighting estimators via the percentile bootstrap,"To identify the estimand in missing data problems and observational studies,
 it is common to base the statistical estimation on the ""missing at random"" and
 ""no unmeasured confounder"" assumptions. However, these assumptions are
 unverifiable using empirical data and pose serious threats to the validity of
 the qualitative conclusions of the statistical inference. A sensitivity
 analysis asks how the conclusions may change if the unverifiable assumptions
 are violated to a certain degree. In this paper we consider a marginal
 sensitivity model which is a natural extension of Rosenbaum's sensitivity model
 that is widely used for matched observational studies. We aim to construct
 confidence intervals based on inverse probability weighting estimators, such
 that asymptotically the intervals have at least nominal coverage of the
 estimand whenever the data generating distribution is in the collection of
 marginal sensitivity models. We use a percentile bootstrap and a generalized
 minimax/maximin inequality to transform this intractable problem to a linear
 fractional programming problem, which can be solved very efficiently. We
 illustrate our method using a real dataset to estimate the causal effect of
 fish consumption on blood mercury level.",0,0,1,1,0,0,
259,From 4G to 5G: Self-organized Network Management meets Machine Learning,"In this paper, we provide an analysis of self-organized network management,
 with an end-to-end perspective of the network. Self-organization as applied to
 cellular networks is usually referred to Self-organizing Networks (SONs), and
 it is a key driver for improving Operations, Administration, and Maintenance
 (OAM) activities. SON aims at reducing the cost of installation and management
 of 4G and future 5G networks, by simplifying operational tasks through the
 capability to configure, optimize and heal itself. To satisfy 5G network
 management requirements, this autonomous management vision has to be extended
 to the end to end network. In literature and also in some instances of products
 available in the market, Machine Learning (ML) has been identified as the key
 tool to implement autonomous adaptability and take advantage of experience when
 making decisions. In this paper, we survey how network management can
 significantly benefit from ML solutions. We review and provide the basic
 concepts and taxonomy for SON, network management and ML. We analyse the
 available state of the art in the literature, standardization, and in the
 market. We pay special attention to 3rd Generation Partnership Project (3GPP)
 evolution in the area of network management and to the data that can be
 extracted from 3GPP networks, in order to gain knowledge and experience in how
 the network is working, and improve network performance in a proactive way.
 Finally, we go through the main challenges associated with this line of
 research, in both 4G and in what 5G is getting designed, while identifying new
 directions for research.",1,0,0,0,0,0,
260,Cyber Risk Analysis of Combined Data Attacks Against Power System State Estimation,"Understanding smart grid cyber attacks is key for developing appropriate
 protection and recovery measures. Advanced attacks pursue maximized impact at
 minimized costs and detectability. This paper conducts risk analysis of
 combined data integrity and availability attacks against the power system state
 estimation. We compare the combined attacks with pure integrity attacks - false
 data injection (FDI) attacks. A security index for vulnerability assessment to
 these two kinds of attacks is proposed and formulated as a mixed integer linear
 programming problem. We show that such combined attacks can succeed with fewer
 resources than FDI attacks. The combined attacks with limited knowledge of the
 system model also expose advantages in keeping stealth against the bad data
 detection. Finally, the risk of combined attacks to reliable system operation
 is evaluated using the results from vulnerability assessment and attack impact
 analysis. The findings in this paper are validated and supported by a detailed
 case study.",1,0,0,0,0,0,
261,A New Family of Near-metrics for Universal Similarity,"We propose a family of near-metrics based on local graph diffusion to capture
 similarity for a wide class of data sets. These quasi-metametrics, as their
 names suggest, dispense with one or two standard axioms of metric spaces,
 specifically distinguishability and symmetry, so that similarity between data
 points of arbitrary type and form could be measured broadly and effectively.
 The proposed near-metric family includes the forward k-step diffusion and its
 reverse, typically on the graph consisting of data objects and their features.
 By construction, this family of near-metrics is particularly appropriate for
 categorical data, continuous data, and vector representations of images and
 text extracted via deep learning approaches. We conduct extensive experiments
 to evaluate the performance of this family of similarity measures and compare
 and contrast with traditional measures of similarity used for each specific
 application and with the ground truth when available. We show that for
 structured data including categorical and continuous data, the near-metrics
 corresponding to normalized forward k-step diffusion (k small) work as one of
 the best performing similarity measures; for vector representations of text and
 images including those extracted from deep learning, the near-metrics derived
 from normalized and reverse k-step graph diffusion (k very small) exhibit
 outstanding ability to distinguish data points from different classes.",1,0,0,1,0,0,
262,Poisoning Attacks to Graph-Based Recommender Systems,"Recommender system is an important component of many web services to help
 users locate items that match their interests. Several studies showed that
 recommender systems are vulnerable to poisoning attacks, in which an attacker
 injects fake data to a given system such that the system makes recommendations
 as the attacker desires. However, these poisoning attacks are either agnostic
 to recommendation algorithms or optimized to recommender systems that are not
 graph-based. Like association-rule-based and matrix-factorization-based
 recommender systems, graph-based recommender system is also deployed in
 practice, e.g., eBay, Huawei App Store. However, how to design optimized
 poisoning attacks for graph-based recommender systems is still an open problem.
 In this work, we perform a systematic study on poisoning attacks to graph-based
 recommender systems. Due to limited resources and to avoid detection, we assume
 the number of fake users that can be injected into the system is bounded. The
 key challenge is how to assign rating scores to the fake users such that the
 target item is recommended to as many normal users as possible. To address the
 challenge, we formulate the poisoning attacks as an optimization problem,
 solving which determines the rating scores for the fake users. We also propose
 techniques to solve the optimization problem. We evaluate our attacks and
 compare them with existing attacks under white-box (recommendation algorithm
 and its parameters are known), gray-box (recommendation algorithm is known but
 its parameters are unknown), and black-box (recommendation algorithm is
 unknown) settings using two real-world datasets. Our results show that our
 attack is effective and outperforms existing attacks for graph-based
 recommender systems. For instance, when 1% fake users are injected, our attack
 can make a target item recommended to 580 times more normal users in certain
 scenarios.",0,0,0,1,0,0,
263,SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models,"This paper describes the Stockholm University/University of Groningen
 (SU-RUG) system for the SIGMORPHON 2017 shared task on morphological
 inflection. Our system is based on an attentional sequence-to-sequence neural
 network model using Long Short-Term Memory (LSTM) cells, with joint training of
 morphological inflection and the inverse transformation, i.e. lemmatization and
 morphological analysis. Our system outperforms the baseline with a large
 margin, and our submission ranks as the 4th best team for the track we
 participate in (task 1, high-resource).",1,0,0,0,0,0,
264,"Neural system identification for large populations separating ""what"" and ""where""","Neuroscientists classify neurons into different types that perform similar
 computations at different locations in the visual field. Traditional methods
 for neural system identification do not capitalize on this separation of 'what'
 and 'where'. Learning deep convolutional feature spaces that are shared among
 many neurons provides an exciting path forward, but the architectural design
 needs to account for data limitations: While new experimental techniques enable
 recordings from thousands of neurons, experimental time is limited so that one
 can sample only a small fraction of each neuron's response space. Here, we show
 that a major bottleneck for fitting convolutional neural networks (CNNs) to
 neural data is the estimation of the individual receptive field locations, a
 problem that has been scratched only at the surface thus far. We propose a CNN
 architecture with a sparse readout layer factorizing the spatial (where) and
 feature (what) dimensions. Our network scales well to thousands of neurons and
 short recordings and can be trained end-to-end. We evaluate this architecture
 on ground-truth data to explore the challenges and limitations of CNN-based
 system identification. Moreover, we show that our network model outperforms
 current state-of-the art system identification models of mouse primary visual
 cortex.",1,0,0,1,0,0,
265,On the Deployment of Distributed Antennas for Wireless Power Transfer with Safety Electromagnetic Radiation Level Requirement,"The extremely low efficiency is regarded as the bottleneck of Wireless Power
 Transfer (WPT) technology. To tackle this problem, either enlarging the
 transfer power or changing the infrastructure of WPT system could be an
 intuitively proposed way. However, the drastically important issue on the user
 exposure of electromagnetic radiation is rarely considered while we try to
 improve the efficiency of WPT. In this paper, a Distributed Antenna Power
 Beacon (DA-PB) based WPT system where these antennas are uniformly distributed
 on a circle is analyzed and optimized with the safety electromagnetic radiation
 level (SERL) requirement. In this model, three key questions are intended to be
 answered: 1) With the SERL, what is the performance of the harvested power at
 the users ? 2) How do we configure the parameters to maximize the efficiency of
 WPT? 3) Under the same constraints, does the DA-PB still have performance gain
 than the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of
 DA-PB is derived to make the radio frequency (RF) electromagnetic radiation
 power density at any location of the charging cell lower than the SERL
 published by the Federal Communications Commission (FCC). Second, the
 closed-form expressions of average harvested Direct Current (DC) power per user
 in the charging cell for pass-loss exponent 2 and 4 are also provided. In order
 to maximize the average efficiency of WPT, the optimal radii for distributed
 antennas elements (DAEs) are derived when the pass-loss exponent takes the
 typical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a
 benchmark. Simulation results verify our derived theoretical results. And it is
 shown that the proposed DA-PB indeed achieves larger average harvested DC power
 than CA-PB and can improve the efficiency of WPT.",1,0,0,0,0,0,
266,A simulation technique for slurries interacting with moving parts and deformable solids with applications,"A numerical method for particle-laden fluids interacting with a deformable
 solid domain and mobile rigid parts is proposed and implemented in a full
 engineering system. The fluid domain is modeled with a lattice Boltzmann
 representation, the particles and rigid parts are modeled with a discrete
 element representation, and the deformable solid domain is modeled using a
 Lagrangian mesh. The main issue of this work, since separately each of these
 methods is a mature tool, is to develop coupling and model-reduction approaches
 in order to efficiently simulate coupled problems of this nature, as occur in
 various geological and engineering applications. The lattice Boltzmann method
 incorporates a large-eddy simulation technique using the Smagorinsky turbulence
 model. The discrete element method incorporates spherical and polyhedral
 particles for stiff contact interactions. A neo-Hookean hyperelastic model is
 used for the deformable solid. We provide a detailed description of how to
 couple the three solvers within a unified algorithm. The technique we propose
 for rubber modeling/coupling exploits a simplification that prevents having to
 solve a finite-element problem each time step. We also develop a technique to
 reduce the domain size of the full system by replacing certain zones with
 quasi-analytic solutions, which act as effective boundary conditions for the
 lattice Boltzmann method. The major ingredients of the routine are are
 separately validated. To demonstrate the coupled method in full, we simulate
 slurry flows in two kinds of piston-valve geometries. The dynamics of the valve
 and slurry are studied and reported over a large range of input parameters.",1,0,0,0,0,0,
267,Dissipative hydrodynamics in superspace,"We construct a Schwinger-Keldysh effective field theory for relativistic
 hydrodynamics for charged matter in a thermal background using a superspace
 formalism. Superspace allows us to efficiently impose the symmetries of the
 problem and to obtain a simple expression for the effective action. We show
 that the theory we obtain is compatible with the Kubo-Martin-Schwinger
 condition, which in turn implies that Green's functions obey the
 fluctuation-dissipation theorem. Our approach complements and extends existing
 formulations found in the literature.",0,1,0,0,0,0,
268,The Two-fold Role of Observables in Classical and Quantum Kinematics,"Observables have a dual nature in both classical and quantum kinematics: they
 are at the same time \emph{quantities}, allowing to separate states by means of
 their numerical values, and \emph{generators of transformations}, establishing
 relations between different states. In this work, we show how this two-fold
 role of observables constitutes a key feature in the conceptual analysis of
 classical and quantum kinematics, shedding a new light on the distinguishing
 feature of the quantum at the kinematical level. We first take a look at the
 algebraic description of both classical and quantum observables in terms of
 Jordan-Lie algebras and show how the two algebraic structures are the precise
 mathematical manifestation of the two-fold role of observables. Then, we turn
 to the geometric reformulation of quantum kinematics in terms of K??hler
 manifolds. A key achievement of this reformulation is to show that the two-fold
 role of observables is the constitutive ingredient defining what an observable
 is. Moreover, it points to the fact that, from the restricted point of view of
 the transformational role of observables, classical and quantum kinematics
 behave in exactly the same way. Finally, we present Landsman's general
 framework of Poisson spaces with transition probability, which highlights with
 unmatched clarity that the crucial difference between the two kinematics lies
 in the way the two roles of observables are related to each other.",0,1,0,0,0,0,
269,On the isoperimetric quotient over scalar-flat conformal classes,"Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with
 smooth boundary $\partial M$. Suppose that $(M,g)$ admits a scalar-flat
 conformal metric. We prove that the supremum of the isoperimetric quotient over
 the scalar-flat conformal class is strictly larger than the best constant of
 the isoperimetric inequality in the Euclidean space, and consequently is
 achieved, if either (i) $n\ge 12$ and $\partial M$ has a nonumbilic point; or
 (ii) $n\ge 10$, $\partial M$ is umbilic and the Weyl tensor does not vanish at
 some boundary point.",0,0,1,0,0,0,
270,On the Spectrum of Random Features Maps of High Dimensional Data,"Random feature maps are ubiquitous in modern statistical machine learning,
 where they generalize random projections by means of powerful, yet often
 difficult to analyze nonlinear operators. In this paper, we leverage the
 ""concentration"" phenomenon induced by random matrix theory to perform a
 spectral analysis on the Gram matrix of these random feature maps, here for
 Gaussian mixture models of simultaneously large dimension and size. Our results
 are instrumental to a deeper understanding on the interplay of the nonlinearity
 and the statistics of the data, thereby allowing for a better tuning of random
 feature-based techniques.",0,0,0,1,0,0,
271,Minimum energy path calculations with Gaussian process regression,"The calculation of minimum energy paths for transitions such as atomic and/or
 spin re-arrangements is an important task in many contexts and can often be
 used to determine the mechanism and rate of transitions. An important challenge
 is to reduce the computational effort in such calculations, especially when ab
 initio or electron density functional calculations are used to evaluate the
 energy since they can require large computational effort. Gaussian process
 regression is used here to reduce significantly the number of energy
 evaluations needed to find minimum energy paths of atomic rearrangements. By
 using results of previous calculations to construct an approximate energy
 surface and then converge to the minimum energy path on that surface in each
 Gaussian process iteration, the number of energy evaluations is reduced
 significantly as compared with regular nudged elastic band calculations. For a
 test problem involving rearrangements of a heptamer island on a crystal
 surface, the number of energy evaluations is reduced to less than a fifth. The
 scaling of the computational effort with the number of degrees of freedom as
 well as various possible further improvements to this approach are discussed.",0,1,0,1,0,0,
272,Evaluating Roles of Central Users in Online Communication Networks: A Case Study of #PanamaLeaks,"Social media has changed the ways of communication, where everyone is
 equipped with the power to express their opinions to others in online
 discussion platforms. Previously, a number of stud- ies have been presented to
 identify opinion leaders in online discussion networks. Feng (""Are you
 connected? Evaluating information cascade in online discussion about the
 #RaceTogether campaign"", Computers in Human Behavior, 2016) identified five
 types of central users and their communication patterns in an online
 communication network of a limited time span. However, to trace the change in
 communication pattern, a long-term analysis is required. In this study, we
 critically analyzed framework presented by Feng based on five types of central
 users in online communication network and their communication pattern in a
 long-term manner. We take another case study presented by Udnor et al.
 (""Determining social media impact on the politics of developing countries using
 social network analytics"", Program, 2016) to further understand the dynamics as
 well as to perform validation . Results indicate that there may not exist all
 of these central users in an online communication network in a long-term
 manner. Furthermore, we discuss the changing positions of opinion leaders and
 their power to keep isolates interested in an online discussion network.",1,1,0,1,0,0,
273,Best polynomial approximation on the triangle,"Let $E_n(f)_{\alpha,\beta,\gamma}$ denote the error of best approximation by
 polynomials of degree at most $n$ in the space
 $L^2(\varpi_{\alpha,\beta,\gamma})$ on the triangle $\{(x,y): x, y \ge 0, x+y
 \le 1\}$, where $\varpi_{\alpha,\beta,\gamma}(x,y) := x^\alpha y ^\beta
 (1-x-y)^\gamma$ for $\alpha,\beta,\gamma > -1$. Our main result gives a sharp
 estimate of $E_n(f)_{\alpha,\beta,\gamma}$ in terms of the error of best
 approximation for higher order derivatives of $f$ in appropriate Sobolev
 spaces. The result also leads to a characterization of
 $E_n(f)_{\alpha,\beta,\gamma}$ by a weighted $K$-functional.",0,0,1,0,0,0,
274,SecureTime: Secure Multicast Time Synchronization,"Due to the increasing dependency of critical infrastructure on synchronized
 clocks, network time synchronization protocols have become an attractive target
 for attackers. We identify data origin authentication as the key security
 objective and suggest to employ recently proposed high-performance digital
 signature schemes (Ed25519 and MQQ-SIG)) as foundation of a novel set of
 security measures to secure multicast time synchronization. We conduct
 experiments to verify the computational and communication efficiency for using
 these signatures in the standard time synchronization protocols NTP and PTP. We
 propose additional security measures to prevent replay attacks and to mitigate
 delay attacks. Our proposed solutions cover 1-step mode for NTP and PTP and we
 extend our security measures specifically to 2-step mode (PTP) and show that
 they have no impact on time synchronization's precision.",1,0,0,0,0,0,
275,Solving the multi-site and multi-orbital Dynamical Mean Field Theory using Density Matrix Renormalization,"We implement an efficient numerical method to calculate response functions of
 complex impurities based on the Density Matrix Renormalization Group (DMRG) and
 use it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This
 method uses the correction vector to obtain precise Green's functions on the
 real frequency axis at zero temperature. By using a self-consistent bath
 configuration with very low entanglement, we take full advantage of the DMRG to
 calculate dynamical response functions paving the way to treat large effective
 impurities such as those corresponding to multi-orbital interacting models and
 multi-site or multi-momenta clusters. This method leads to reliable
 calculations of non-local self energies at arbitrary dopings and interactions
 and at any energy scale.",0,1,0,0,0,0,
276,Topologically Invariant Double Dirac States in Bismuth based Perovskites: Consequence of Ambivalent Charge States and Covalent Bonding,"Bulk and surface electronic structures, calculated using density functional
 theory and a tight-binding model Hamiltonian, reveal the existence of two
 topologically invariant (TI) surface states in the family of cubic Bi
 perovskites (ABiO$_3$; A = Na, K, Rb, Cs, Mg, Ca, Sr and Ba). The two TI
 states, one lying in the valence band (TI-V) and other lying in the conduction
 band (TI-C) are formed out of bonding and antibonding states of the
 Bi-$\{$s,p$\}$ - O-$\{$p$\}$ coordinated covalent interaction. Below a certain
 critical thickness of the film, which varies with A, TI states of top and
 bottom surfaces couple to destroy the Dirac type linear dispersion and
 consequently to open surface energy gaps. The origin of s-p band inversion,
 necessary to form a TI state, classifies the family of ABiO$_3$ into two. For
 class-I (A = Na, K, Rb, Cs and Mg) the band inversion, leading to TI-C state,
 is induced by spin-orbit coupling of the Bi-p states and for class-II (A = Ca,
 Sr and Ba) the band inversion is induced through weak but sensitive second
 neighbor Bi-Bi interactions.",0,1,0,0,0,0,
277,Identitas: A Better Way To Be Meaningless,"It is often recommended that identifiers for ontology terms should be
 semantics-free or meaningless. In practice, ontology developers tend to use
 numeric identifiers, starting at 1 and working upwards. In this paper we
 present a critique of current ontology semantics-free identifiers;
 monotonically increasing numbers have a number of significant usability flaws
 which make them unsuitable as a default option, and we present a series of
 alternatives. We have provide an implementation of these alternatives which can
 be freely combined.",1,0,0,0,0,0,
278,Learning from Between-class Examples for Deep Sound Recognition,"Deep learning methods have achieved high performance in sound recognition
 tasks. Deciding how to feed the training data is important for further
 performance improvement. We propose a novel learning method for deep sound
 recognition: Between-Class learning (BC learning). Our strategy is to learn a
 discriminative feature space by recognizing the between-class sounds as
 between-class sounds. We generate between-class sounds by mixing two sounds
 belonging to different classes with a random ratio. We then input the mixed
 sound to the model and train the model to output the mixing ratio. The
 advantages of BC learning are not limited only to the increase in variation of
 the training data; BC learning leads to an enlargement of Fisher's criterion in
 the feature space and a regularization of the positional relationship among the
 feature distributions of the classes. The experimental results show that BC
 learning improves the performance on various sound recognition networks,
 datasets, and data augmentation schemes, in which BC learning proves to be
 always beneficial. Furthermore, we construct a new deep sound recognition
 network (EnvNet-v2) and train it with BC learning. As a result, we achieved a
 performance surpasses the human level.",1,0,0,1,0,0,
279,DAGGER: A sequential algorithm for FDR control on DAGs,"We propose a linear-time, single-pass, top-down algorithm for multiple
 testing on directed acyclic graphs (DAGs), where nodes represent hypotheses and
 edges specify a partial ordering in which hypotheses must be tested. The
 procedure is guaranteed to reject a sub-DAG with bounded false discovery rate
 (FDR) while satisfying the logical constraint that a rejected node's parents
 must also be rejected. It is designed for sequential testing settings, when the
 DAG structure is known a priori, but the $p$-values are obtained selectively
 (such as in a sequence of experiments), but the algorithm is also applicable in
 non-sequential settings when all $p$-values can be calculated in advance (such
 as variable/model selection). Our DAGGER algorithm, shorthand for Greedily
 Evolving Rejections on DAGs, provably controls the false discovery rate under
 independence, positive dependence or arbitrary dependence of the $p$-values.
 The DAGGER procedure specializes to known algorithms in the special cases of
 trees and line graphs, and simplifies to the classical Benjamini-Hochberg
 procedure when the DAG has no edges. We explore the empirical performance of
 DAGGER using simulations, as well as a real dataset corresponding to a gene
 ontology, showing favorable performance in terms of time and power.",0,0,1,1,0,0,
280,On nonlinear profile decompositions and scattering for a NLS-ODE model,"In this paper, we consider a Hamiltonian system combining a nonlinear Schr\""
 odinger equation (NLS) and an ordinary differential equation (ODE). This system
 is a simplified model of the NLS around soliton solutions. Following Nakanishi
 \cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.
 The proof is based on Nakanishi's framework and Fermi Golden Rule estimates on
 $L^4$ in time norms.",0,0,1,0,0,0,
281,Blockchain and human episodic memory,"We relate the concepts used in decentralized ledger technology to studies of
 episodic memory in the mammalian brain. Specifically, we introduce the standard
 concepts of linked list, hash functions, and sharding, from computer science.
 We argue that these concepts may be more relevant to studies of the neural
 mechanisms of memory than has been previously appreciated. In turn, we also
 highlight that certain phenomena studied in the brain, namely metacognition,
 reality monitoring, and how perceptual conscious experiences come about, may
 inspire development in blockchain technology too, specifically regarding
 probabilistic consensus protocols.",0,0,0,0,1,0,
282,Epidemic Spreading and Aging in Temporal Networks with Memory,"Time-varying network topologies can deeply influence dynamical processes
 mediated by them. Memory effects in the pattern of interactions among
 individuals are also known to affect how diffusive and spreading phenomena take
 place. In this paper we analyze the combined effect of these two ingredients on
 epidemic dynamics on networks. We study the susceptible-infected-susceptible
 (SIS) and the susceptible-infected-removed (SIR) models on the recently
 introduced activity-driven networks with memory. By means of an activity-based
 mean-field approach we derive, in the long time limit, analytical predictions
 for the epidemic threshold as a function of the parameters describing the
 distribution of activities and the strength of the memory effects. Our results
 show that memory reduces the threshold, which is the same for SIS and SIR
 dynamics, therefore favouring epidemic spreading. The theoretical approach
 perfectly agrees with numerical simulations in the long time asymptotic regime.
 Strong aging effects are present in the preasymptotic regime and the epidemic
 threshold is deeply affected by the starting time of the epidemics. We discuss
 in detail the origin of the model-dependent preasymptotic corrections, whose
 understanding could potentially allow for epidemic control on correlated
 temporal networks.",1,0,0,0,1,0,
283,"The Shattered Gradients Problem: If resnets are the answer, then what is the question?","A long-standing obstacle to progress in deep learning is the problem of
 vanishing and exploding gradients. Although, the problem has largely been
 overcome via carefully constructed initializations and batch normalization,
 architectures incorporating skip-connections such as highway and resnets
 perform much better than standard feedforward architectures despite well-chosen
 initialization and batch normalization. In this paper, we identify the
 shattered gradients problem. Specifically, we show that the correlation between
 gradients in standard feedforward networks decays exponentially with depth
 resulting in gradients that resemble white noise whereas, in contrast, the
 gradients in architectures with skip-connections are far more resistant to
 shattering, decaying sublinearly. Detailed empirical evidence is presented in
 support of the analysis, on both fully-connected networks and convnets.
 Finally, we present a new ""looks linear"" (LL) initialization that prevents
 shattering, with preliminary experiments showing the new initialization allows
 to train very deep networks without the addition of skip-connections.",1,0,0,1,0,0,
284,Pr$_2$Ir$_2$O$_7$: when Luttinger semimetal meets Melko-Hertog-Gingras spin ice state,"We study the band structure topology and engineering from the interplay
 between local moments and itinerant electrons in the context of pyrochlore
 iridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction
 electrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.
 While the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned
 into an ordered spin ice with a finite ordering wavevector, dubbed
 ""Melko-Hertog-Gingras"" state, by varying Ir and O contents. We point out that
 the ordered spin ice of the Pr local moments generates an internal magnetic
 field that reconstructs the band structure of the Luttinger semimetal. Besides
 the broad existence of Weyl nodes, we predict that the magnetic translation of
 the ""Melko-Hertog-Gingras"" state for the Pr moments protects the Dirac band
 touching at certain time reversal invariant momenta for the Ir conduction
 electrons. We propose the magnetic fields to control the Pr magnetic structure
 and thereby indirectly influence the topological and other properties of the Ir
 electrons. Our prediction may be immediately tested in the ordered
 Pr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed
 examination of the band structure, magneto-transport, and other properties of
 Pr$_2$Ir$_2$O$_7$.",0,1,0,0,0,0,
285,A 2-edge partial inverse problem for the Sturm-Liouville operators with singular potentials on a star-shaped graph,"Boundary value problems for Sturm-Liouville operators with potentials from
 the class $W_2^{-1}$ on a star-shaped graph are considered. We assume that the
 potentials are known on all the edges of the graph except two, and show that
 the potentials on the remaining edges can be constructed by fractional parts of
 two spectra. A uniqueness theorem is proved, and an algorithm for the
 constructive solution of the partial inverse problem is provided. The main
 ingredient of the proofs is the Riesz-basis property of specially constructed
 systems of functions.",0,0,1,0,0,0,
286,Jastrow form of the Ground State Wave Functions for Fractional Quantum Hall States,"The topological morphology--order of zeros at the positions of electrons with
 respect to a specific electron--of Laughlin state at filling fractions $1/m$
 ($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the
 positions of other electrons. Although fairly accurate ground state wave
 functions for most of the other quantum Hall states in the lowest Landau level
 are quite well-known, it had been an open problem in expressing the ground
 state wave functions in terms of flux-attachment to particles, {\em a la}, this
 morphology of Laughlin state. With a very general consideration of
 flux-particle relations only, in spherical geometry, we here report a novel
 method for determining morphologies of these states. Based on these, we
 construct almost exact ground state wave-functions for the Coulomb interaction.
 Although the form of interaction may change the ground state wave-function, the
 same morphology constructs the latter irrespective of the nature of the
 interaction between electrons.",0,1,0,0,0,0,
287,On a common refinement of Stark units and Gross-Stark units,"The purpose of this paper is to formulate and study a common refinement of a
 version of Stark's conjecture and its $p$-adic analogue, in terms of Fontaine's
 $p$-adic period ring and $p$-adic Hodge theory. We construct period-ring-valued
 functions under a generalization of Yoshida's conjecture on the transcendental
 parts of CM-periods. Then we conjecture a reciprocity law on their special
 values concerning the absolute Frobenius action. We show that our conjecture
 implies a part of Stark's conjecture when the base field is an arbitrary real
 field and the splitting place is its real place. It also implies a refinement
 of the Gross-Stark conjecture under a certain assumption. When the base field
 is the rational number field, our conjecture follows from Coleman's formula on
 Fermat curves. We also prove some partial results in other cases.",0,0,1,0,0,0,
288,An Integrated Decision and Control Theoretic Solution to Multi-Agent Co-Operative Search Problems,"This paper considers the problem of autonomous multi-agent cooperative target
 search in an unknown environment using a decentralized framework under a
 no-communication scenario. The targets are considered as static targets and the
 agents are considered to be homogeneous. The no-communication scenario
 translates as the agents do not exchange either the information about the
 environment or their actions among themselves. We propose an integrated
 decision and control theoretic solution for a search problem which generates
 feasible agent trajectories. In particular, a perception based algorithm is
 proposed which allows an agent to estimate the probable strategies of other
 agents' and to choose a decision based on such estimation. The algorithm shows
 robustness with respect to the estimation accuracy to a certain degree. The
 performance of the algorithm is compared with random strategies and numerical
 simulation shows considerable advantages.",1,0,0,0,0,0,
289,Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan,"This study explores the validity of chain effects of clean water, which are
 known as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan.
 Recent studies have reported that water purifications systems are responsible
 for huge contributions to human capital. Although a few studies have
 investigated the short-term effects of water-supply systems in pre-war Japan,
 little is known about the benefits associated with these systems. By analyzing
 city-level cause-specific mortality data from the years 1922-1940, we found
 that eliminating typhoid fever infections decreased the risk of deaths due to
 non-waterborne diseases. Our estimates show that for one additional typhoid
 death, there were approximately one to three deaths due to other causes, such
 as tuberculosis and pneumonia. This suggests that the observed Mills-Reincke
 phenomenon could have resulted from the prevention typhoid fever in a
 previously-developing Asian country.",0,0,0,1,1,0,
290,Learning Transferable Architectures for Scalable Image Recognition,"Developing neural network image classification models often requires
 significant architecture engineering. In this paper, we study a method to learn
 the model architectures directly on the dataset of interest. As this approach
 is expensive when the dataset is large, we propose to search for an
 architectural building block on a small dataset and then transfer the block to
 a larger dataset. The key contribution of this work is the design of a new
 search space (the ""NASNet search space"") which enables transferability. In our
 experiments, we search for the best convolutional layer (or ""cell"") on the
 CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking
 together more copies of this cell, each with their own parameters to design a
 convolutional architecture, named ""NASNet architecture"". We also introduce a
 new regularization technique called ScheduledDropPath that significantly
 improves generalization in the NASNet models. On CIFAR-10 itself, NASNet
 achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet
 achieves, among the published works, state-of-the-art accuracy of 82.7% top-1
 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than
 the best human-invented architectures while having 9 billion fewer FLOPS - a
 reduction of 28% in computational demand from the previous state-of-the-art
 model. When evaluated at different levels of computational cost, accuracies of
 NASNets exceed those of the state-of-the-art human-designed models. For
 instance, a small version of NASNet also achieves 74% top-1 accuracy, which is
 3.1% better than equivalently-sized, state-of-the-art models for mobile
 platforms. Finally, the learned features by NASNet used with the Faster-RCNN
 framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO
 dataset.",1,0,0,0,0,0,
291,Fast Multi-frame Stereo Scene Flow with Motion Segmentation,"We propose a new multi-frame method for efficiently computing scene flow
 (dense depth and optical flow) and camera ego-motion for a dynamic scene
 observed from a moving stereo camera rig. Our technique also segments out
 moving objects from the rigid scene. In our method, we first estimate the
 disparity map and the 6-DOF camera motion using stereo matching and visual
 odometry. We then identify regions inconsistent with the estimated camera
 motion and compute per-pixel optical flow only at these regions. This flow
 proposal is fused with the camera motion-based flow proposal using fusion moves
 to obtain the final optical flow and motion segmentation. This unified
 framework benefits all four tasks - stereo, optical flow, visual odometry and
 motion segmentation leading to overall higher accuracy and efficiency. Our
 method is currently ranked third on the KITTI 2015 scene flow benchmark.
 Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3
 orders of magnitude faster than the top six methods. We also report a thorough
 evaluation on challenging Sintel sequences with fast camera and object motion,
 where our method consistently outperforms OSF [Menze and Geiger, 2015], which
 is currently ranked second on the KITTI benchmark.",1,0,0,0,0,0,
292,Pointed $p^2q$-dimensional Hopf algebras in positive characteristic,"Let $\K$ be an algebraically closed field of positive characteristic $p$. We
 mainly classify pointed Hopf algebras over $\K$ of dimension $p^2q$, $pq^2$ and
 $pqr$ where $p,q,r$ are distinct prime numbers. We obtain a complete
 classification of such Hopf algebras except two subcases when they are not
 generated by the first terms of coradical filtration. In particular, we obtain
 many new examples of non-commutative and non-cocommutative finite-dimensional
 Hopf algebras.",0,0,1,0,0,0,
293,Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems,"We present the mixed Galerkin discretization of distributed parameter
 port-Hamiltonian systems. On the prototypical example of hyperbolic systems of
 two conservation laws in arbitrary spatial dimension, we derive the main
 contributions: (i) A weak formulation of the underlying geometric
 (Stokes-Dirac) structure with a segmented boundary according to the causality
 of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac
 structure by a finite-dimensional Dirac structure is realized using a mixed
 Galerkin approach and power-preserving linear maps, which define minimal
 discrete power variables. (iii) With a consistent approximation of the
 Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.
 By the degrees of freedom in the power-preserving maps, the resulting family of
 structure-preserving schemes allows for trade-offs between centered
 approximations and upwinding. We illustrate the method on the example of
 Whitney finite elements on a 2D simplicial triangulation and compare the
 eigenvalue approximation in 1D with a related approach.",1,0,0,0,0,0,
294,Clamped seismic metamaterials: Ultra-low broad frequency stop-bands,"The regularity of earthquakes, their destructive power, and the nuisance of
 ground vibration in urban environments, all motivate designs of defence
 structures to lessen the impact of seismic and ground vibration waves on
 buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and
 up to a few tens of Hz for vibrations generated by human activities, cause a
 large amount of damage, or inconvenience, depending on the geological
 conditions they can travel considerable distances and may match the resonant
 fundamental frequency of buildings. The ultimate aim of any seismic
 metamaterial, or any other seismic shield, is to protect over this entire range
 of frequencies, the long wavelengths involved, and low frequency, have meant
 this has been unachievable to date.
 Elastic flexural waves, applicable in the mechanical vibrations of thin
 elastic plates, can be designed to have a broad zero-frequency stop-band using
 a periodic array of very small clamped circles. Inspired by this experimental
 and theoretical observation, all be it in a situation far removed from seismic
 waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)
 and body (pressure P and shear S) wave reflectors at very large wavelengths in
 structured soils modelled as a fully elastic layer periodically clamped to
 bedrock.
 We identify zero frequency stop-bands that only exist in the limit of columns
 of concrete clamped at their base to the bedrock. In a realistic configuration
 of a sedimentary basin 15 meters deep we observe a zero frequency stop-band
 covering a broad frequency range of $0$ to $30$ Hz.",0,1,0,0,0,0,
295,Difference analogue of second main theorems for meromorphic mapping into algebraic variety,"In this paper, we prove some difference analogue of second main theorems of
 meromorphic mapping from Cm into an algebraic variety V intersecting a finite
 set of fixed hypersurfaces in subgeneral position. As an application, we prove
 a result on algebraically degenerate of holomorphic curves intersecting
 hypersurfaces and difference analogue of Picard's theorem on holomorphic
 curves. Furthermore, we obtain a second main theorem of meromorphic mappings
 intersecting hypersurfaces in N-subgeneral position for Veronese embedding in
 Pn(C) and a uniqueness theorem sharing hypersurfaces.",0,0,1,0,0,0,
296,An Effective Way to Improve YouTube-8M Classification Accuracy in Google Cloud Platform,"Large-scale datasets have played a significant role in progress of neural
 network and deep learning areas. YouTube-8M is such a benchmark dataset for
 general multi-label video classification. It was created from over 7 million
 YouTube videos (450,000 hours of video) and includes video labels from a
 vocabulary of 4716 classes (3.4 labels/video on average). It also comes with
 pre-extracted audio & visual features from every second of video (3.2 billion
 feature vectors in total). Google cloud recently released the datasets and
 organized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.
 Competitors are challenged to develop classification algorithms that assign
 video-level labels using the new and improved Youtube-8M V2 dataset. Inspired
 by the competition, we started exploration of audio understanding and
 classification using deep learning algorithms and ensemble methods. We built
 several baseline predictions according to the benchmark paper and public github
 tensorflow code. Furthermore, we improved global prediction accuracy (GAP) from
 base level 77% to 80.7% through approaches of ensemble.",1,0,0,1,0,0,
297,Experimental Design of a Prescribed Burn Instrumentation,"Observational data collected during experiments, such as the planned Fire and
 Smoke Model Evaluation Experiment (FASMEE), are critical for progressing and
 transitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM
 into operational use. Historical meteorological data, representing typical
 weather conditions for the anticipated burn locations and times, have been
 processed to initialize and run a set of simulations representing the planned
 experimental burns. Based on an analysis of these numerical simulations, this
 paper provides recommendations on the experimental setup that include the
 ignition procedures, size and duration of the burns, and optimal sensor
 placement. New techniques are developed to initialize coupled fire-atmosphere
 simulations with weather conditions typical of the planned burn locations and
 time of the year. Analysis of variation and sensitivity analysis of simulation
 design to model parameters by repeated Latin Hypercube Sampling are used to
 assess the locations of the sensors. The simulations provide the locations of
 the measurements that maximize the expected variation of the sensor outputs
 with the model parameters.",0,0,0,1,0,0,
298,Seifert surgery on knots via Reidemeister torsion and Casson-Walker-Lescop invariant III,"For a knot $K$ in a homology $3$-sphere $\Sigma$, let $M$ be the result of
 $2/q$-surgery on $K$, and let $X$ be the universal abelian covering of $M$. Our
 first theorem is that if the first homology of $X$ is finite cyclic and $M$ is
 a Seifert fibered space with $N\ge 3$ singular fibers, then $N\ge 4$ if and
 only if the first homology of the universal abelian covering of $X$ is
 infinite. Our second theorem is that under an appropriate assumption on the
 Alexander polynomial of $K$, if $M$ is a Seifert fibered space, then $q=\pm 1$
 (i.e.\ integral surgery).",0,0,1,0,0,0,
299,Sparse mean localization by information theory,"Sparse feature selection is necessary when we fit statistical models, we have
 access to a large group of features, don't know which are relevant, but assume
 that most are not. Alternatively, when the number of features is larger than
 the available data the model becomes over parametrized and the sparse feature
 selection task involves selecting the most informative variables for the model.
 When the model is a simple location model and the number of relevant features
 does not grow with the total number of features, sparse feature selection
 corresponds to sparse mean estimation. We deal with a simplified mean
 estimation problem consisting of an additive model with gaussian noise and mean
 that is in a restricted, finite hypothesis space. This restriction simplifies
 the mean estimation problem into a selection problem of combinatorial nature.
 Although the hypothesis space is finite, its size is exponential in the
 dimension of the mean. In limited data settings and when the size of the
 hypothesis space depends on the amount of data or on the dimension of the data,
 choosing an approximation set of hypotheses is a desirable approach. Choosing a
 set of hypotheses instead of a single one implies replacing the bias-variance
 trade off with a resolution-stability trade off. Generalization capacity
 provides a resolution selection criterion based on allowing the learning
 algorithm to communicate the largest amount of information in the data to the
 learner without error. In this work the theory of approximation set coding and
 generalization capacity is explored in order to understand this approach. We
 then apply the generalization capacity criterion to the simplified sparse mean
 estimation problem and detail an importance sampling algorithm which at once
 solves the difficulty posed by large hypothesis spaces and the slow convergence
 of uniform sampling algorithms.",0,0,0,1,0,0,
300,Joint Power and Admission Control based on Channel Distribution Information: A Novel Two-Timescale Approach,"In this letter, we consider the joint power and admission control (JPAC)
 problem by assuming that only the channel distribution information (CDI) is
 available. Under this assumption, we formulate a new chance (probabilistic)
 constrained JPAC problem, where the signal to interference plus noise ratio
 (SINR) outage probability of the supported links is enforced to be not greater
 than a prespecified tolerance. To efficiently deal with the chance SINR
 constraint, we employ the sample approximation method to convert them into
 finitely many linear constraints. Then, we propose a convex approximation based
 deflation algorithm for solving the sample approximation JPAC problem. Compared
 to the existing works, this letter proposes a novel two-timescale JPAC
 approach, where admission control is performed by the proposed deflation
 algorithm based on the CDI in a large timescale and transmission power is
 adapted instantly with fast fadings in a small timescale. The effectiveness of
 the proposed algorithm is illustrated by simulations.",1,0,1,0,0,0,
301,A Closer Look at the Alpha Persei Coronal Conundrum,"A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest
 star, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness
 were similar to coronally active late-type dwarf members. Later, in 2010, a
 Hubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found
 far-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious
 offset of the ROSAT source, suggested that a late-type companion might be
 responsible for the X-rays. Recently, a multi-faceted program tested that
 premise. Groundbased optical coronography, and near-UV imaging with HST Wide
 Field Camera 3, searched for any close-in faint candidate coronal objects, but
 without success. Then, a Chandra pointing found the X-ray source single and
 coincident with the bright star. Significantly, the SiIV emissions of Alpha
 Persei, in a deeper FUV spectrum collected by HST COS as part of the joint
 program, aligned well with chromospheric atomic oxygen (which must be intrinsic
 to the luminous star), within the context of cooler late-F and early-G
 supergiants, including Cepheid variables. This pointed to the X-rays as the
 fundamental anomaly. The over-luminous X-rays still support the case for a
 hyperactive dwarf secondary, albeit now spatially unresolved. However, an
 alternative is that Alpha Persei represents a novel class of coronal source.
 Resolving the first possibility now has become more difficult, because the easy
 solution -- a well separated companion -- has been eliminated. Testing the
 other possibility will require a broader high-energy census of the early-F
 supergiants.",0,1,0,0,0,0,
302,The challenge of realistic music generation: modelling raw audio at scale,"Realistic music generation is a challenging task. When building generative
 models of music that are learnt from data, typically high-level representations
 such as scores or MIDI are used that abstract away the idiosyncrasies of a
 particular performance. But these nuances are very important for our perception
 of musicality and realism, so in this work we embark on modelling music in the
 raw audio domain. It has been shown that autoregressive models excel at
 generating raw audio waveforms of speech, but when applied to music, we find
 them biased towards capturing local signal structure at the expense of
 modelling long-range correlations. This is problematic because music exhibits
 structure at many different timescales. In this work, we explore autoregressive
 discrete autoencoders (ADAs) as a means to enable autoregressive models to
 capture long-range correlations in waveforms. We find that they allow us to
 unconditionally generate piano music directly in the raw audio domain, which
 shows stylistic consistency across tens of seconds.",0,0,0,1,0,0,
303,Interpretations of family size distributions: The Datura example,"Young asteroid families are unique sources of information about fragmentation
 physics and the structure of their parent bodies, since their physical
 properties have not changed much since their birth. Families have different
 properties such as age, size, taxonomy, collision severity and others, and
 understanding the effect of those properties on our observations of the
 size-frequency distribution (SFD) of family fragments can give us important
 insights into the hypervelocity collision processes at scales we cannot achieve
 in our laboratories. Here we take as an example the very young Datura family,
 with a small 8-km parent body, and compare its size distribution to other
 families, with both large and small parent bodies, and created by both
 catastrophic and cratering formation events. We conclude that most likely
 explanation for the shallower size distribution compared to larger families is
 a more pronounced observational bias because of its small size. Its size
 distribution is perfectly normal when its parent body size is taken into
 account. We also discuss some other possibilities. In addition, we study
 another common feature: an offset or ""bump"" in the distribution occurring for a
 few of the larger elements. We hypothesize that it can be explained by a newly
 described regime of cratering, ""spall cratering"", which controls the majority
 of impact craters on the surface of small asteroids like Datura.",0,1,0,0,0,0,
304,"Intersections of $??$ classes in $\overline{\mathcal{M}}_{g,n}$","We provide a graph formula which describes an arbitrary monomial in {\omega}
 classes (also referred to as stable {\psi} classes) in terms of a simple family
 of dual graphs (pinwheel graphs) with edges decorated by rational functions in
 {\psi} classes. We deduce some numerical consequences and in particular a
 combinatorial formula expressing top intersections of \k{appa} classes on Mg in
 terms of top intersections of {\psi} classes.",0,0,1,0,0,0,
305,GENFIRE: A generalized Fourier iterative reconstruction algorithm for high-resolution 3D imaging,"Tomography has made a radical impact on diverse fields ranging from the study
 of 3D atomic arrangements in matter to the study of human health in medicine.
 Despite its very diverse applications, the core of tomography remains the same,
 that is, a mathematical method must be implemented to reconstruct the 3D
 structure of an object from a number of 2D projections. In many scientific
 applications, however, the number of projections that can be measured is
 limited due to geometric constraints, tolerable radiation dose and/or
 acquisition speed. Thus it becomes an important problem to obtain the
 best-possible reconstruction from a limited number of projections. Here, we
 present the mathematical implementation of a tomographic algorithm, termed
 GENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between
 real and reciprocal space, GENFIRE searches for a global solution that is
 concurrently consistent with the measured data and general physical
 constraints. The algorithm requires minimal human intervention and also
 incorporates angular refinement to reduce the tilt angle error. We demonstrate
 that GENFIRE can produce superior results relative to several other popular
 tomographic reconstruction techniques by numerical simulations, and by
 experimentally by reconstructing the 3D structure of a porous material and a
 frozen-hydrated marine cyanobacterium. Equipped with a graphical user
 interface, GENFIRE is freely available from our website and is expected to find
 broad applications across different disciplines.",0,1,0,0,0,0,
306,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,"Generative Adversarial Networks (GANs) excel at creating realistic images
 with complex models for which maximum likelihood is infeasible. However, the
 convergence of GAN training has still not been proved. We propose a two
 time-scale update rule (TTUR) for training GANs with stochastic gradient
 descent on arbitrary GAN loss functions. TTUR has an individual learning rate
 for both the discriminator and the generator. Using the theory of stochastic
 approximation, we prove that the TTUR converges under mild assumptions to a
 stationary local Nash equilibrium. The convergence carries over to the popular
 Adam optimization, for which we prove that it follows the dynamics of a heavy
 ball with friction and thus prefers flat minima in the objective landscape. For
 the evaluation of the performance of GANs at image generation, we introduce the
 ""Fr??chet Inception Distance"" (FID) which captures the similarity of generated
 images to real ones better than the Inception Score. In experiments, TTUR
 improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)
 outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN
 Bedrooms, and the One Billion Word Benchmark.",1,0,0,1,0,0,
307,"SPIRou Input Catalog: Activity, Rotation and Magnetic Field of Cool Dwarfs","Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we
 present new measurements of activity and magnetic field proxies of 442 low-mass
 K5-M7 dwarfs. The objects were analysed as potential targets to search for
 planetary-mass companions with the new spectropolarimeter and high-precision
 velocimeter, SPIRou. We have analysed their high-resolution spectra in an
 homogeneous way: circular polarisation, chromospheric features, and Zeeman
 broadening of the FeH infrared line. The complex relationship between these
 activity indicators is analysed: while no strong connection is found between
 the large-scale and small-scale magnetic fields, the latter relates with the
 non-thermal flux originating in the chromosphere.
 We then examine the relationship between various activity diagnostics and the
 optical radial-velocity jitter available in the literature, especially for
 planet host stars. We use this to derive for all stars an activity merit
 function (higher for quieter stars) with the goal of identifying the most
 favorable stars where the radial-velocity jitter is low enough for planet
 searches. We find that the main contributors to the RV jitter are the
 large-scale magnetic field and the chromospheric non-thermal emission.
 In addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed
 along their rotation using the spectropolarimetric mode, and we derive their
 magnetic topology. These very slow rotators are good representatives of future
 SPIRou targets. They are compared to other stars where the magnetic topology is
 also known. The poloidal component of the magnetic field is predominent in all
 three stars.",0,1,0,0,0,0,
308,Objective Procedure for Reconstructing Couplings in Complex Systems,"Inferring directional connectivity from point process data of multiple
 elements is desired in various scientific fields such as neuroscience,
 geography, economics, etc. Here, we propose an inference procedure for this
 goal based on the kinetic Ising model. The procedure is composed of two steps:
 (1) determination of the time-bin size for transforming the point-process data
 to discrete time binary data and (2) screening of relevant couplings from the
 estimated networks. For these, we develop simple methods based on information
 theory and computational statistics. Applications to data from artificial and
 \textit{in vitro} neuronal networks show that the proposed procedure performs
 fairly well when identifying relevant couplings, including the discrimination
 of their signs, with low computational cost. These results highlight the
 potential utility of the kinetic Ising model to analyze real interacting
 systems with event occurrences.",0,0,0,0,1,0,
309,Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines: A Majorization--Minimization Algorithm Approach,"Support vector machines (SVMs) are an important tool in modern data analysis.
 Traditionally, support vector machines have been fitted via quadratic
 programming, either using purpose-built or off-the-shelf algorithms. We present
 an alternative approach to SVM fitting via the majorization--minimization (MM)
 paradigm. Algorithms that are derived via MM algorithm constructions can be
 shown to monotonically decrease their objectives at each iteration, as well as
 be globally convergent to stationary points. We demonstrate the construction of
 iteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,
 for SVM risk minimization problems involving the hinge, least-square,
 squared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net
 penalizations. Successful implementations of our algorithms are presented via
 some numerical examples.",1,0,0,1,0,0,
310,Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search Queries,"Estimating vaccination uptake is an integral part of ensuring public health.
 It was recently shown that vaccination uptake can be estimated automatically
 from web data, instead of slowly collected clinical records or population
 surveys. All prior work in this area assumes that features of vaccination
 uptake collected from the web are temporally regular. We present the first ever
 method to remove this assumption from vaccination uptake estimation: our method
 dynamically adapts to temporal fluctuations in time series web data used to
 estimate vaccination uptake. We show our method to outperform the state of the
 art compared to competitive baselines that use not only web data but also
 curated clinical data. This performance improvement is more pronounced for
 vaccines whose uptake has been irregular due to negative media attention (HPV-1
 and HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of
 12 years old (whose vaccination is more irregular compared to younger
 children).",1,0,0,1,0,0,
311,Over Recurrence for Mixing Transformations,"We show that every invertible strong mixing transformation on a Lebesgue
 space has strictly over-recurrent sets. Also, we give an explicit procedure for
 constructing strong mixing transformations with no under-recurrent sets. This
 answers both parts of a question of V. Bergelson.
 We define $\epsilon$-over-recurrence and show that given $\epsilon > 0$, any
 ergodic measure preserving invertible transformation (including discrete
 spectrum) has $\epsilon$-over-recurrent sets of arbitrarily small measure.
 Discrete spectrum transformations and rotations do not have over-recurrent
 sets, but we construct a weak mixing rigid transformation with strictly
 over-recurrent sets.",0,0,1,0,0,0,
312,Joint Atlas-Mapping of Multiple Histological Series combined with Multimodal MRI of Whole Marmoset Brains,"Development of a mesoscale neural circuitry map of the common marmoset is an
 essential task due to the ideal characteristics of the marmoset as a model
 organism for neuroscience research. To facilitate this development there is a
 need for new computational tools to cross-register multi-modal data sets
 containing MRI volumes as well as multiple histological series, and to register
 the combined data set to a common reference atlas. We present a fully automatic
 pipeline for same-subject-MRI guided reconstruction of image volumes from a
 series of histological sections of different modalities, followed by
 diffeomorphic mapping to a reference atlas. We show registration results for
 Nissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo
 MRI as our reference and show that our method achieves accurate registration
 and eliminates artifactual warping that may be result from the absence of a
 reference MRI data set. Examination of the determinant of the local metric
 tensor of the diffeomorphic mapping between each subject's ex-vivo MRI and
 resultant Nissl reconstruction allows an unprecedented local quantification of
 geometrical distortions resulting from the histological processing, showing a
 slight shrinkage, a median linear scale change of ~-1% in going from the
 ex-vivo MRI to the tape-transfer generated histological image data.",0,0,0,0,1,0,
313,A Practical Approach for Successive Omniscience,"The system that we study in this paper contains a set of users that observe a
 discrete memoryless multiple source and communicate via noise-free channels
 with the aim of attaining omniscience, the state that all users recover the
 entire multiple source. We adopt the concept of successive omniscience (SO),
 i.e., letting the local omniscience in some user subset be attained before the
 global omniscience in the entire system, and consider the problem of how to
 efficiently attain omniscience in a successive manner. Based on the existing
 results on SO, we propose a CompSetSO algorithm for determining a complimentary
 set, a user subset in which the local omniscience can be attained first without
 increasing the sum-rate, the total number of communications, for the global
 omniscience. We also derive a sufficient condition for a user subset to be
 complimentary so that running the CompSetSO algorithm only requires a lower
 bound, instead of the exact value, of the minimum sum-rate for attaining global
 omniscience. The CompSetSO algorithm returns a complimentary user subset in
 polynomial time. We show by example how to recursively apply the CompSetSO
 algorithm so that the global omniscience can be attained by multi-stages of SO.",1,0,0,0,0,0,
314,Scholars on Twitter: who and how many are they?,"In this paper we present a novel methodology for identifying scholars with a
 Twitter account. By combining bibliometric data from Web of Science and Twitter
 users identified by Altmetric.com we have obtained the largest set of
 individual scholars matched with Twitter users made so far. Our methodology
 consists of a combination of matching algorithms, considering different
 linguistic elements of both author names and Twitter names; followed by a
 rule-based scoring system that weights the common occurrence of several
 elements related with the names, individual elements and activities of both
 Twitter users and scholars matched. Our results indicate that about 2% of the
 overall population of scholars in the Web of Science is active on Twitter. By
 domain we find a strong presence of researchers from the Social Sciences and
 the Humanities. Natural Sciences is the domain with the lowest level of
 scholars on Twitter. Researchers on Twitter also tend to be younger than those
 that are not on Twitter. As this is a bibliometric-based approach, it is
 important to highlight the reliance of the method on the number of publications
 produced and tweeted by the scholars, thus the share of scholars on Twitter
 ranges between 1% and 5% depending on their level of productivity. Further
 research is suggested in order to improve and expand the methodology.",1,0,0,0,0,0,
315,General notions of regression depth function,"As a measure for the centrality of a point in a set of multivariate data,
 statistical depth functions play important roles in multivariate analysis,
 because one may conveniently construct descriptive as well as inferential
 procedures relying on them. Many depth notions have been proposed in the
 literature to fit to different applications. However, most of them are mainly
 developed for the location setting. In this paper, we discuss the possibility
 of extending some of them into the regression setting. A general concept of
 regression depth function is also provided.",0,0,0,1,0,0,
316,Photonic topological pumping through the edges of a dynamical four-dimensional quantum Hall system,"When a two-dimensional electron gas is exposed to a perpendicular magnetic
 field and an in-plane electric field, its conductance becomes quantized in the
 transverse in-plane direction: this is known as the quantum Hall (QH) effect.
 This effect is a result of the nontrivial topology of the system's electronic
 band structure, where an integer topological invariant known as the first Chern
 number leads to the quantization of the Hall conductance. Interestingly, it was
 shown that the QH effect can be generalized mathematically to four spatial
 dimensions (4D), but this effect has never been realized for the obvious reason
 that experimental systems are bound to three spatial dimensions. In this work,
 we harness the high tunability and control offered by photonic waveguide arrays
 to experimentally realize a dynamically-generated 4D QH system using a 2D array
 of coupled optical waveguides. The inter-waveguide separation is constructed
 such that the propagation of light along the device samples over
 higher-dimensional momenta in the directions orthogonal to the two physical
 dimensions, thus realizing a 2D topological pump. As a result, the device's
 band structure is associated with 4D topological invariants known as second
 Chern numbers which support a quantized bulk Hall response with a 4D symmetry.
 In a finite-sized system, the 4D topological bulk response is carried by
 localized edges modes that cross the sample as a function of of the modulated
 auxiliary momenta. We directly observe this crossing through photon pumping
 from edge-to-edge and corner-to-corner of our system. These are equivalent to
 the pumping of charge across a 4D system from one 3D hypersurface to the
 opposite one and from one 2D hyperedge to another, and serve as first
 experimental realization of higher-dimensional topological physics.",0,1,0,0,0,0,
317,On Scalable Inference with Stochastic Gradient Descent,"In many applications involving large dataset or online updating, stochastic
 gradient descent (SGD) provides a scalable way to compute parameter estimates
 and has gained increasing popularity due to its numerical convenience and
 memory efficiency. While the asymptotic properties of SGD-based estimators have
 been established decades ago, statistical inference such as interval estimation
 remains much unexplored. The traditional resampling method such as the
 bootstrap is not computationally feasible since it requires to repeatedly draw
 independent samples from the entire dataset. The plug-in method is not
 applicable when there are no explicit formulas for the covariance matrix of the
 estimator. In this paper, we propose a scalable inferential procedure for
 stochastic gradient descent, which, upon the arrival of each observation,
 updates the SGD estimate as well as a large number of randomly perturbed SGD
 estimates. The proposed method is easy to implement in practice. We establish
 its theoretical properties for a general class of models that includes
 generalized linear models and quantile regression models as special cases. The
 finite-sample performance and numerical utility is evaluated by simulation
 studies and two real data applications.",1,0,0,1,0,0,
318,The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes,"In the work of Peng et al. in 2012, a new measure was proposed for fault
 diagnosis of systems: namely, g-good-neighbor conditional diagnosability, which
 requires that any fault-free vertex has at least g fault-free neighbors in the
 system. In this paper, we establish the g-good-neighbor conditional
 diagnosability of locally twisted cubes under the PMC model and the MM^* model.",1,0,1,0,0,0,
319,Coherence for lenses and open games,"Categories of polymorphic lenses in computer science, and of open games in
 compositional game theory, have a curious structure that is reminiscent of
 compact closed categories, but differs in some crucial ways. Specifically they
 have a family of morphisms that behave like the counits of a compact closed
 category, but have no corresponding units; and they have a `partial' duality
 that behaves like transposition in a compact closed category when it is
 defined. We axiomatise this structure, which we refer to as a `teleological
 category'. We precisely define a diagrammatic language suitable for these
 categories, and prove a coherence theorem for them. This underpins the use of
 diagrammatic reasoning in compositional game theory, which has previously been
 used only informally.",1,0,0,0,0,0,
320,Streaming Algorithm for Euler Characteristic Curves of Multidimensional Images,"We present an efficient algorithm to compute Euler characteristic curves of
 gray scale images of arbitrary dimension. In various applications the Euler
 characteristic curve is used as a descriptor of an image.
 Our algorithm is the first streaming algorithm for Euler characteristic
 curves. The usage of streaming removes the necessity to store the entire image
 in RAM. Experiments show that our implementation handles terabyte scale images
 on commodity hardware. Due to lock-free parallelism, it scales well with the
 number of processor cores. Our software---CHUNKYEuler---is available as open
 source on Bitbucket.
 Additionally, we put the concept of the Euler characteristic curve in the
 wider context of computational topology. In particular, we explain the
 connection with persistence diagrams.",1,0,1,0,0,0,
321,An automata group of intermediate growth and exponential activity,"We give a new example of an automata group of intermediate growth. It is
 generated by an automaton with 4 states on an alphabet with 8 letters. This
 automata group has exponential activity and its limit space is not simply
 connected.",0,0,1,0,0,0,
322,Tuning across the BCS-BEC crossover in the multiband superconductor Fe$_{1+y}$Se$_x$Te$_{1-x}$ : An angle-resolved photoemission study,"The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to
 Bose-Einstein condensation (BEC) is difficult to realize in quantum materials
 because, unlike in ultracold atoms, one cannot tune the pairing interaction. We
 realize the BCS-BEC crossover in a nearly compensated semimetal
 Fe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\epsilon_F$, via
 chemical doping, which permits us to systematically change $\Delta /
 \epsilon_F$ from 0.16 to 0.5 were $\Delta$ is the superconducting (SC) gap. We
 use angle-resolved photoemission spectroscopy to measure the Fermi energy, the
 SC gap and characteristic changes in the SC state electronic dispersion as the
 system evolves from a BCS to a BEC regime. Our results raise important
 questions about the crossover in multiband superconductors which go beyond
 those addressed in the context of cold atoms.",0,1,0,0,0,0,
323,GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking,"Model compression is essential for serving large deep neural nets on devices
 with limited resources or applications that require real-time responses. As a
 case study, a state-of-the-art neural language model usually consists of one or
 more recurrent layers sandwiched between an embedding layer used for
 representing input tokens and a softmax layer for generating output tokens. For
 problems with a very large vocabulary size, the embedding and the softmax
 matrices can account for more than half of the model size. For instance, the
 bigLSTM model achieves state-of- the-art performance on the One-Billion-Word
 (OBW) dataset with around 800k vocabulary, and its word embedding and softmax
 matrices use more than 6GBytes space, and are responsible for over 90% of the
 model parameters. In this paper, we propose GroupReduce, a novel compression
 method for neural language models, based on vocabulary-partition (block) based
 low-rank matrix approximation and the inherent frequency distribution of tokens
 (the power-law distribution of words). The experimental results show our method
 can significantly outperform traditional compression methods such as low-rank
 approximation and pruning. On the OBW dataset, our method achieved 6.6 times
 compression rate for the embedding and softmax matrices, and when combined with
 quantization, our method can achieve 26 times compression rate, which
 translates to a factor of 12.8 times compression for the entire model with very
 little degradation in perplexity.",0,0,0,1,0,0,
324,Morphological characterization of Ge ion implanted SiO2 matrix using multifractal technique,"200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy
 were implanted into SiO2 matrix with Different fluences. The implanted samples
 were annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of
 implanted as well as annealed samples were captured by the atomic force
 microscopy (AFM). Two dimension (2D) multifractal detrended fluctuation
 analysis (MFDFA) based on the partition function approach has been used to
 study the surfaces of ion implanted and annealed samples. The partition
 function is used to calculate generalized Hurst exponent with the segment size.
 Moreover, it is seen that the generalized Hurst exponents vary nonlinearly with
 the moment, thereby exhibiting the multifractal nature. The multifractality of
 surface is pronounced after annealing for the surface implanted with fluence
 7.5X1016 ions/cm^2.",0,1,0,0,0,0,
325,Preliminary corrosion studies of IN-RAFM steel with stagnant Lead Lithium at 550 C,"Corrosion of Indian RAFMS (reduced activation ferritic martensitic steel)
 material with liquid metal, Lead Lithium ( Pb-Li) has been studied under static
 condition, maintaining Pb-Li at 550 C for different time durations, 2500, 5000
 and 9000 hours. Corrosion rate was calculated from weight loss measurements.
 Microstructure analysis was carried out using SEM and chemical composition by
 SEM-EDX measurements. Micro Vickers hardness and tensile testing were also
 carried out. Chromium was found leaching from the near surface regions and
 surface hardness was found to decrease in all the three cases. Grain boundaries
 were affected. Some grains got detached from the surface giving rise to pebble
 like structures in the surface micrographs. There was no significant reduction
 in the tensile strength, after exposure to liquid metal. This paper discusses
 the experimental details and the results obtained.",0,1,0,0,0,0,
326,Magnetocapillary self-assemblies: locomotion and micromanipulation along a liquid interface,"This paper presents an overview and discussion of magnetocapillary
 self-assemblies. New results are presented, in particular concerning the
 possible development of future applications. These self-organizing structures
 possess the notable ability to move along an interface when powered by an
 oscillatory, uniform magnetic field. The system is constructed as follows. Soft
 magnetic particles are placed on a liquid interface, and submitted to a
 magnetic induction field. An attractive force due to the curvature of the
 interface around the particles competes with an interaction between magnetic
 dipoles. Ordered structures can spontaneously emerge from these conditions.
 Furthermore, time-dependent magnetic fields can produce a wide range of dynamic
 behaviours, including non-time-reversible deformation sequences that produce
 translational motion at low Reynolds number. In other words, due to a
 spontaneous breaking of time-reversal symmetry, the assembly can turn into a
 surface microswimmer. Trajectories have been shown to be precisely
 controllable. As a consequence, this system offers a way to produce microrobots
 able to perform different tasks. This is illustrated in this paper by the
 capture, transport and release of a floating cargo, and the controlled mixing
 of fluids at low Reynolds number.",0,1,0,0,0,0,
327,On asymptotically minimax nonparametric detection of signal in Gaussian white noise,"For the problem of nonparametric detection of signal in Gaussian white noise
 we point out strong asymptotically minimax tests. The sets of alternatives are
 a ball in Besov space $B^r_{2\infty}$ with ""small"" balls in $L_2$ removed.",0,0,1,1,0,0,
328,Bayesian Metabolic Flux Analysis reveals intracellular flux couplings,"Metabolic flux balance analyses are a standard tool in analysing metabolic
 reaction rates compatible with measurements, steady-state and the metabolic
 reaction network stoichiometry. Flux analysis methods commonly place
 unrealistic assumptions on fluxes due to the convenience of formulating the
 problem as a linear programming model, and most methods ignore the notable
 uncertainty in flux estimates. We introduce a novel paradigm of Bayesian
 metabolic flux analysis that models the reactions of the whole genome-scale
 cellular system in probabilistic terms, and can infer the full flux vector
 distribution of genome-scale metabolic systems based on exchange and
 intracellular (e.g. 13C) flux measurements, steady-state assumptions, and
 target function assumptions. The Bayesian model couples all fluxes jointly
 together in a simple truncated multivariate posterior distribution, which
 reveals informative flux couplings. Our model is a plug-in replacement to
 conventional metabolic balance methods, such as flux balance analysis (FBA).
 Our experiments indicate that we can characterise the genome-scale flux
 covariances, reveal flux couplings, and determine more intracellular unobserved
 fluxes in C. acetobutylicum from 13C data than flux variability analysis. The
 COBRA compatible software is available at github.com/markusheinonen/bamfa",0,0,0,1,0,0,
329,Robust Estimation of Change-Point Location,"We introduce a robust estimator of the location parameter for the
 change-point in the mean based on the Wilcoxon statistic and establish its
 consistency for $L_1$ near epoch dependent processes. It is shown that the
 consistency rate depends on the magnitude of change. A simulation study is
 performed to evaluate finite sample properties of the Wilcoxon-type estimator
 in standard cases, as well as under heavy-tailed distributions and disturbances
 by outliers, and to compare it with a CUSUM-type estimator. It shows that the
 Wilcoxon-type estimator is equivalent to the CUSUM-type estimator in standard
 cases, but outperforms the CUSUM-type estimator in presence of heavy tails or
 outliers in the data.",0,0,1,1,0,0,
330,Growing length scale accompanying the vitrification: A perspective based on non-singular density fluctuations,"In glass forming liquids close to the glass transition point, even a very
 slight increase in the macroscopic density results in a dramatic slowing down
 of the macroscopic relaxation. Concomitantly, the local density itself
 fluctuates in space. Therefore, one can imagine that even very small local
 density variations control the local glassy nature. Based on this perspective,
 a model for describing growing length scale accompanying the vitrification is
 introduced, in which we assume that in a subsystem whose density is above a
 certain threshold value, $\rho_{\rm c}$, owing to steric constraints, particle
 rearrangements are highly suppressed for a sufficiently long time period
 ($\sim$ structural relaxation time). We regard such a subsystem as a glassy
 cluster. Then, based on the statistics of the subsystem-density, we predict
 that with compression (increasing average density $\rho$) at a fixed
 temperature $T$ in supercooled states, the characteristic length of the
 clusters, $\xi$, diverges as $\xi\sim(\rho_{\rm c}-\rho)^{-2/d}$, where $d$ is
 the spatial dimensionality. This $\xi$ measures the average persistence length
 of the steric constraints in blocking the rearrangement motions and is
 determined by the subsystem density. Additionally, with decreasing $T$ at a
 fixed $\rho$, the length scale diverges in the same manner as $\xi\sim(T-T_{\rm
 c})^{-2/d}$, for which $\rho$ is identical to $\rho_{\rm c}$ at $T=T_{\rm c}$.
 The exponent describing the diverging length scale is the same as the one
 predicted by some theoretical models and indeed has been observed in some
 simulations and experiments. However, the basic mechanism for this divergence
 is different; that is, we do not invoke thermodynamic anomalies associated with
 the thermodynamic phase transition as the origin of the growing length scale.
 We further present arguements for the cooperative properties based on the
 clusters.",0,1,0,0,0,0,
331,Many-Objective Pareto Local Search,"We propose a new Pareto Local Search Algorithm for the many-objective
 combinatorial optimization. Pareto Local Search proved to be a very effective
 tool in the case of the bi-objective combinatorial optimization and it was used
 in a number of the state-of-the-art algorithms for problems of this kind. On
 the other hand, the standard Pareto Local Search algorithm becomes very
 inefficient for problems with more than two objectives. We build an effective
 Many-Objective Pareto Local Search algorithm using three new mechanisms: the
 efficient update of large Pareto archives with ND-Tree data structure, a new
 mechanism for the selection of the promising solutions for the neighborhood
 exploration, and a partial exploration of the neighborhoods. We apply the
 proposed algorithm to the instances of two different problems, i.e. the
 traveling salesperson problem and the traveling salesperson problem with
 profits with up to 5 objectives showing high effectiveness of the proposed
 algorithm.",1,0,0,0,0,0,
332,From Natural to Artificial Camouflage: Components and Systems,"We identify the components of bio-inspired artificial camouflage systems
 including actuation, sensing, and distributed computation. After summarizing
 recent results in understanding the physiology and system-level performance of
 a variety of biological systems, we describe computational algorithms that can
 generate similar patterns and have the potential for distributed
 implementation. We find that the existing body of work predominately treats
 component technology in an isolated manner that precludes a material-like
 implementation that is scale-free and robust. We conclude with open research
 challenges towards the realization of integrated camouflage solutions.",1,0,0,0,1,0,
333,Bayesian nonparametric inference for the M/G/1 queueing systems based on the marked departure process,"In the present work we study Bayesian nonparametric inference for the
 continuous-time M/G/1 queueing system. In the focus of the study is the
 unobservable service time distribution. We assume that the only available data
 of the system are the marked departure process of customers with the marks
 being the queue lengths just after departure instants. These marks constitute
 an embedded Markov chain whose distribution may be parametrized by stochastic
 matrices of a special delta form. We develop the theory in order to obtain
 integral mixtures of Markov measures with respect to suitable prior
 distributions. We have found a sufficient statistic with a distribution of a
 so-called S-structure sheding some new light on the inner statistical structure
 of the M/G/1 queue. Moreover, it allows to update suitable prior distributions
 to the posterior. Our inference methods are validated by large sample results
 as posterior consistency and posterior normality.",0,0,1,1,0,0,
334,On some polynomials and series of Bloch-Polya Type,"We will show that $(1-q)(1-q^2)\dots (1-q^m)$ is a polynomial in $q$ with
 coefficients from $\{-1,0,1\}$ iff $m=1,\ 2,\ 3,$ or $5$ and explore some
 interesting consequences of this result. We find explicit formulas for the
 $q$-series coefficients of $(1-q^2)(1-q^3)(1-q^4)(1-q^5)\dots$ and
 $(1-q^3)(1-q^4)(1-q^5)(1-q^6)\dots$. In doing so, we extend certain
 observations made by Sudler in 1964. We also discuss the classification of the
 products $(1-q)(1-q^2)\dots (1-q^m)$ and some related series with respect to
 their absolute largest coefficients.",0,0,1,0,0,0,
335,"Improvement in the UAV position estimation with low-cost GPS, INS and vision-based system: Application to a quadrotor UAV","In this paper, we develop a position estimation system for Unmanned Aerial
 Vehicles formed by hardware and software. It is based on low-cost devices: GPS,
 commercial autopilot sensors and dense optical flow algorithm implemented in an
 onboard microcomputer. Comparative tests were conducted using our approach and
 the conventional one, where only fusion of GPS and inertial sensors are used.
 Experiments were conducted using a quadrotor in two flying modes: hovering and
 trajectory tracking in outdoor environments. Results demonstrate the
 effectiveness of the proposed approach in comparison with the conventional
 approaches presented in the vast majority of commercial drones.",1,0,0,0,0,0,
336,Structured low rank decomposition of multivariate Hankel matrices,"We study the decomposition of a multivariate Hankel matrix H\_$\sigma$ as a
 sum of Hankel matrices of small rank in correlation with the decomposition of
 its symbol $\sigma$ as a sum of polynomial-exponential series. We present a new
 algorithm to compute the low rank decomposition of the Hankel operator and the
 decomposition of its symbol exploiting the properties of the associated
 Artinian Gorenstein quotient algebra A\_$\sigma$. A basis of A\_$\sigma$ is
 computed from the Singular Value Decomposition of a sub-matrix of the Hankel
 matrix H\_$\sigma$. The frequencies and the weights are deduced from the
 generalized eigenvectors of pencils of shifted sub-matrices of H $\sigma$.
 Explicit formula for the weights in terms of the eigenvectors avoid us to solve
 a Vandermonde system. This new method is a multivariate generalization of the
 so-called Pencil method for solving Prony-type decomposition problems. We
 analyse its numerical behaviour in the presence of noisy input moments, and
 describe a rescaling technique which improves the numerical quality of the
 reconstruction for frequencies of high amplitudes. We also present a new Newton
 iteration, which converges locally to the closest multivariate Hankel matrix of
 low rank and show its impact for correcting errors on input moments.",0,0,1,0,0,0,
337,Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework,"Linear time-periodic (LTP) dynamical systems frequently appear in the
 modeling of phenomena related to fluid dynamics, electronic circuits, and
 structural mechanics via linearization centered around known periodic orbits of
 nonlinear models. Such LTP systems can reach orders that make repeated
 simulation or other necessary analysis prohibitive, motivating the need for
 model reduction.
 We develop here an algorithmic framework for constructing reduced models that
 retains the linear time-periodic structure of the original LTP system. Our
 approach generalizes optimal approaches that have been established previously
 for linear time-invariant (LTI) model reduction problems. We employ an
 extension of the usual H2 Hardy space defined for the LTI setting to
 time-periodic systems and within this broader framework develop an a posteriori
 error bound expressible in terms of related LTI systems. Optimization of this
 bound motivates our algorithm. We illustrate the success of our method on two
 numerical examples.",1,0,0,0,0,0,
338,Software metadata: How much is enough?,"Broad efforts are underway to capture metadata about research software and
 retain it across services; notable in this regard is the CodeMeta project. What
 metadata are important to have about (research) software? What metadata are
 useful for searching for codes? What would you like to learn about astronomy
 software? This BoF sought to gather information on metadata most desired by
 researchers and users of astro software and others interested in registering,
 indexing, capturing, and doing research on this software. Information from this
 BoF could conceivably result in changes to the Astrophysics Source Code Library
 (ASCL) or other resources for the benefit of the community or provide input
 into other projects concerned with software metadata.",1,1,0,0,0,0,
339,A Categorical Approach for Recognizing Emotional Effects of Music,"Recently, digital music libraries have been developed and can be plainly
 accessed. Latest research showed that current organization and retrieval of
 music tracks based on album information are inefficient. Moreover, they
 demonstrated that people use emotion tags for music tracks in order to search
 and retrieve them. In this paper, we discuss separability of a set of emotional
 labels, proposed in the categorical emotion expression, using Fisher's
 separation theorem. We determine a set of adjectives to tag music parts: happy,
 sad, relaxing, exciting, epic and thriller. Temporal, frequency and energy
 features have been extracted from the music parts. It could be seen that the
 maximum separability within the extracted features occurs between relaxing and
 epic music parts. Finally, we have trained a classifier using Support Vector
 Machines to automatically recognize and generate emotional labels for a music
 part. Accuracy for recognizing each label has been calculated; where the
 results show that epic music can be recognized more accurately (77.4%),
 comparing to the other types of music.",1,0,0,1,0,0,
340,Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores,"One key requirement for effective supply chain management is the quality of
 its inventory management. Various inventory management methods are typically
 employed for different types of products based on their demand patterns,
 product attributes, and supply network. In this paper, our goal is to develop
 robust demand prediction methods for weather sensitive products at retail
 stores. We employ historical datasets from Walmart, whose customers and markets
 are often exposed to extreme weather events which can have a huge impact on
 sales regarding the affected stores and products. We want to accurately predict
 the sales of 111 potentially weather-sensitive products around the time of
 major weather events at 45 of Walmart retails locations in the U.S.
 Intuitively, we may expect an uptick in the sales of umbrellas before a big
 thunderstorm, but it is difficult for replenishment managers to predict the
 level of inventory needed to avoid being out-of-stock or overstock during and
 after that storm. While they rely on a variety of vendor tools to predict sales
 around extreme weather events, they mostly employ a time-consuming process that
 lacks a systematic measure of effectiveness. We employ all the methods critical
 to any analytics project and start with data exploration. Critical features are
 extracted from the raw historical dataset for demand forecasting accuracy and
 robustness. In particular, we employ Artificial Neural Network for forecasting
 demand for each product sold around the time of major weather events. Finally,
 we evaluate our model to evaluate their accuracy and robustness.",1,0,0,1,0,0,
341,Deformable Generator Network: Unsupervised Disentanglement of Appearance and Geometry,"We propose a deformable generator model to disentangle the appearance and
 geometric information from images into two independent latent vectors. The
 appearance generator produces the appearance information, including color,
 illumination, identity or category, of an image. The geometric generator
 produces displacement of the coordinates of each pixel and performs geometric
 warping, such as stretching and rotation, on the appearance generator to obtain
 the final synthesized image. The proposed model can learn both representations
 from image data in an unsupervised manner. The learned geometric generator can
 be conveniently transferred to the other image datasets to facilitate
 downstream AI tasks.",0,0,0,1,0,0,
342,Gaussian Kernel in Quantum Paradigm,"The Gaussian kernel is a very popular kernel function used in many
 machine-learning algorithms, especially in support vector machines (SVM). For
 nonlinear training instances in machine learning, it often outperforms
 polynomial kernels in model accuracy. We use Gaussian kernel profoundly in
 formulating nonlinear classical SVM. In the recent research, P. Rebentrost
 et.al. discuss a very elegant quantum version of least square support vector
 machine using the quantum version of polynomial kernel, which is exponentially
 faster than the classical counterparts. In this paper, we have demonstrated a
 quantum version of the Gaussian kernel and analyzed its complexity in the
 context of quantum SVM. Our analysis shows that the computational complexity of
 the quantum Gaussian kernel is O(\epsilon^(-1)logN) with N-dimensional
 instances and \epsilon with a Taylor remainder error term |R_m (\epsilon^(-1)
 logN)|.",1,0,0,0,0,0,
343,Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems,"Security, privacy, and fairness have become critical in the era of data
 science and machine learning. More and more we see that achieving universally
 secure, private, and fair systems is practically impossible. We have seen for
 example how generative adversarial networks can be used to learn about the
 expected private training data; how the exploitation of additional data can
 reveal private information in the original one; and how what looks like
 unrelated features can teach us about each other. Confronted with this
 challenge, in this paper we open a new line of research, where the security,
 privacy, and fairness is learned and used in a closed environment. The goal is
 to ensure that a given entity (e.g., the company or the government), trusted to
 infer certain information with our data, is blocked from inferring protected
 information from it. For example, a hospital might be allowed to produce
 diagnosis on the patient (the positive task), without being able to infer the
 gender of the subject (negative task). Similarly, a company can guarantee that
 internally it is not using the provided data for any undesired task, an
 important goal that is not contradicting the virtually impossible challenge of
 blocking everybody from the undesired task. We design a system that learns to
 succeed on the positive task while simultaneously fail at the negative one, and
 illustrate this with challenging cases where the positive task is actually
 harder than the negative one being blocked. Fairness, to the information in the
 negative task, is often automatically obtained as a result of this proposed
 approach. The particular framework and examples open the door to security,
 privacy, and fairness in very important closed scenarios, ranging from private
 data accumulation companies like social networks to law-enforcement and
 hospitals.",1,0,0,1,0,0,
344,Performance of Energy Harvesting Receivers with Power Optimization,"The difficulty of modeling energy consumption in communication systems leads
 to challenges in energy harvesting (EH) systems, in which nodes scavenge energy
 from their environment. An EH receiver must harvest enough energy for
 demodulating and decoding. The energy required depends upon factors, like code
 rate and signal-to-noise ratio, which can be adjusted dynamically. We consider
 a receiver which harvests energy from ambient sources and the transmitter,
 meaning the received signal is used for both EH and information decoding.
 Assuming a generalized function for energy consumption, we maximize the total
 number of information bits decoded, under both average and peak power
 constraints at the transmitter, by carefully optimizing the power used for EH,
 power used for information transmission, fraction of time for EH, and code
 rate. For transmission over a single block, we find there exist problem
 parameters for which either maximizing power for information transmission or
 maximizing power for EH is optimal. In the general case, the optimal solution
 is a tradeoff of the two. For transmission over multiple blocks, we give an
 upper bound on performance and give sufficient and necessary conditions to
 achieve this bound. Finally, we give some numerical results to illustrate our
 results and analysis.",1,0,0,0,0,0,
345,On Convergence Rate of a Continuous-Time Distributed Self-Appraisal Model with Time-Varying Relative Interaction Matrices,"This paper studies a recently proposed continuous-time distributed
 self-appraisal model with time-varying interactions among a network of $n$
 individuals which are characterized by a sequence of time-varying relative
 interaction matrices. The model describes the evolution of the
 social-confidence levels of the individuals via a reflected appraisal mechanism
 in real time. We first show by example that when the relative interaction
 matrices are stochastic (not doubly stochastic), the social-confidence levels
 of the individuals may not converge to a steady state. We then show that when
 the relative interaction matrices are doubly stochastic, the $n$ individuals'
 self-confidence levels will all converge to $1/n$, which indicates a democratic
 state, exponentially fast under appropriate assumptions, and provide an
 explicit expression of the convergence rate.",0,0,1,0,0,0,
346,Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration,"When the brain receives input from multiple sensory systems, it is faced with
 the question of whether it is appropriate to process the inputs in combination,
 as if they originated from the same event, or separately, as if they originated
 from distinct events. Furthermore, it must also have a mechanism through which
 it can keep sensory inputs calibrated to maintain the accuracy of its internal
 representations. We have developed a neural network architecture capable of i)
 approximating optimal multisensory spatial integration, based on Bayesian
 causal inference, and ii) recalibrating the spatial encoding of sensory
 systems. The architecture is based on features of the dorsal processing
 hierarchy, including the spatial tuning properties of unisensory neurons and
 the convergence of different sensory inputs onto multisensory neurons.
 Furthermore, we propose that these unisensory and multisensory neurons play
 dual roles in i) encoding spatial location as separate or integrated estimates
 and ii) accumulating evidence for the independence or relatedness of
 multisensory stimuli. We further propose that top-down feedback connections
 spanning the dorsal pathway play key a role in recalibrating spatial encoding
 at the level of early unisensory cortices. Our proposed architecture provides
 possible explanations for a number of human electrophysiological and
 neuroimaging results and generates testable predictions linking neurophysiology
 with behaviour.",0,0,0,0,1,0,
347,Block CUR: Decomposing Matrices using Groups of Columns,"A common problem in large-scale data analysis is to approximate a matrix
 using a combination of specifically sampled rows and columns, known as CUR
 decomposition. Unfortunately, in many real-world environments, the ability to
 sample specific individual rows or columns of the matrix is limited by either
 system constraints or cost. In this paper, we consider matrix approximation by
 sampling predefined \emph{blocks} of columns (or rows) from the matrix. We
 present an algorithm for sampling useful column blocks and provide novel
 guarantees for the quality of the approximation. This algorithm has application
 in problems as diverse as biometric data analysis to distributed computing. We
 demonstrate the effectiveness of the proposed algorithms for computing the
 Block CUR decomposition of large matrices in a distributed setting with
 multiple nodes in a compute cluster, where such blocks correspond to columns
 (or rows) of the matrix stored on the same node, which can be retrieved with
 much less overhead than retrieving individual columns stored across different
 nodes. In the biometric setting, the rows correspond to different users and
 columns correspond to users' biometric reaction to external stimuli, {\em
 e.g.,}~watching video content, at a particular time instant. There is
 significant cost in acquiring each user's reaction to lengthy content so we
 sample a few important scenes to approximate the biometric response. An
 individual time sample in this use case cannot be queried in isolation due to
 the lack of context that caused that biometric reaction. Instead, collections
 of time segments ({\em i.e.,} blocks) must be presented to the user. The
 practical application of these algorithms is shown via experimental results
 using real-world user biometric data from a content testing environment.",1,0,0,1,0,0,
348,Synchronous Observation on the Spontaneous Transformation of Liquid Metal under Free Falling Microgravity Situation,"The unusually high surface tension of room temperature liquid metal is
 molding it as unique material for diverse newly emerging areas. However, unlike
 its practices on earth, such metal fluid would display very different behaviors
 when working in space where gravity disappears and surface property dominates
 the major physics. So far, few direct evidences are available to understand
 such effect which would impede further exploration of liquid metal use for
 space. Here to preliminarily probe into this intriguing issue, a low cost
 experimental strategy to simulate microgravity environment on earth was
 proposed through adopting bridges with high enough free falling distance as the
 test platform. Then using digital cameras amounted along x, y, z directions on
 outside wall of the transparent container with liquid metal and allied solution
 inside, synchronous observations on the transient flow and transformational
 activities of liquid metal were performed. Meanwhile, an unmanned aerial
 vehicle was adopted to record the whole free falling dynamics of the test
 capsule from the far end which can help justify subsequent experimental
 procedures. A series of typical fundamental phenomena were thus observed as:
 (a) A relatively large liquid metal object would spontaneously transform from
 its original planar pool state into a sphere and float in the container if
 initiating the free falling; (b) The liquid metal changes its three-dimensional
 shape due to dynamic microgravity strength due to free falling and rebound of
 the test capsule; and (c) A quick spatial transformation of liquid metal
 immersed in the solution can easily be induced via external electrical fields.
 The mechanisms of the surface tension driven liquid metal actuation in space
 were interpreted. All these findings indicated that microgravity effect should
 be fully treated in developing future generation liquid metal space
 technologies.",0,1,0,0,0,0,
349,Continuously tempered Hamiltonian Monte Carlo,"Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)
 method for performing approximate inference in complex probabilistic models of
 continuous variables. In common with many MCMC methods, however, the standard
 HMC approach performs poorly in distributions with multiple isolated modes. We
 present a method for augmenting the Hamiltonian system with an extra continuous
 temperature control variable which allows the dynamic to bridge between
 sampling a complex target distribution and a simpler unimodal base
 distribution. This augmentation both helps improve mixing in multimodal targets
 and allows the normalisation constant of the target distribution to be
 estimated. The method is simple to implement within existing HMC code,
 requiring only a standard leapfrog integrator. We demonstrate experimentally
 that the method is competitive with annealed importance sampling and simulating
 tempering methods at sampling from challenging multimodal distributions and
 estimating their normalising constants.",0,0,0,1,0,0,
350,Automated Synthesis of Safe Digital Controllers for Sampled-Data Stochastic Nonlinear Systems,"We present a new method for the automated synthesis of digital controllers
 with formal safety guarantees for systems with nonlinear dynamics, noisy output
 measurements, and stochastic disturbances. Our method derives digital
 controllers such that the corresponding closed-loop system, modeled as a
 sampled-data stochastic control system, satisfies a safety specification with
 probability above a given threshold. The proposed synthesis method alternates
 between two steps: generation of a candidate controller pc, and verification of
 the candidate. pc is found by maximizing a Monte Carlo estimate of the safety
 probability, and by using a non-validated ODE solver for simulating the system.
 Such a candidate is therefore sub-optimal but can be generated very rapidly. To
 rule out unstable candidate controllers, we prove and utilize Lyapunov's
 indirect method for instability of sampled-data nonlinear systems. In the
 subsequent verification step, we use a validated solver based on SMT
 (Satisfiability Modulo Theories) to compute a numerically and statistically
 valid confidence interval for the safety probability of pc. If the probability
 so obtained is not above the threshold, we expand the search space for
 candidates by increasing the controller degree. We evaluate our technique on
 three case studies: an artificial pancreas model, a powertrain control model,
 and a quadruple-tank process.",1,0,0,0,0,0,
351,Magnus integrators on multicore CPUs and GPUs,"In the present paper we consider numerical methods to solve the discrete
 Schr??dinger equation with a time dependent Hamiltonian (motivated by problems
 encountered in the study of spin systems). We will consider both short-range
 interactions, which lead to evolution equations involving sparse matrices, and
 long-range interactions, which lead to dense matrices. Both of these settings
 show very different computational characteristics. We use Magnus integrators
 for time integration and employ a framework based on Leja interpolation to
 compute the resulting action of the matrix exponential. We consider both
 traditional Magnus integrators (which are extensively used for these types of
 problems in the literature) as well as the recently developed commutator-free
 Magnus integrators and implement them on modern CPU and GPU (graphics
 processing unit) based systems.
 We find that GPUs can yield a significant speed-up (up to a factor of $10$ in
 the dense case) for these types of problems. In the sparse case GPUs are only
 advantageous for large problem sizes and the achieved speed-ups are more
 modest. In most cases the commutator-free variant is superior but especially on
 the GPU this advantage is rather small. In fact, none of the advantage of
 commutator-free methods on GPUs (and on multi-core CPUs) is due to the
 elimination of commutators. This has important consequences for the design of
 more efficient numerical methods.",1,1,0,0,0,0,
352,High Dimensional Estimation and Multi-Factor Models,"This paper re-investigates the estimation of multiple factor models relaxing
 the convention that the number of factors is small and using a new approach for
 identifying factors. We first obtain the collection of all possible factors and
 then provide a simultaneous test, security by security, of which factors are
 significant. Since the collection of risk factors is large and highly
 correlated, high-dimension methods (including the LASSO and prototype
 clustering) have to be used. The multi-factor model is shown to have a
 significantly better fit than the Fama-French 5-factor model. Robustness tests
 are also provided.",0,0,0,1,0,1,
353,Scaling Law for Three-body Collisions in Identical Fermions with $p$-wave Interactions,"We experimentally confirmed the threshold behavior and scattering length
 scaling law of the three-body loss coefficients in an ultracold spin-polarized
 gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the
 three-body loss coefficients as functions of temperature and scattering volume,
 and found that the threshold law and the scattering length scaling law hold in
 limited temperature and magnetic field regions. We also found that the
 breakdown of the scaling laws is due to the emergence of the effective-range
 term. This work is an important first step toward full understanding of the
 loss of identical fermions with $p$-wave interactions.",0,1,0,0,0,0,
354,An Expanded Local Variance Gamma model,"The paper proposes an expanded version of the Local Variance Gamma model of
 Carr and Nadtochiy by adding drift to the governing underlying process. Still
 in this new model it is possible to derive an ordinary differential equation
 for the option price which plays a role of Dupire's equation for the standard
 local volatility model. It is shown how calibration of multiple smiles (the
 whole local volatility surface) can be done in such a case. Further, assuming
 the local variance to be a piecewise linear function of strike and piecewise
 constant function of time this ODE is solved in closed form in terms of
 Confluent hypergeometric functions. Calibration of the model to market smiles
 does not require solving any optimization problem and, in contrast, can be done
 term-by-term by solving a system of non-linear algebraic equations for each
 maturity, which is fast.",0,0,0,0,0,1,
355,An attentive neural architecture for joint segmentation and parsing and its application to real estate ads,"In processing human produced text using natural language processing (NLP)
 techniques, two fundamental subtasks that arise are (i) segmentation of the
 plain text into meaningful subunits (e.g., entities), and (ii) dependency
 parsing, to establish relations between subunits. In this paper, we develop a
 relatively simple and effective neural joint model that performs both
 segmentation and dependency parsing together, instead of one after the other as
 in most state-of-the-art works. We will focus in particular on the real estate
 ad setting, aiming to convert an ad to a structured description, which we name
 property tree, comprising the tasks of (1) identifying important entities of a
 property (e.g., rooms) from classifieds and (2) structuring them into a tree
 format. In this work, we propose a new joint model that is able to tackle the
 two tasks simultaneously and construct the property tree by (i) avoiding the
 error propagation that would arise from the subtasks one after the other in a
 pipelined fashion, and (ii) exploiting the interactions between the subtasks.
 For this purpose, we perform an extensive comparative study of the pipeline
 methods and the new proposed joint model, reporting an improvement of over
 three percentage points in the overall edge F1 score of the property tree.
 Also, we propose attention methods, to encourage our model to focus on salient
 tokens during the construction of the property tree. Thus we experimentally
 demonstrate the usefulness of attentive neural architectures for the proposed
 joint model, showcasing a further improvement of two percentage points in edge
 F1 score for our application.",1,0,0,0,0,0,
356,Multilevel maximum likelihood estimation with application to covariance matrices,"The asymptotic variance of the maximum likelihood estimate is proved to
 decrease when the maximization is restricted to a subspace that contains the
 true parameter value. Maximum likelihood estimation allows a systematic fitting
 of covariance models to the sample, which is important in data assimilation.
 The hierarchical maximum likelihood approach is applied to the spectral
 diagonal covariance model with different parameterizations of eigenvalue decay,
 and to the sparse inverse covariance model with specified parameter values on
 different sets of nonzero entries. It is shown computationally that using
 smaller sets of parameters can decrease the sampling noise in high dimension
 substantially.",0,0,1,1,0,0,
357,Auto-Meta: Automated Gradient Based Meta Learner Search,"Fully automating machine learning pipelines is one of the key challenges of
 current artificial intelligence research, since practical machine learning
 often requires costly and time-consuming human-powered processes such as model
 design, algorithm development, and hyperparameter tuning. In this paper, we
 verify that automated architecture search synergizes with the effect of
 gradient-based meta learning. We adopt the progressive neural architecture
 search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal
 architectures for meta-learners. The gradient based meta-learner whose
 architecture was automatically found achieved state-of-the-art results on the
 5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy,
 which is $11.54\%$ improvement over the result obtained by the first
 gradient-based meta-learner called MAML
 \cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is
 the first successful neural architecture search implementation in the context
 of meta learning.",0,0,0,1,0,0,
358,Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry,"We provide a comprehensive study of the convergence of forward-backward
 algorithm under suitable geometric conditions leading to fast rates. We present
 several new results and collect in a unified view a variety of results
 scattered in the literature, often providing simplified proofs. Novel
 contributions include the analysis of infinite dimensional convex minimization
 problems, allowing the case where minimizers might not exist. Further, we
 analyze the relation between different geometric conditions, and discuss novel
 connections with a priori conditions in linear inverse problems, including
 source conditions, restricted isometry properties and partial smoothness.",0,0,1,1,0,0,
359,Calibration-Free Relaxation-Based Multi-Color Magnetic Particle Imaging,"Magnetic Particle Imaging (MPI) is a novel imaging modality with important
 applications such as angiography, stem cell tracking, and cancer imaging.
 Recently, there have been efforts to increase the functionality of MPI via
 multi-color imaging methods that can distinguish the responses of different
 nanoparticles, or nanoparticles in different environmental conditions. The
 proposed techniques typically rely on extensive calibrations that capture the
 differences in the harmonic responses of the nanoparticles. In this work, we
 propose a method to directly estimate the relaxation time constant of the
 nanoparticles from the MPI signal, which is then used to generate a multi-color
 relaxation map. The technique is based on the underlying mirror symmetry of the
 adiabatic MPI signal when the same region is scanned back and forth. We
 validate the proposed method via extensive simulations, and via experiments on
 our in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our
 in-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be
 successfully distinguished with the proposed technique, without any calibration
 or prior knowledge about the nanoparticles.",0,1,0,0,0,0,
360,Neural Machine Translation,"Draft of textbook chapter on neural machine translation. a comprehensive
 treatment of the topic, ranging from introduction to neural networks,
 computation graphs, description of the currently dominant attentional
 sequence-to-sequence model, recent refinements, alternative architectures and
 challenges. Written as chapter for the textbook Statistical Machine
 Translation. Used in the JHU Fall 2017 class on machine translation.",1,0,0,0,0,0,
361,On algebraically integrable domains in Euclidean spaces,"Let $D$ be a bounded domain $D$ in $\mathbb R^n $ with infinitely smooth
 boundary and $n$ is odd. We prove that if the volume cut off from the domain by
 a hyperplane is an algebraic function of the hyperplane, free of real singular
 points, then the domain is an ellipsoid. This partially answers a question of
 V.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically
 integrable domains?",0,0,1,0,0,0,
362,Vocabulary-informed Extreme Value Learning,"The novel unseen classes can be formulated as the extreme values of known
 classes. This inspired the recent works on open-set recognition
 \cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no
 way of naming the novel unseen classes. To solve this problem, we propose the
 Extreme Value Learning (EVL) formulation to learn the mapping from visual
 feature to semantic space. To model the margin and coverage distributions of
 each class, the Vocabulary-informed Learning (ViL) is adopted by using vast
 open vocabulary in the semantic space. Essentially, by incorporating the EVL
 and ViL, we for the first time propose a novel semantic embedding paradigm --
 Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual
 features into semantic space in a probabilistic way. The learned embedding can
 be directly used to solve supervised learning, zero-shot and open set
 recognition simultaneously. Experiments on two benchmark datasets demonstrate
 the effectiveness of proposed frameworks.",1,0,1,1,0,0,
363,Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks,"In this paper, we study the performance of two cross-layer optimized dynamic
 routing techniques for radio interference mitigation across multiple coexisting
 wireless body area networks (BANs), based on real-life measurements. At the
 network layer, the best route is selected according to channel state
 information from the physical layer, associated with low duty cycle TDMA at the
 MAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel
 cooperative multi-path routing (CMR) incorporating 3-branch selection
 combining) perform real-time and reliable data transfer across BANs operating
 near the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'
 mixed-activities is used for analyzing the proposed cross-layer optimization.
 We show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and
 even 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage
 probability at a realistic signal-to-interference-plus-noise ratio (SINR).
 Acceptable packet delivery ratios (PDR) and spectral efficiencies are obtained
 from SPR and CMR with reasonably sensitive receivers across a range of TDMA low
 duty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The
 distribution fits for received SINR through routing are also derived and
 validated with theoretical analysis.",1,0,0,0,0,0,
364,A Team-Formation Algorithm for Faultline Minimization,"In recent years, the proliferation of online resumes and the need to evaluate
 large populations of candidates for on-site and virtual teams have led to a
 growing interest in automated team-formation. Given a large pool of candidates,
 the general problem requires the selection of a team of experts to complete a
 given task. Surprisingly, while ongoing research has studied numerous
 variations with different constraints, it has overlooked a factor with a
 well-documented impact on team cohesion and performance: team faultlines.
 Addressing this gap is challenging, as the available measures for faultlines in
 existing teams cannot be efficiently applied to faultline optimization. In this
 work, we meet this challenge with a new measure that can be efficiently used
 for both faultline measurement and minimization. We then use the measure to
 solve the problem of automatically partitioning a large population into
 low-faultline teams. By introducing faultlines to the team-formation
 literature, our work creates exciting opportunities for algorithmic work on
 faultline optimization, as well as on work that combines and studies the
 connection of faultlines with other influential team characteristics.",1,0,0,0,0,0,
365,A Survey of Model Compression and Acceleration for Deep Neural Networks,"Deep convolutional neural networks (CNNs) have recently achieved great
 success in many visual recognition tasks. However, existing deep neural network
 models are computationally expensive and memory intensive, hindering their
 deployment in devices with low memory resources or in applications with strict
 latency requirements. Therefore, a natural thought is to perform model
 compression and acceleration in deep networks without significantly decreasing
 the model performance. During the past few years, tremendous progress has been
 made in this area. In this paper, we survey the recent advanced techniques for
 compacting and accelerating CNNs model developed. These techniques are roughly
 categorized into four schemes: parameter pruning and sharing, low-rank
 factorization, transferred/compact convolutional filters, and knowledge
 distillation. Methods of parameter pruning and sharing will be described at the
 beginning, after that the other techniques will be introduced. For each scheme,
 we provide insightful analysis regarding the performance, related applications,
 advantages, and drawbacks etc. Then we will go through a few very recent
 additional successful methods, for example, dynamic capacity networks and
 stochastic depths networks. After that, we survey the evaluation matrix, the
 main datasets used for evaluating the model performance and recent benchmarking
 efforts. Finally, we conclude this paper, discuss remaining challenges and
 possible directions on this topic.",1,0,0,0,0,0,
366,Non-Parametric Calibration of Probabilistic Regression,"The task of calibration is to retrospectively adjust the outputs from a
 machine learning model to provide better probability estimates on the target
 variable. While calibration has been investigated thoroughly in classification,
 it has not yet been well-established for regression tasks. This paper considers
 the problem of calibrating a probabilistic regression model to improve the
 estimated probability densities over the real-valued targets. We propose to
 calibrate a regression model through the cumulative probability density, which
 can be derived from calibrating a multi-class classifier. We provide three
 non-parametric approaches to solve the problem, two of which provide empirical
 estimates and the third providing smooth density estimates. The proposed
 approaches are experimentally evaluated to show their ability to improve the
 performance of regression models on the predictive likelihood.",0,0,0,1,0,0,
367,Cyclotron resonant scattering feature simulations. II. Description of the CRSF simulation process,"Cyclotron resonant scattering features (CRSFs) are formed by scattering of
 X-ray photons off quantized plasma electrons in the strong magnetic field (of
 the order 10^12 G) close to the surface of an accreting X-ray pulsar. The line
 profiles of CRSFs cannot be described by an analytic expression. Numerical
 methods such as Monte Carlo (MC) simulations of the scattering processes are
 required in order to predict precise line shapes for a given physical setup,
 which can be compared to observations to gain information about the underlying
 physics in these systems.
 A versatile simulation code is needed for the generation of synthetic
 cyclotron lines. Sophisticated geometries should be investigatable by making
 their simulation possible for the first time.
 The simulation utilizes the mean free path tables described in the first
 paper of this series for the fast interpolation of propagation lengths. The
 code is parallelized to make the very time consuming simulations possible on
 convenient time scales. Furthermore, it can generate responses to
 mono-energetic photon injections, producing Green's functions, which can be
 used later to generate spectra for arbitrary continua.
 We develop a new simulation code to generate synthetic cyclotron lines for
 complex scenarios, allowing for unprecedented physical interpretation of the
 observed data. An associated XSPEC model implementation is used to fit
 synthetic line profiles to NuSTAR data of Cep X-4. The code has been developed
 with the main goal of overcoming previous geometrical constraints in MC
 simulations of CRSFs. By applying this code also to more simple, classic
 geometries used in previous works, we furthermore address issues of code
 verification and cross-comparison of various models. The XSPEC model and the
 Green's function tables are available online at
 this http URL .",0,1,0,0,0,0,
368,New quantum mds constacylŽñc codes,"This paper is devoted to the study of the construction of new quantum MDS
 codes. Based on constacyclic codes over Fq2 , we derive four new families of
 quantum MDS codes, one of which is an explicit generalization of the
 construction given in Theorem 7 in [22]. We also extend the result of Theorem
 3:3 given in [17].",1,0,0,0,0,0,
369,Infinitary first-order categorical logic,"We present a unified categorical treatment of completeness theorems for
 several classical and intuitionistic infinitary logics with a proposed
 axiomatization. This provides new completeness theorems and subsumes previous
 ones by G??del, Kripke, Beth, Karp, Joyal, Makkai and Fourman/Grayson. As an
 application we prove, using large cardinals assumptions, the disjunction and
 existence properties for infinitary intuitionistic first-order logics.",0,0,1,0,0,0,
370,Stochastic Gradient Monomial Gamma Sampler,"Recent advances in stochastic gradient techniques have made it possible to
 estimate posterior distributions from large datasets via Markov Chain Monte
 Carlo (MCMC). However, when the target posterior is multimodal, mixing
 performance is often poor. This results in inadequate exploration of the
 posterior distribution. A framework is proposed to improve the sampling
 efficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A
 generalized kinetic function is leveraged, delivering superior stationary
 mixing, especially for multimodal distributions. Techniques are also discussed
 to overcome the practical issues introduced by this generalization. It is shown
 that the proposed approach is better at exploring complex multimodal posterior
 distributions, as demonstrated on multiple applications and in comparison with
 other stochastic gradient MCMC methods.",1,0,0,1,0,0,
371,Gini estimation under infinite variance,"We study the problems related to the estimation of the Gini index in presence
 of a fat-tailed data generating process, i.e. one in the stable distribution
 class with finite mean but infinite variance (i.e. with tail index
 $\alpha\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be
 reliably estimated using conventional nonparametric methods, because of a
 downward bias that emerges under fat tails. This has important implications for
 the ongoing discussion about economic inequality.
 We start by discussing how the nonparametric estimator of the Gini index
 undergoes a phase transition in the symmetry structure of its asymptotic
 distribution, as the data distribution shifts from the domain of attraction of
 a light-tailed distribution to that of a fat-tailed one, especially in the case
 of infinite variance. We also show how the nonparametric Gini bias increases
 with lower values of $\alpha$. We then prove that maximum likelihood estimation
 outperforms nonparametric methods, requiring a much smaller sample size to
 reach efficiency.
 Finally, for fat-tailed data, we provide a simple correction mechanism to the
 small sample bias of the nonparametric estimator based on the distance between
 the mode and the mean of its asymptotic distribution.",0,0,0,1,0,0,
372,Training Neural Networks Using Features Replay,"Training a neural network using backpropagation algorithm requires passing
 error gradients sequentially through the network. The backward locking prevents
 us from updating network layers in parallel and fully leveraging the computing
 resources. Recently, there are several works trying to decouple and parallelize
 the backpropagation algorithm. However, all of them suffer from severe accuracy
 loss or memory explosion when the neural network is deep. To address these
 challenging issues, we propose a novel parallel-objective formulation for the
 objective function of the neural network. After that, we introduce features
 replay algorithm and prove that it is guaranteed to converge to critical points
 for the non-convex problem under certain conditions. Finally, we apply our
 method to training deep convolutional neural networks, and the experimental
 results show that the proposed method achieves {faster} convergence, {lower}
 memory consumption, and {better} generalization error than compared methods.",0,0,0,1,0,0,
373,The anti-spherical category,"We study a diagrammatic categorification (the ""anti-spherical category"") of
 the anti-spherical module for any Coxeter group. We deduce that Deodhar's
 (sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients,
 and that a monotonicity conjecture of Brenti's holds. The main technical
 observation is a localisation procedure for the anti-spherical category, from
 which we construct a ""light leaves"" basis of morphisms. Our techniques may be
 used to calculate many new elements of the $p$-canonical basis in the
 anti-spherical module. The results use generators and relations for Soergel
 bimodules (""Soergel calculus"") in a crucial way.",0,0,1,0,0,0,
374,Trajectories and orbital angular momentum of necklace beams in nonlinear colloidal suspensions,"Recently, we have predicted that the modulation instability of optical vortex
 solitons propagating in nonlinear colloidal suspensions with exponential
 saturable nonlinearity leads to formation of necklace beams (NBs)
 [S.~Z.~Silahli, W.~Walasik and N.~M.~Litchinitser, Opt.~Lett., \textbf{40},
 5714 (2015)]. Here, we investigate the dynamics of NB formation and
 propagation, and show that the distance at which the NB is formed depends on
 the input power of the vortex beam. Moreover, we show that the NB trajectories
 are not necessarily tangent to the initial vortex ring, and that their
 velocities have components stemming both from the beam diffraction and from the
 beam orbital angular momentum. We also demonstrate the generation of twisted
 solitons and analyze the influence of losses on their propagation. Finally, we
 investigate the conservation of the orbital angular momentum in necklace and
 twisted beams. Our studies, performed in ideal lossless media and in realistic
 colloidal suspensions with losses, provide a detailed description of NB
 dynamics and may be useful in studies of light propagation in highly scattering
 colloids and biological samples.",0,1,0,0,0,0,
375,Unified Treatment of Spin Torques using a Coupled Magnetisation Dynamics and Three-Dimensional Spin Current Solver,"A three-dimensional spin current solver based on a generalised spin
 drift-diffusion description, including the spin Hall effect, is integrated with
 a magnetisation dynamics solver. The resulting model is shown to simultaneously
 reproduce the spin-orbit torques generated using the spin Hall effect, spin
 pumping torques generated by magnetisation dynamics in multilayers, as well as
 the spin transfer torques acting on magnetisation regions with spatial
 gradients, whilst field-like and spin-like torques are reproduced in a spin
 valve geometry. Two approaches to modelling interfaces are analysed, one based
 on the spin mixing conductance and the other based on continuity of spin
 currents where the spin dephasing length governs the absorption of transverse
 spin components. In both cases analytical formulas are derived for the
 spin-orbit torques in a heavy metal / ferromagnet bilayer geometry, showing in
 general both field-like and damping-like torques are generated. The limitations
 of the analytical approach are discussed, showing that even in a simple bilayer
 geometry, due to the non-uniformity of the spin currents, a full
 three-dimensional treatment is required. Finally the model is applied to the
 quantitative analysis of the spin Hall angle in Pt by reproducing published
 experimental data on the ferromagnetic resonance linewidth in the bilayer
 geometry.",0,1,0,0,0,0,
376,A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization,"This paper aims to explore models based on the extreme gradient boosting
 (XGBoost) approach for business risk classification. Feature selection (FS)
 algorithms and hyper-parameter optimizations are simultaneously considered
 during model training. The five most commonly used FS methods including weight
 by Gini, weight by Chi-square, hierarchical variable clustering, weight by
 correlation, and weight by information are applied to alleviate the effect of
 redundant features. Two hyper-parameter optimization approaches, random search
 (RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
 XGBoost. The effect of different FS and hyper-parameter optimization methods on
 the model performance are investigated by the Wilcoxon Signed Rank Test. The
 performance of XGBoost is compared to the traditionally utilized logistic
 regression (LR) model in terms of classification accuracy, area under the curve
 (AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
 show that hierarchical clustering is the optimal FS method for LR while weight
 by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
 optimization in XGBoost outperform LR significantly. TPE optimization shows a
 superiority over RS since it results in a significantly higher accuracy and a
 marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
 tuning shows a lower variability than the RS method. Finally, the ranking of
 feature importance based on XGBoost enhances the model interpretation.
 Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
 operative while powerful approach for business risk modeling.",1,0,0,1,0,0,
377,Rheology of High-Capillary Number Flow in Porous Media,"Immiscible fluids flowing at high capillary numbers in porous media may be
 characterized by an effective viscosity. We demonstrate that the effective
 viscosity is well described by the Lichtenecker-Rother equation. The exponent
 $\alpha$ in this equation takes either the value 1 or 0.6 in two- and 0.5 in
 three-dimensional systems depending on the pore geometry. Our arguments are
 based on analytical and numerical methods.",0,1,0,0,0,0,
378,Quantum Charge Pumps with Topological Phases in Creutz Ladder,"Quantum charge pumping phenomenon connects band topology through the dynamics
 of a one-dimensional quantum system. In terms of a microscopic model, the
 Su-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful
 starting point for many considerations of topological physics. Here we present
 a generalized Creutz scheme as a distinct two-band quantum pump model. By
 noting that it undergoes two kinds of topological band transitions accompanying
 with a Zak-phase-difference of $\pi$ and $2\pi$, respectively, various charge
 pumping schemes are studied by applying an elaborate Peierl's phase
 substitution. Translating into real space, the transportation of quantized
 charges is a result of cooperative quantum interference effect. In particular,
 an all-flux quantum pump emerges which operates with time-varying fluxes only
 and transports two charge units. This puts cold atoms with artificial gauge
 fields as an unique system where this kind of phenomena can be realized.",0,1,0,0,0,0,
379,On Deep Neural Networks for Detecting Heart Disease,"Heart disease is the leading cause of death, and experts estimate that
 approximately half of all heart attacks and strokes occur in people who have
 not been flagged as ""at risk."" Thus, there is an urgent need to improve the
 accuracy of heart disease diagnosis. To this end, we investigate the potential
 of using data analysis, and in particular the design and use of deep neural
 networks (DNNs) for detecting heart disease based on routine clinical data. Our
 main contribution is the design, evaluation, and optimization of DNN
 architectures of increasing depth for heart disease diagnosis. This work led to
 the discovery of a novel five layer DNN architecture - named Heart Evaluation
 for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
 best prediction accuracy. HEARO-5's design employs regularization optimization
 and automatically deals with missing data and/or data outliers. To evaluate and
 tune the architectures we use k-way cross-validation as well as Matthews
 correlation coefficient (MCC) to measure the quality of our classifications.
 The study is performed on the publicly available Cleveland dataset of medical
 information, and we are making our developments open source, to further
 facilitate openness and research on the use of DNNs in medicine. The HEARO-5
 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
 currently published research in the area.",1,0,0,1,0,0,
380,Exponential Stability Analysis via Integral Quadratic Constraints,"The theory of integral quadratic constraints (IQCs) allows verification of
 stability and gain-bound properties of systems containing nonlinear or
 uncertain elements. Gain bounds often imply exponential stability, but it can
 be challenging to compute useful numerical bounds on the exponential decay
 rate. This work presents a generalization of the classical IQC results of
 Megretski and Rantzer that leads to a tractable computational procedure for
 finding exponential rate certificates that are far less conservative than ones
 computed from $L_2$ gain bounds alone. An expanded library of IQCs for
 certifying exponential stability is also provided and the effectiveness of the
 technique is demonstrated via numerical examples.",1,0,1,0,0,0,
381,Fermi acceleration of electrons inside foreshock transient cores,"Foreshock transients upstream of Earth's bow shock have been recently
 observed to accelerate electrons to many times their thermal energy. How such
 acceleration occurs is unknown, however. Using THEMIS case studies, we examine
 a subset of acceleration events (31 of 247 events) in foreshock transients with
 cores that exhibit gradual electron energy increases accompanied by low
 background magnetic field strength and large-amplitude magnetic fluctuations.
 Using the evolution of electron distributions and the energy increase rates at
 multiple spacecraft, we suggest that Fermi acceleration between a converging
 foreshock transient's compressional boundary and the bow shock is responsible
 for the observed electron acceleration. We then show that a one-dimensional
 test particle simulation of an ideal Fermi acceleration model in fluctuating
 fields prescribed by the observations can reproduce the observed evolution of
 electron distributions, energy increase rate, and pitch-angle isotropy,
 providing further support for our hypothesis. Thus, Fermi acceleration is
 likely the principal electron acceleration mechanism in at least this subset of
 foreshock transient cores.",0,1,0,0,0,0,
382,Quantum Speed Limit is Not Quantum,"The quantum speed limit (QSL), or the energy-time uncertainty relation,
 describes the fundamental maximum rate for quantum time evolution and has been
 regarded as being unique in quantum mechanics. In this study, we obtain a
 classical speed limit corresponding to the QSL using the Hilbert space for the
 classical Liouville equation. Thus, classical mechanics has a fundamental speed
 limit, and QSL is not a purely quantum phenomenon but a universal dynamical
 property of the Hilbert space. Furthermore, we obtain similar speed limits for
 the imaginary-time Schroedinger equations such as the master equation.",0,1,0,0,0,0,
383,Adaptive Diffusion Processes of Time-Varying Local Information on Networks,"This paper mainly discusses the diffusion on complex networks with
 time-varying couplings. We propose a model to describe the adaptive diffusion
 process of local topological and dynamical information, and find that the
 Barabasi-Albert scale-free network (BA network) is beneficial to the diffusion
 and leads nodes to arrive at a larger state value than other networks do. The
 ability of diffusion for a node is related to its own degree. Specifically,
 nodes with smaller degrees are more likely to change their states and reach
 larger values, while those with larger degrees tend to stick to their original
 states. We introduce state entropy to analyze the thermodynamic mechanism of
 the diffusion process, and interestingly find that this kind of diffusion
 process is a minimization process of state entropy. We use the inequality
 constrained optimization method to reveal the restriction function of the
 minimization and find that it has the same form as the Gibbs free energy. The
 thermodynamical concept allows us to understand dynamical processes on complex
 networks from a brand-new perspective. The result provides a convenient means
 of optimizing relevant dynamical processes on practical circuits as well as
 related complex systems.",1,0,0,0,0,0,
384,ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder,"This paper proposes a non-parallel many-to-many voice conversion (VC) method
 using a variant of the conditional variational autoencoder (VAE) called an
 auxiliary classifier VAE (ACVAE). The proposed method has three key features.
 First, it adopts fully convolutional architectures to construct the encoder and
 decoder networks so that the networks can learn conversion rules that capture
 time dependencies in the acoustic feature sequences of source and target
 speech. Second, it uses an information-theoretic regularization for the model
 training to ensure that the information in the attribute class label will not
 be lost in the conversion process. With regular CVAEs, the encoder and decoder
 are free to ignore the attribute class label input. This can be problematic
 since in such a situation, the attribute class label will have little effect on
 controlling the voice characteristics of input speech at test time. Such
 situations can be avoided by introducing an auxiliary classifier and training
 the encoder and decoder so that the attribute classes of the decoder outputs
 are correctly predicted by the classifier. Third, it avoids producing
 buzzy-sounding speech at test time by simply transplanting the spectral details
 of the input speech into its converted version. Subjective evaluation
 experiments revealed that this simple method worked reasonably well in a
 non-parallel many-to-many speaker identity conversion task.",1,0,0,1,0,0,
385,Spectral analysis of jet turbulence,"Informed by LES data and resolvent analysis of the mean flow, we examine the
 structure of turbulence in jets in the subsonic, transonic, and supersonic
 regimes. Spectral (frequency-space) proper orthogonal decomposition is used to
 extract energy spectra and decompose the flow into energy-ranked coherent
 structures. The educed structures are generally well predicted by the resolvent
 analysis. Over a range of low frequencies and the first few azimuthal mode
 numbers, these jets exhibit a low-rank response characterized by
 Kelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer
 up to the end of the potential core and that are excited by forcing in the
 very-near-nozzle shear layer. These modes too the have been experimentally
 observed before and predicted by quasi-parallel stability theory and other
 approximations--they comprise a considerable portion of the total turbulent
 energy. At still lower frequencies, particularly for the axisymmetric mode, and
 again at high frequencies for all azimuthal wavenumbers, the response is not
 low rank, but consists of a family of similarly amplified modes. These modes,
 which are primarily active downstream of the potential core, are associated
 with the Orr mechanism. They occur also as sub-dominant modes in the range of
 frequencies dominated by the KH response. Our global analysis helps tie
 together previous observations based on local spatial stability theory, and
 explains why quasi-parallel predictions were successful at some frequencies and
 azimuthal wavenumbers, but failed at others.",0,1,0,0,0,0,
386,A dual framework for low-rank tensor completion,"One of the popular approaches for low-rank tensor completion is to use the
 latent trace norm regularization. However, most existing works in this
 direction learn a sparse combination of tensors. In this work, we fill this gap
 by proposing a variant of the latent trace norm that helps in learning a
 non-sparse combination of tensors. We develop a dual framework for solving the
 low-rank tensor completion problem. We first show a novel characterization of
 the dual solution space with an interesting factorization of the optimal
 solution. Overall, the optimal solution is shown to lie on a Cartesian product
 of Riemannian manifolds. Furthermore, we exploit the versatile Riemannian
 optimization framework for proposing computationally efficient trust region
 algorithm. The experiments illustrate the efficacy of the proposed algorithm on
 several real-world datasets across applications.",1,0,0,1,0,0,
387,La notion d'involution dans le Brouillon Project de Girard Desargues,"Nous tentons dans cet article de proposer une th??se coh??rente concernant
 la formation de la notion d'involution dans le Brouillon Project de Desargues.
 Pour cela, nous donnons une analyse d??taill??e des dix premi??res pages
 dudit Brouillon, comprenant les d??veloppements de cas particuliers qui aident
 ?ÿ comprendre l'intention de Desargues. Nous mettons cette analyse en regard
 de la lecture qu'en fait Jean de Beaugrand et que l'on trouve dans les Advis
 Charitables.
 The purpose of this article is to propose a coherent thesis on how Girard
 Desargues arrived at the notion of involution in his Brouillon Project of 1639.
 To this purpose we give a detailed analysis of the ten first pages of the
 Brouillon, including developments of particular cases which help to understand
 the goal of Desargues, as well as to clarify the links between the notion of
 involution and that of harmonic division. We compare the conclusions of this
 analysis with the very critical reading Jean de Beaugrand made of the Brouillon
 Project in the Advis Charitables of 1640.",0,0,1,0,0,0,
388,Framing U-Net via Deep Convolutional Framelets: Application to Sparse-view CT,"X-ray computed tomography (CT) using sparse projection views is a recent
 approach to reduce the radiation dose. However, due to the insufficient
 projection views, an analytic reconstruction approach using the filtered back
 projection (FBP) produces severe streaking artifacts. Recently, deep learning
 approaches using large receptive field neural networks such as U-Net have
 demonstrated impressive performance for sparse- view CT reconstruction.
 However, theoretical justification is still lacking. Inspired by the recent
 theory of deep convolutional framelets, the main goal of this paper is,
 therefore, to reveal the limitation of U-Net and propose new multi-resolution
 deep learning schemes. In particular, we show that the alternative U- Net
 variants such as dual frame and the tight frame U-Nets satisfy the so-called
 frame condition which make them better for effective recovery of high frequency
 edges in sparse view- CT. Using extensive experiments with real patient data
 set, we demonstrate that the new network architectures provide better
 reconstruction performance.",1,0,0,1,0,0,
389,Lie $\infty$-algebroids and singular foliations,"A singular (or Hermann) foliation on a smooth manifold $M$ can be seen as a
 subsheaf of the sheaf $\mathfrak{X}$ of vector fields on $M$. We show that if
 this singular foliation admits a resolution (in the sense of sheaves)
 consisting of sections of a graded vector bundle of finite type, then one can
 lift the Lie bracket of vector fields to a Lie $\infty$-algebroid structure on
 this resolution, that we call a universal Lie $\infty$-algebroid associated to
 the foliation. The name is justified because it is isomorphic (up to homotopy)
 to any other Lie $\infty$-algebroid structure built on any other resolution of
 the given singular foliation.",0,0,1,0,0,0,
390,Distinct evolutions of Weyl fermion quasiparticles and Fermi arcs with bulk band topology in Weyl semimetals,"The Weyl semimetal phase is a recently discovered topological quantum state
 of matter characterized by the presence of topologically protected degeneracies
 near the Fermi level. These degeneracies are the source of exotic phenomena,
 including the realization of chiral Weyl fermions as quasiparticles in the bulk
 and the formation of Fermi arc states on the surfaces. Here, we demonstrate
 that these two key signatures show distinct evolutions with the bulk band
 topology by performing angle-resolved photoemission spectroscopy, supported by
 first-principle calculations, on transition-metal monophosphides. While Weyl
 fermion quasiparticles exist only when the chemical potential is located
 between two saddle points of the Weyl cone features, the Fermi arc states
 extend in a larger energy scale and are robust across the bulk Lifshitz
 transitions associated with the recombination of two non-trivial Fermi surfaces
 enclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl
 points of opposite chirality. Therefore, in some systems (e.g. NbP),
 topological Fermi arc states are preserved even if Weyl fermion quasiparticles
 are absent in the bulk. Our findings not only provide insight into the
 relationship between the exotic physical phenomena and the intrinsic bulk band
 topology in Weyl semimetals, but also resolve the apparent puzzle of the
 different magneto-transport properties observed in TaAs, TaP and NbP, where the
 Fermi arc states are similar.",0,1,0,0,0,0,
391,Assessing inter-modal and inter-regional dependencies in prodromal Alzheimer's disease using multimodal MRI/PET and Gaussian graphical models,"A sequence of pathological changes takes place in Alzheimer's disease, which
 can be assessed in vivo using various brain imaging methods. Currently, there
 is no appropriate statistical model available that can easily integrate
 multiple imaging modalities, being able to utilize the additional information
 provided from the combined data. We applied Gaussian graphical models (GGMs)
 for analyzing the conditional dependency networks of multimodal neuroimaging
 data and assessed alterations of the network structure in mild cognitive
 impairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy
 controls.
 Data from N=667 subjects were obtained from the Alzheimer's Disease
 Neuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism
 (FDG-PET), and gray matter volume (MRI) was calculated for each brain region.
 Separate GGMs were estimated using a Bayesian framework for the combined
 multimodal data for each diagnostic category. Graph-theoretical statistics were
 calculated to determine network alterations associated with disease severity.
 Network measures clustering coefficient, path length and small-world
 coefficient were significantly altered across diagnostic groups, with a
 biphasic u-shape trajectory, i.e. increased small-world coefficient in early
 MCI, intermediate values in late MCI, and decreased values in AD patients
 compared to controls. In contrast, no group differences were found for
 clustering coefficient and small-world coefficient when estimating conditional
 dependency networks on single imaging modalities.
 GGMs provide a useful methodology to analyze the conditional dependency
 networks of multimodal neuroimaging data.",0,0,0,1,1,0,
392,On the economics of knowledge creation and sharing,"This work bridges the technical concepts underlying distributed computing and
 blockchain technologies with their profound socioeconomic and sociopolitical
 implications, particularly on academic research and the healthcare industry.
 Several examples from academia, industry, and healthcare are explored
 throughout this paper. The limiting factor in contemporary life sciences
 research is often funding: for example, to purchase expensive laboratory
 equipment and materials, to hire skilled researchers and technicians, and to
 acquire and disseminate data through established academic channels. In the case
 of the U.S. healthcare system, hospitals generate massive amounts of data, only
 a small minority of which is utilized to inform current and future medical
 practice. Similarly, corporations too expend large amounts of money to collect,
 secure and transmit data from one centralized source to another. In all three
 scenarios, data moves under the traditional paradigm of centralization, in
 which data is hosted and curated by individuals and organizations and of
 benefit to only a small subset of people.",1,0,0,0,0,0,
393,Seismic fragility curves for structures using non-parametric representations,"Fragility curves are commonly used in civil engineering to assess the
 vulnerability of structures to earthquakes. The probability of failure
 associated with a prescribed criterion (e.g. the maximal inter-storey drift of
 a building exceeding a certain threshold) is represented as a function of the
 intensity of the earthquake ground motion (e.g. peak ground acceleration or
 spectral acceleration). The classical approach relies on assuming a lognormal
 shape of the fragility curves; it is thus parametric. In this paper, we
 introduce two non-parametric approaches to establish the fragility curves
 without employing the above assumption, namely binned Monte Carlo simulation
 and kernel density estimation. As an illustration, we compute the fragility
 curves for a three-storey steel frame using a large number of synthetic ground
 motions. The curves obtained with the non-parametric approaches are compared
 with respective curves based on the lognormal assumption. A similar comparison
 is presented for a case when a limited number of recorded ground motions is
 available. It is found that the accuracy of the lognormal curves depends on the
 ground motion intensity measure, the failure criterion and most importantly, on
 the employed method for estimating the parameters of the lognormal shape.",0,0,0,1,0,0,
394,Metastable Markov chains: from the convergence of the trace to the convergence of the finite-dimensional distributions,"We consider continuous-time Markov chains which display a family of wells at
 the same depth. We provide sufficient conditions which entail the convergence
 of the finite-dimensional distributions of the order parameter to the ones of a
 finite state Markov chain. We also show that the state of the process can be
 represented as a time-dependent convex combination of metastable states, each
 of which is supported on one well.",0,0,1,0,0,0,
395,Construction of embedded periodic surfaces in $\mathbb{R}^n$,"We construct embedded minimal surfaces which are $n$-periodic in
 $\mathbb{R}^n$. They are new for codimension $n-2\ge 2$. We start with a Jordan
 curve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk
 which Schwarz reflection extends to a complete minimal surface. Studying the
 group of Schwarz reflections, we can characterize those Jordan curves for which
 the complete surface is embedded. For example, for $n=4$ exactly five such
 Jordan curves generate embedded surfaces. Our results apply to surface classes
 other than minimal as well, for instance polygonal surfaces.",0,0,1,0,0,0,
396,A System of Three Super Earths Transiting the Late K-Dwarf GJ 9827 at Thirty Parsecs,"We report the discovery of three small transiting planets orbiting GJ 9827, a
 bright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\pm0.11$
 $R_{\rm \oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$
 $R_{\rm \oplus}$ super Earth on a 3.6 day period, and a $2.07\pm0.14$ $R_{\rm
 \oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting
 GJ 9827 span the transition between predominantly rocky and gaseous planets,
 and GJ 9827 b and c fall in or close to the known gap in the radius
 distribution of small planets between these populations. At a distance of 30
 parsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making
 these planets well-suited for atmospheric studies with the upcoming James Webb
 Space Telescope. The GJ 9827 system provides a valuable opportunity to
 characterize interior structure and atmospheric properties of coeval planets
 spanning the rocky to gaseous transition.",0,1,0,0,0,0,
397,State Sum Invariants of Three Manifolds from Spherical Multi-fusion Categories,"We define a family of quantum invariants of closed oriented $3$-manifolds
 using spherical multi-fusion categories. The state sum nature of this invariant
 leads directly to $(2+1)$-dimensional topological quantum field theories
 ($\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury
 ($\text{TVBW}$) $\text{TQFT}$s from spherical fusion categories. The invariant
 is given as a state sum over labeled triangulations, which is mostly parallel
 to, but richer than the $\text{TVBW}$ approach in that here the labels live not
 only on $1$-simplices but also on $0$-simplices. It is shown that a
 multi-fusion category in general cannot be a spherical fusion category in the
 usual sense. Thus we introduce the concept of a spherical multi-fusion category
 by imposing a weakened version of sphericity. Besides containing the
 $\text{TVBW}$ theory, our construction also includes the recent higher gauge
 theory $(2+1)$-$\text{TQFT}$s given by Kapustin and Thorngren, which was not
 known to have a categorical origin before.",0,1,1,0,0,0,
398,Sparse Neural Networks Topologies,"We propose Sparse Neural Network architectures that are based on random or
 structured bipartite graph topologies. Sparse architectures provide compression
 of the models learned and speed-ups of computations, they can also surpass
 their unstructured or fully connected counterparts. As we show, even more
 compact topologies of the so-called SNN (Sparse Neural Network) can be achieved
 with the use of structured graphs of connections between consecutive layers of
 neurons. In this paper, we investigate how the accuracy and training speed of
 the models depend on the topology and sparsity of the neural network. Previous
 approaches using sparcity are all based on fully connected neural network
 models and create sparcity during training phase, instead we explicitly define
 a sparse architectures of connections before the training. Building compact
 neural network models is coherent with empirical observations showing that
 there is much redundancy in learned neural network models. We show
 experimentally that the accuracy of the models learned with neural networks
 depends on expander-like properties of the underlying topologies such as the
 spectral gap and algebraic connectivity rather than the density of the graphs
 of connections.",1,0,0,1,0,0,
399,Human perception in computer vision,"Computer vision has made remarkable progress in recent years. Deep neural
 network (DNN) models optimized to identify objects in images exhibit
 unprecedented task-trained accuracy and, remarkably, some generalization
 ability: new visual problems can now be solved more easily based on previous
 learning. Biological vision (learned in life and through evolution) is also
 accurate and general-purpose. Is it possible that these different learning
 regimes converge to similar problem-dependent optimal computations? We
 therefore asked whether the human system-level computation of visual perception
 has DNN correlates and considered several anecdotal test cases. We found that
 perceptual sensitivity to image changes has DNN mid-computation correlates,
 while sensitivity to segmentation, crowding and shape has DNN end-computation
 correlates. Our results quantify the applicability of using DNN computation to
 estimate perceptual loss, and are consistent with the fascinating theoretical
 view that properties of human perception are a consequence of
 architecture-independent visual learning.",1,0,0,0,0,0,
400,Analyses and estimation of certain design parameters of micro-grooved heat pipes,"A numerical analysis of heat conduction through the cover plate of a heat
 pipe is carried out to determine the temperature of the working substance,
 average temperature of heating and cooling surfaces, heat spread in the
 transmitter, and the heat bypass through the cover plate. Analysis has been
 extended for the estimation of heat transfer requirements at the outer surface
 of the con- denser under different heat load conditions using Genetic
 Algorithm. This paper also presents the estimation of an average heat transfer
 coefficient for the boiling and condensation of the working substance inside
 the microgrooves corresponding to a known temperature of the heat source. The
 equation of motion of the working fluid in the meniscus of an equilateral
 triangular groove has been presented from which a new term called the minimum
 surface tension required for avoiding the dry out condition is defined.
 Quantitative results showing the effect of thickness of cover plate, heat load,
 angle of inclination and viscosity of the working fluid on the different
 aspects of the heat transfer, minimum surface tension required to avoid dry
 out, velocity distribution of the liquid, and radius of liquid meniscus inside
 the micro-grooves have been presented and discussed.",0,1,0,0,0,0,
401,"Merge decompositions, two-sided Krohn-Rhodes, and aperiodic pointlikes","This paper provides short proofs of two fundamental theorems of finite
 semigroup theory whose previous proofs were significantly longer, namely the
 two-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike
 theorem, using a new algebraic technique that we call the merge decomposition.
 A prototypical application of this technique decomposes a semigroup $T$ into a
 two-sided semidirect product whose components are built from two subsemigroups
 $T_1,T_2$, which together generate $T$, and the subsemigroup generated by their
 setwise product $T_1T_2$. In this sense we decompose $T$ by merging the
 subsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup
 homomorphisms from free semigroups.",1,0,1,0,0,0,
402,Deep Multimodal Image-Repurposing Detection,"Nefarious actors on social media and other platforms often spread rumors and
 falsehoods through images whose metadata (e.g., captions) have been modified to
 provide visual substantiation of the rumor/falsehood. This type of modification
 is referred to as image repurposing, in which often an unmanipulated image is
 published along with incorrect or manipulated metadata to serve the actor's
 ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
 dataset, a substantially challenging dataset over that which has been
 previously available to support research into image repurposing detection. The
 new dataset includes location, person, and organization manipulations on
 real-world data sourced from Flickr. We also present a novel, end-to-end, deep
 multimodal learning model for assessing the integrity of an image by combining
 information extracted from the image with related information from a knowledge
 base. The proposed method is compared against state-of-the-art techniques on
 existing datasets as well as MEIR, where it outperforms existing methods across
 the board, with AUC improvement up to 0.23.",1,0,0,0,0,0,
403,DeepFense: Online Accelerated Defense Against Adversarial Deep Learning,"Recent advances in adversarial Deep Learning (DL) have opened up a largely
 unexplored surface for malicious attacks jeopardizing the integrity of
 autonomous DL systems. With the wide-spread usage of DL in critical and
 time-sensitive applications, including unmanned vehicles, drones, and video
 surveillance systems, online detection of malicious inputs is of utmost
 importance. We propose DeepFense, the first end-to-end automated framework that
 simultaneously enables efficient and safe execution of DL models. DeepFense
 formalizes the goal of thwarting adversarial attacks as an optimization problem
 that minimizes the rarely observed regions in the latent feature space spanned
 by a DL network. To solve the aforementioned minimization problem, a set of
 complementary but disjoint modular redundancies are trained to validate the
 legitimacy of the input samples in parallel with the victim DL model. DeepFense
 leverages hardware/software/algorithm co-design and customized acceleration to
 achieve just-in-time performance in resource-constrained settings. The proposed
 countermeasure is unsupervised, meaning that no adversarial sample is leveraged
 to train modular redundancies. We further provide an accompanying API to reduce
 the non-recurring engineering cost and ensure automated adaptation to various
 platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
 of magnitude performance improvement while enabling online adversarial sample
 detection.",1,0,0,1,0,0,
404,Retrospective Higher-Order Markov Processes for User Trails,"Users form information trails as they browse the web, checkin with a
 geolocation, rate items, or consume media. A common problem is to predict what
 a user might do next for the purposes of guidance, recommendation, or
 prefetching. First-order and higher-order Markov chains have been widely used
 methods to study such sequences of data. First-order Markov chains are easy to
 estimate, but lack accuracy when history matters. Higher-order Markov chains,
 in contrast, have too many parameters and suffer from overfitting the training
 data. Fitting these parameters with regularization and smoothing only offers
 mild improvements. In this paper we propose the retrospective higher-order
 Markov process (RHOMP) as a low-parameter model for such sequences. This model
 is a special case of a higher-order Markov chain where the transitions depend
 retrospectively on a single history state instead of an arbitrary combination
 of history states. There are two immediate computational advantages: the number
 of parameters is linear in the order of the Markov chain and the model can be
 fit to large state spaces. Furthermore, by providing a specific structure to
 the higher-order chain, RHOMPs improve the model accuracy by efficiently
 utilizing history states without risks of overfitting the data. We demonstrate
 how to estimate a RHOMP from data and we demonstrate the effectiveness of our
 method on various real application datasets spanning geolocation data, review
 sequences, and business locations. The RHOMP model uniformly outperforms
 higher-order Markov chains, Kneser-Ney regularization, and tensor
 factorizations in terms of prediction accuracy.",1,0,0,1,0,0,
405,Frank-Wolfe with Subsampling Oracle,"We analyze two novel randomized variants of the Frank-Wolfe (FW) or
 conditional gradient algorithm. While classical FW algorithms require solving a
 linear minimization problem over the domain at each iteration, the proposed
 method only requires to solve a linear minimization problem over a small
 \emph{subset} of the original domain. The first algorithm that we propose is a
 randomized variant of the original FW algorithm and achieves a
 $\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic
 counterpart. The second algorithm is a randomized variant of the Away-step FW
 algorithm, and again as its deterministic counterpart, reaches linear (i.e.,
 exponential) convergence rate making it the first provably convergent
 randomized variant of Away-step FW. In both cases, while subsampling reduces
 the convergence rate by a constant factor, the linear minimization step can be
 a fraction of the cost of that of the deterministic versions, especially when
 the data is streamed. We illustrate computational gains of the algorithms on
 regression problems, involving both $\ell_1$ and latent group lasso penalties.",0,0,0,1,0,0,
406,A mean value formula and a Liouville theorem for the complex Monge-Amp??re equation,"In this paper, we prove a mean value formula for bounded subharmonic
 Hermitian matrix valued function on a complete Riemannian manifold with
 nonnegative Ricci curvature. As its application, we obtain a Liouville type
 theorem for the complex Monge-Amp??re equation on product manifolds.",0,0,1,0,0,0,
407,Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI Super-Resolution,"In this work, we investigate the value of uncertainty modeling in 3D
 super-resolution with convolutional neural networks (CNNs). Deep learning has
 shown success in a plethora of medical image transformation problems, such as
 super-resolution (SR) and image synthesis. However, the highly ill-posed nature
 of such problems results in inevitable ambiguity in the learning of networks.
 We propose to account for intrinsic uncertainty through a per-patch
 heteroscedastic noise model and for parameter uncertainty through approximate
 Bayesian inference in the form of variational dropout. We show that the
 combined benefits of both lead to the state-of-the-art performance SR of
 diffusion MR brain images in terms of errors compared to ground truth. We
 further show that the reduced error scores produce tangible benefits in
 downstream tractography. In addition, the probabilistic nature of the methods
 naturally confers a mechanism to quantify uncertainty over the super-resolved
 output. We demonstrate through experiments on both healthy and pathological
 brains the potential utility of such an uncertainty measure in the risk
 assessment of the super-resolved images for subsequent clinical use.",1,0,0,0,0,0,
408,Yu-Shiba-Rusinov bands in superconductors in contact with a magnetic insulator,"Superconductor-Ferromagnet (SF) heterostructures are of interest due to
 numerous phenomena related to the spin-dependent interaction of Cooper pairs
 with the magnetization. Here we address the effects of a magnetic insulator on
 the density of states of a superconductor based on a recently developed
 boundary condition for strongly spin-dependent interfaces. We show that the
 boundary to a magnetic insulator has a similar effect like the presence of
 magnetic impurities. In particular we find that the impurity effects of
 strongly scattering localized spins leading to the formation of Shiba bands can
 be mapped onto the boundary problem.",0,1,0,0,0,0,
409,Bayesian Optimization for Probabilistic Programs,"We present the first general purpose framework for marginal maximum a
 posteriori estimation of probabilistic program variables. By using a series of
 code transformations, the evidence of any probabilistic program, and therefore
 of any graphical model, can be optimized with respect to an arbitrary subset of
 its sampled variables. To carry out this optimization, we develop the first
 Bayesian optimization package to directly exploit the source code of its
 target, leading to innovations in problem-independent hyperpriors, unbounded
 optimization, and implicit constraint satisfaction; delivering significant
 performance improvements over prominent existing packages. We present
 applications of our method to a number of tasks including engineering design
 and parameter optimization.",1,0,0,1,0,0,
410,Long-Term Load Forecasting Considering Volatility Using Multiplicative Error Model,"Long-term load forecasting plays a vital role for utilities and planners in
 terms of grid development and expansion planning. An overestimate of long-term
 electricity load will result in substantial wasted investment in the
 construction of excess power facilities, while an underestimate of future load
 will result in insufficient generation and unmet demand. This paper presents
 first-of-its-kind approach to use multiplicative error model (MEM) in
 forecasting load for long-term horizon. MEM originates from the structure of
 autoregressive conditional heteroscedasticity (ARCH) model where conditional
 variance is dynamically parameterized and it multiplicatively interacts with an
 innovation term of time-series. Historical load data, accessed from a U.S.
 regional transmission operator, and recession data for years 1993-2016 is used
 in this study. The superiority of considering volatility is proven by
 out-of-sample forecast results as well as directional accuracy during the great
 economic recession of 2008. To incorporate future volatility, backtesting of
 MEM model is performed. Two performance indicators used to assess the proposed
 model are mean absolute percentage error (for both in-sample model fit and
 out-of-sample forecasts) and directional accuracy.",0,0,0,1,0,0,
411,Geometrically stopped Markovian random growth processes and Pareto tails,"Many empirical studies document power law behavior in size distributions of
 economic interest such as cities, firms, income, and wealth. One mechanism for
 generating such behavior combines independent and identically distributed
 Gaussian additive shocks to log-size with a geometric age distribution. We
 generalize this mechanism by allowing the shocks to be non-Gaussian (but
 light-tailed) and dependent upon a Markov state variable. Our main results
 provide sharp bounds on tail probabilities and simple formulas for Pareto
 exponents. We present two applications: (i) we show that the tails of the
 wealth distribution in a heterogeneous-agent dynamic general equilibrium model
 with idiosyncratic endowment risk decay exponentially, unlike models with
 investment risk where the tails may be Paretian, and (ii) we show that a random
 growth model for the population dynamics of Japanese prefectures is consistent
 with the observed Pareto exponent but only after allowing for Markovian
 dynamics.",0,0,1,0,0,0,
412,Mathematics of Topological Quantum Computing,"In topological quantum computing, information is encoded in ""knotted"" quantum
 states of topological phases of matter, thus being locked into topology to
 prevent decay. Topological precision has been confirmed in quantum Hall liquids
 by experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum
 memory. In this survey, we discuss the conceptual development of this
 interdisciplinary field at the juncture of mathematics, physics and computer
 science. Our focus is on computing and physical motivations, basic mathematical
 notions and results, open problems and future directions related to and/or
 inspired by topological quantum computing.",0,1,1,0,0,0,
413,Graph Clustering using Effective Resistance,"$ \def\vecc#1{\boldsymbol{#1}} $We design a polynomial time algorithm that
 for any weighted undirected graph $G = (V, E,\vecc w)$ and sufficiently large
 $\delta > 1$, partitions $V$ into subsets $V_1, \ldots, V_h$ for some $h\geq
 1$, such that
 $\bullet$ at most $\delta^{-1}$ fraction of the weights are between clusters,
 i.e. \[ w(E - \cup_{i = 1}^h E(V_i)) \lesssim \frac{w(E)}{\delta};\]
 $\bullet$ the effective resistance diameter of each of the induced subgraphs
 $G[V_i]$ is at most $\delta^3$ times the average weighted degree, i.e. \[
 \max_{u, v \in V_i} \mathsf{Reff}_{G[V_i]}(u, v) \lesssim \delta^3 \cdot
 \frac{|V|}{w(E)} \quad \text{ for all } i=1, \ldots, h.\]
 In particular, it is possible to remove one percent of weight of edges of any
 given graph such that each of the resulting connected components has effective
 resistance diameter at most the inverse of the average weighted degree.
 Our proof is based on a new connection between effective resistance and low
 conductance sets. We show that if the effective resistance between two vertices
 $u$ and $v$ is large, then there must be a low conductance cut separating $u$
 from $v$. This implies that very mildly expanding graphs have constant
 effective resistance diameter. We believe that this connection could be of
 independent interest in algorithm design.",1,0,0,0,0,0,
414,Self-supervised learning: When is fusion of the primary and secondary sensor cue useful?,"Self-supervised learning (SSL) is a reliable learning mechanism in which a
 robot enhances its perceptual capabilities. Typically, in SSL a trusted,
 primary sensor cue provides supervised training data to a secondary sensor cue.
 In this article, a theoretical analysis is performed on the fusion of the
 primary and secondary cue in a minimal model of SSL. A proof is provided that
 determines the specific conditions under which it is favorable to perform
 fusion. In short, it is favorable when (i) the prior on the target value is
 strong or (ii) the secondary cue is sufficiently accurate. The theoretical
 findings are validated with computational experiments. Subsequently, a
 real-world case study is performed to investigate if fusion in SSL is also
 beneficial when assumptions of the minimal model are not met. In particular, a
 flying robot learns to map pressure measurements to sonar height measurements
 and then fuses the two, resulting in better height estimation. Fusion is also
 beneficial in the opposite case, when pressure is the primary cue. The analysis
 and results are encouraging to study SSL fusion also for other robots and
 sensors.",1,0,0,0,0,0,
415,Playing Atari with Six Neurons,"Deep reinforcement learning on Atari games maps pixel directly to actions;
 internally, the deep neural network bears the responsibility of both extracting
 useful information and making decisions based on it. Aiming at devoting entire
 deep networks to decision making alone, we propose a new method for learning
 policies and compact state representations separately but simultaneously for
 policy approximation in reinforcement learning. State representations are
 generated by a novel algorithm based on Vector Quantization and Sparse Coding,
 trained online along with the network, and capable of growing its dictionary
 size over time. We also introduce new techniques allowing both the neural
 network and the evolution strategy to cope with varying dimensions. This
 enables networks of only 6 to 18 neurons to learn to play a selection of Atari
 games with performance comparable---and occasionally superior---to
 state-of-the-art techniques using evolution strategies on deep networks two
 orders of magnitude larger.",0,0,0,1,0,0,
416,Contraction and uniform convergence of isotonic regression,"We consider the problem of isotonic regression, where the underlying signal
 $x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the
 cone $\{ x\in\mathbb{R}^n : x_1 \leq \dots \leq x_n\}$. We study the isotonic
 projection operator (projection to this cone), and find a necessary and
 sufficient condition characterizing all norms with respect to which this
 projection is contractive. This enables a simple and non-asymptotic analysis of
 the convergence properties of isotonic regression, yielding uniform confidence
 bands that adapt to the local Lipschitz properties of the signal.",0,0,1,1,0,0,
417,A Systematic Approach for Exploring Tradeoffs in Predictive HVAC Control Systems for Buildings,"Heating, Ventilation, and Cooling (HVAC) systems are often the most
 significant contributor to the energy usage, and the operational cost, of large
 office buildings. Therefore, to understand the various factors affecting the
 energy usage, and to optimize the operational efficiency of building HVAC
 systems, energy analysts and architects often create simulations (e.g.,
 EnergyPlus or DOE-2), of buildings prior to construction or renovation to
 determine energy savings and quantify the Return-on-Investment (ROI). While
 useful, these simulations usually use static HVAC control strategies such as
 lowering room temperature at night, or reactive control based on simulated room
 occupancy. Recently, advances have been made in HVAC control algorithms that
 predict room occupancy. However, these algorithms depend on costly sensor
 installations and the tradeoffs between predictive accuracy, energy savings,
 comfort and expenses are not well understood. Current simulation frameworks do
 not support easy analysis of these tradeoffs. Our contribution is a simulation
 framework that can be used to explore this design space by generating objective
 estimates of the energy savings and occupant comfort for different levels of
 HVAC prediction and control performance. We validate our framework on a
 real-world occupancy dataset spanning 6 months for 235 rooms in a large
 university office building. Using the gold standard of energy use modeling and
 simulation (Revit and Energy Plus), we compare the energy consumption and
 occupant comfort in 29 independent simulations that explore our parameter
 space. Our results highlight a number of potentially useful tradeoffs with
 respect to energy savings, comfort, and algorithmic performance among
 predictive, reactive, and static schedules, for a stakeholder of our building.",1,0,0,0,0,0,
418,On types of degenerate critical points of real polynomial functions,"In this paper, we consider the problem of identifying the type (local
 minimizer, maximizer or saddle point) of a given isolated real critical point
 $c$, which is degenerate, of a multivariate polynomial function $f$. To this
 end, we introduce the definition of faithful radius of $c$ by means of the
 curve of tangency of $f$. We show that the type of $c$ can be determined by the
 global extrema of $f$ over the Euclidean ball centered at $c$ with a faithful
 radius.We propose algorithms to compute a faithful radius of $c$ and determine
 its type.",0,0,1,0,0,0,
419,Contribution of Data Categories to Readmission Prediction Accuracy,"Identification of patients at high risk for readmission could help reduce
 morbidity and mortality as well as healthcare costs. Most of the existing
 studies on readmission prediction did not compare the contribution of data
 categories. In this study we analyzed relative contribution of 90,101 variables
 across 398,884 admission records corresponding to 163,468 patients, including
 patient demographics, historical hospitalization information, discharge
 disposition, diagnoses, procedures, medications and laboratory test results. We
 established an interpretable readmission prediction model based on Logistic
 Regression in scikit-learn, and added the available variables to the model one
 by one in order to analyze the influences of individual data categories on
 readmission prediction accuracy. Diagnosis related groups (c-statistic
 increment of 0.0933) and discharge disposition (c-statistic increment of
 0.0269) were the strongest contributors to model accuracy. Additionally, we
 also identified the top ten contributing variables in every data category.",0,0,0,0,1,0,
420,Tropical recurrent sequences,"Tropical recurrent sequences are introduced satisfying a given vector (being
 a tropical counterpart of classical linear recurrent sequences). We consider
 the case when Newton polygon of the vector has a single (bounded) edge. In this
 case there are periodic tropical recurrent sequences which are similar to
 classical linear recurrent sequences. A question is studied when there exists a
 non-periodic tropical recurrent sequence satisfying a given vector, and partial
 answers are provided to this question. Also an algorithm is designed which
 tests existence of non-periodic tropical recurrent sequences satisfying a given
 vector with integer coordinates. Finally, we introduce a tropical entropy of a
 vector and provide some bounds on it.",1,0,0,0,0,0,
421,Invariance of Weight Distributions in Rectified MLPs,"An interesting approach to analyzing neural networks that has received
 renewed attention is to examine the equivalent kernel of the neural network.
 This is based on the fact that a fully connected feedforward network with one
 hidden layer, a certain weight distribution, an activation function, and an
 infinite number of neurons can be viewed as a mapping into a Hilbert space. We
 derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for
 all rotationally-invariant weight distributions, generalizing a previous result
 that required Gaussian weight distributions. Additionally, the Central Limit
 Theorem is used to show that for certain activation functions, kernels
 corresponding to layers with weight distributions having $0$ mean and finite
 absolute third moment are asymptotically universal, and are well approximated
 by the kernel corresponding to layers with spherical Gaussian weights. In deep
 networks, as depth increases the equivalent kernel approaches a pathological
 fixed point, which can be used to argue why training randomly initialized
 networks can be difficult. Our results also have implications for weight
 initialization.",1,0,0,1,0,0,
422,Learning to Adapt by Minimizing Discrepancy,"We explore whether useful temporal neural generative models can be learned
 from sequential data without back-propagation through time. We investigate the
 viability of a more neurocognitively-grounded approach in the context of
 unsupervised generative modeling of sequences. Specifically, we build on the
 concept of predictive coding, which has gained influence in cognitive science,
 in a neural framework. To do so we develop a novel architecture, the Temporal
 Neural Coding Network, and its learning algorithm, Discrepancy Reduction. The
 underlying directed generative model is fully recurrent, meaning that it
 employs structural feedback connections and temporal feedback connections,
 yielding information propagation cycles that create local learning signals.
 This facilitates a unified bottom-up and top-down approach for information
 transfer inside the architecture. Our proposed algorithm shows promise on the
 bouncing balls generative modeling problem. Further experiments could be
 conducted to explore the strengths and weaknesses of our approach.",1,0,0,1,0,0,
423,Incarnation of Majorana Fermions in Kitaev Quantum Spin Lattice,"Kitaev quantum spin liquid is a topological magnetic quantum state
 characterized by Majorana fermions of fractionalized spin excitations, which
 are identical to their own antiparticles. Here, we demonstrate emergence of
 Majorana fermions thermally fractionalized in the Kitaev honeycomb spin lattice
 {\alpha}-RuCl3. The specific heat data unveil the characteristic two-stage
 release of magnetic entropy involving localized and itinerant Majorana
 fermions. The inelastic neutron scattering results further corroborate these
 two distinct fermions by exhibiting quasielastic excitations at low energies
 around the Brillouin zone center and Y-shaped magnetic continuum at high
 energies, which are evident for the ferromagnetic Kitaev model. Our results
 provide an opportunity to build a unified conceptual framework of
 fractionalized excitations, applicable also for the quantum Hall states,
 superconductors, and frustrated magnets.",0,1,0,0,0,0,
424,Haro 11: Where is the Lyman continuum source?,"Identifying the mechanism by which high energy Lyman continuum (LyC) photons
 escaped from early galaxies is one of the most pressing questions in cosmic
 evolution. Haro 11 is the best known local LyC leaking galaxy, providing an
 important opportunity to test our understanding of LyC escape. The observed LyC
 emission in this galaxy presumably originates from one of the three bright,
 photoionizing knots known as A, B, and C. It is known that Knot C has strong
 Ly$\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray
 source, which may be a low-luminosity AGN. To clarify the LyC source, we carry
 out ionization-parameter mapping (IPM) by obtaining narrow-band imaging from
 the Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved
 ratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization
 structure of the interstellar medium and allows us to identify optically thin
 regions. To optimize the continuum subtraction, we introduce a new method for
 determining the best continuum scale factor derived from the mode of the
 continuum-subtracted, image flux distribution. We find no conclusive evidence
 of LyC escape from Knots B or C, but instead, we identify a high-ionization
 region extending over at least 1 kpc from Knot A. Knot A shows evidence of an
 extremely young age ($\lesssim 1$ Myr), perhaps containing very massive stars
 ($>100$ M$_\odot$). It is weak in Ly$\alpha$, so if it is confirmed as the LyC
 source, our results imply that LyC emission may be independent of Ly$\alpha$
 emission.",0,1,0,0,0,0,
425,Typed Closure Conversion for the Calculus of Constructions,"Dependently typed languages such as Coq are used to specify and verify the
 full functional correctness of source programs. Type-preserving compilation can
 be used to preserve these specifications and proofs of correctness through
 compilation into the generated target-language programs. Unfortunately,
 type-preserving compilation of dependent types is hard. In essence, the problem
 is that dependent type systems are designed around high-level compositional
 abstractions to decide type checking, but compilation interferes with the
 type-system rules for reasoning about run-time terms.
 We develop a type-preserving closure-conversion translation from the Calculus
 of Constructions (CC) with strong dependent pairs ($\Sigma$ types)---a subset
 of the core language of Coq---to a type-safe, dependently typed compiler
 intermediate language named CC-CC. The central challenge in this work is how to
 translate the source type-system rules for reasoning about functions into
 target type-system rules for reasoning about closures. To justify these rules,
 we prove soundness of CC-CC by giving a model in CC. In addition to type
 preservation, we prove correctness of separate compilation.",1,0,0,0,0,0,
426,Untangling Planar Curves,"Any generic closed curve in the plane can be transformed into a simple closed
 curve by a finite sequence of local transformations called homotopy moves. We
 prove that simplifying a planar closed curve with $n$ self-crossings requires
 $\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the
 best previous upper bound $O(n^2)$, which is already implicit in the classical
 work of Steinitz; the matching lower bound follows from the construction of
 closed curves with large defect, a topological invariant of generic closed
 curves introduced by Aicardi and Arnold. Our lower bound also implies that
 $\Omega(n^{3/2})$ facial electrical transformations are required to reduce any
 plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex, matching
 known upper bounds for rectangular and cylindrical grid graphs. More generally,
 we prove that transforming one immersion of $k$ circles with at most $n$
 self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy
 moves in the worst case. Finally, we prove that transforming one
 noncontractible closed curve to another on any orientable surface requires
 $\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if
 the curve is homotopic to a simple closed curve.",1,0,1,0,0,0,
427,Greedy-Merge Degrading has Optimal Power-Law,"Consider a channel with a given input distribution. Our aim is to degrade it
 to a channel with at most L output letters. One such degradation method is the
 so called ""greedy-merge"" algorithm. We derive an upper bound on the reduction
 in mutual information between input and output. For fixed input alphabet size
 and variable L, the upper bound is within a constant factor of an
 algorithm-independent lower bound. Thus, we establish that greedy-merge is
 optimal in the power-law sense.",1,0,1,0,0,0,
428,Radially distributed values and normal families,"Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let
 ${\mathcal F}$ be the family of all functions holomorphic in the unit disk
 ${\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on
 $L_1$. It is shown that ${\mathcal F}$ is normal in ${\mathbb
 D}\backslash\{0\}$. The case where $L_0$ is the positive real axis and $L_1$ is
 the negative real axis is studied in more detail.",0,0,1,0,0,0,
429,Characterizing Exoplanet Habitability,"A habitable exoplanet is a world that can maintain stable liquid water on its
 surface. Techniques and approaches to characterizing such worlds are essential,
 as performing a census of Earth-like planets that may or may not have life will
 inform our understanding of how frequently life originates and is sustained on
 worlds other than our own. Observational techniques like high contrast imaging
 and transit spectroscopy can reveal key indicators of habitability for
 exoplanets. Both polarization measurements and specular reflectance from oceans
 (also known as ""glint"") can provide direct evidence for surface liquid water,
 while constraining surface pressure and temperature (from moderate resolution
 spectra) can indicate liquid water stability. Indirect evidence for
 habitability can come from a variety of sources, including observations of
 variability due to weather, surface mapping studies, and/or measurements of
 water vapor or cloud profiles that indicate condensation near a surface.
 Approaches to making the types of measurements that indicate habitability are
 diverse, and have different considerations for the required wavelength range,
 spectral resolution, maximum noise levels, stellar host temperature, and
 observing geometry.",0,1,0,0,0,0,
430,Deep Learning for Classification Tasks on Geospatial Vector Polygons,"In this paper, we evaluate the accuracy of deep learning approaches on
 geospatial vector geometry classification tasks. The purpose of this evaluation
 is to investigate the ability of deep learning models to learn from geometry
 coordinates directly. Previous machine learning research applied to geospatial
 polygon data did not use geometries directly, but derived properties thereof.
 These are produced by way of extracting geometry properties such as Fourier
 descriptors. Instead, our introduced deep neural net architectures are able to
 learn on sequences of coordinates mapped directly from polygons. In three
 classification tasks we show that the deep learning architectures are
 competitive with common learning algorithms that require extracted features.",0,0,0,1,0,0,
431,The Distance Standard Deviation,"The distance standard deviation, which arises in distance correlation
 analysis of multivariate data, is studied as a measure of spread. New
 representations for the distance standard deviation are obtained in terms of
 Gini's mean difference and in terms of the moments of spacings of order
 statistics. Inequalities for the distance variance are derived, proving that
 the distance standard deviation is bounded above by the classical standard
 deviation and by Gini's mean difference. Further, it is shown that the distance
 standard deviation satisfies the axiomatic properties of a measure of spread.
 Explicit closed-form expressions for the distance variance are obtained for a
 broad class of parametric distributions. The asymptotic distribution of the
 sample distance variance is derived.",0,0,1,1,0,0,
432,Combining learned and analytical models for predicting action effects,"One of the most basic skills a robot should possess is predicting the effect
 of physical interactions with objects in the environment. This enables optimal
 action selection to reach a certain goal state. Traditionally, dynamics are
 approximated by physics-based analytical models. These models rely on specific
 state representations that may be hard to obtain from raw sensory data,
 especially if no knowledge of the object shape is assumed. More recently, we
 have seen learning approaches that can predict the effect of complex physical
 interactions directly from sensory input. It is however an open question how
 far these models generalize beyond their training data. In this work, we
 investigate the advantages and limitations of neural network based learning
 approaches for predicting the effects of actions based on sensory input and
 show how analytical and learned models can be combined to leverage the best of
 both worlds. As physical interaction task, we use planar pushing, for which
 there exists a well-known analytical model and a large real-world dataset. We
 propose to use a convolutional neural network to convert raw depth images or
 organized point clouds into a suitable representation for the analytical model
 and compare this approach to using neural networks for both, perception and
 prediction. A systematic evaluation of the proposed approach on a very large
 real-world dataset shows two main advantages of the hybrid architecture.
 Compared to a pure neural network, it significantly (i) reduces required
 training data and (ii) improves generalization to novel physical interaction.",1,0,0,0,0,0,
433,Leavitt path algebras: Graded direct-finiteness and graded $?œ$-injective simple modules,"In this paper, we give a complete characterization of Leavitt path algebras
 which are graded $\Sigma $-$V$ rings, that is, rings over which a direct sum of
 arbitrary copies of any graded simple module is graded injective. Specifically,
 we show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded
 $\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of
 arbitrary size but with finitely many non-zero entries over $K$ or
 $K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical
 characterization of such a graded $\Sigma $-$V$ ring $L$% . When the graph $E$
 is finite, we show that $L$ is a graded $\Sigma $-$V$ ring $\Longleftrightarrow
 L$ is graded directly-finite $\Longleftrightarrow L $ has bounded index of
 nilpotence $\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that
 the equivalence of these properties in the preceding statement no longer holds
 when the graph $E$ is infinite. Following this, we also characterize Leavitt
 path algebras $L$ which are non-graded $\Sigma $-$V$ rings. Graded rings which
 are graded directly-finite are explored and it is shown that if a Leavitt path
 algebra $L$ is a graded $\Sigma$-$V$ ring, then $L$ is always graded
 directly-finite. Examples show the subtle differences between graded and
 non-graded directly-finite rings. Leavitt path algebras which are graded
 directly-finite are shown to be directed unions of graded semisimple rings.
 Using this, we give an alternative proof of a theorem of Va­ \cite{V} on
 directly-finite Leavitt path algebras.",0,0,1,0,0,0,
434,Ensemble Estimation of Mutual Information,"We derive the mean squared error convergence rates of kernel density-based
 plug-in estimators of mutual information measures between two multidimensional
 random variables $\mathbf{X}$ and $\mathbf{Y}$ for two cases: 1) $\mathbf{X}$
 and $\mathbf{Y}$ are both continuous; 2) $\mathbf{X}$ is continuous and
 $\mathbf{Y}$ is discrete. Using the derived rates, we propose an ensemble
 estimator of these information measures for the second case by taking a
 weighted sum of the plug-in estimators with varied bandwidths. The resulting
 ensemble estimator achieves the $1/N$ parametric convergence rate when the
 conditional densities of the continuous variables are sufficiently smooth. To
 the best of our knowledge, this is the first nonparametric mutual information
 estimator known to achieve the parametric convergence rate for this case, which
 frequently arises in applications (e.g. variable selection in classification).
 The estimator is simple to implement as it uses the solution to an offline
 convex optimization problem and simple plug-in estimators. A central limit
 theorem is also derived for the ensemble estimator. Ensemble estimators that
 achieve the parametric rate are also derived for the first case ($\mathbf{X}$
 and $\mathbf{Y}$ are both continuous) and another case 3) $\mathbf{X}$ and
 $\mathbf{Y}$ may have any mixture of discrete and continuous components.",1,0,1,1,0,0,
435,Social media mining for identification and exploration of health-related information from pregnant women,"Widespread use of social media has led to the generation of substantial
 amounts of information about individuals, including health-related information.
 Social media provides the opportunity to study health-related information about
 selected population groups who may be of interest for a particular study. In
 this paper, we explore the possibility of utilizing social media to perform
 targeted data collection and analysis from a particular population group --
 pregnant women. We hypothesize that we can use social media to identify cohorts
 of pregnant women and follow them over time to analyze crucial health-related
 information. To identify potentially pregnant women, we employ simple
 rule-based searches that attempt to detect pregnancy announcements with
 moderate precision. To further filter out false positives and noise, we employ
 a supervised classifier using a small number of hand-annotated data. We then
 collect their posts over time to create longitudinal health timelines and
 attempt to divide the timelines into different pregnancy trimesters. Finally,
 we assess the usefulness of the timelines by performing a preliminary analysis
 to estimate drug intake patterns of our cohort at different trimesters. Our
 rule-based cohort identification technique collected 53,820 users over thirty
 months from Twitter. Our pregnancy announcement classification technique
 achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user
 timelines. Analysis of the timelines revealed that pertinent health-related
 information, such as drug-intake and adverse reactions can be mined from the
 data. Our approach to using user timelines in this fashion has produced very
 encouraging results and can be employed for other important tasks where
 cohorts, for which health-related information may not be available from other
 sources, are required to be followed over time to derive population-based
 estimates.",1,0,0,0,0,0,
436,The vortex method for 2D ideal flows in the exterior of a disk,"The vortex method is a common numerical and theoretical approach used to
 implement the motion of an ideal flow, in which the vorticity is approximated
 by a sum of point vortices, so that the Euler equations read as a system of
 ordinary differential equations. Such a method is well justified in the full
 plane, thanks to the explicit representation formulas of Biot and Savart. In an
 exterior domain, we also replace the impermeable boundary by a collection of
 point vortices generating the circulation around the obstacle. The density of
 these point vortices is chosen in order that the flow remains tangent at
 midpoints between adjacent vortices. In this work, we provide a rigorous
 justification for this method in exterior domains. One of the main mathematical
 difficulties being that the Biot-Savart kernel defines a singular integral
 operator when restricted to a curve. For simplicity and clarity, we only treat
 the case of the unit disk in the plane approximated by a uniformly distributed
 mesh of point vortices. The complete and general version of our work is
 available in [arXiv:1707.01458].",0,0,1,0,0,0,
437,Neeman's characterization of K(R-Proj) via Bousfield localization,"Let $R$ be an associative ring with unit and denote by $K({\rm R
 \mbox{-}Proj})$ the homotopy category of complexes of projective left
 $R$-modules. Neeman proved the theorem that $K({\rm R \mbox{-}Proj})$ is
 $\aleph_1$-compactly generated, with the category $K^+ ({\rm R \mbox{-}proj})$
 of left bounded complexes of finitely generated projective $R$-modules
 providing an essentially small class of such generators. Another proof of
 Neeman's theorem is explained, using recent ideas of Christensen and Holm, and
 Emmanouil. The strategy of the proof is to show that every complex in $K({\rm R
 \mbox{-}Proj})$ vanishes in the Bousfield localization $K({\rm R
 \mbox{-}Flat})/\langle K^+ ({\rm R \mbox{-}proj}) \rangle.$",0,0,1,0,0,0,
438,Qualification Conditions in Semi-algebraic Programming,"For an arbitrary finite family of semi-algebraic/definable functions, we
 consider the corresponding inequality constraint set and we study qualification
 conditions for perturbations of this set. In particular we prove that all
 positive diagonal perturbations, save perhaps a finite number of them, ensure
 that any point within the feasible set satisfies Mangasarian-Fromovitz
 constraint qualification. Using the Milnor-Thom theorem, we provide a bound for
 the number of singular perturbations when the constraints are polynomial
 functions. Examples show that the order of magnitude of our exponential bound
 is relevant. Our perturbation approach provides a simple protocol to build
 sequences of ""regular"" problems approximating an arbitrary
 semi-algebraic/definable problem. Applications to sequential quadratic
 programming methods and sum of squares relaxation are provided.",0,0,1,0,0,0,
439,Curvature in Hamiltonian Mechanics And The Einstein-Maxwell-Dilaton Action,"Riemannian geometry is a particular case of Hamiltonian mechanics: the orbits
 of the hamiltonian $H=\frac{1}{2}g^{ij}p_{i}p_{j}$ are the geodesics. Given a
 symplectic manifold (\Gamma,\omega), a hamiltonian $H:\Gamma\to\mathbb{R}$ and
 a Lagrangian sub-manifold $M\subset\Gamma$ we find a generalization of the
 notion of curvature. The particular case
 $H=\frac{1}{2}g^{ij}\left[p_{i}-A_{i}\right]\left[p_{j}-A_{j}\right]+\phi $ of
 a particle moving in a gravitational, electromagnetic and scalar fields is
 studied in more detail. The integral of the generalized Ricci tensor w.r.t. the
 Boltzmann weight reduces to the action principle
 $\int\left[R+\frac{1}{4}F_{ik}F_{jl}g^{kl}g^{ij}-g^{ij}\partial_{i}\phi\partial_{j}\phi\right]e^{-\phi}\sqrt{g}d^{n}q$
 for the scalar, vector and tensor fields.",0,0,1,0,0,0,
440,End-to-End Navigation in Unknown Environments using Neural Networks,"We investigate how a neural network can learn perception actions loops for
 navigation in unknown environments. Specifically, we consider how to learn to
 navigate in environments populated with cul-de-sacs that represent convex local
 minima that the robot could fall into instead of finding a set of feasible
 actions that take it to the goal. Traditional methods rely on maintaining a
 global map to solve the problem of over coming a long cul-de-sac. However, due
 to errors induced from local and global drift, it is highly challenging to
 maintain such a map for long periods of time. One way to mitigate this problem
 is by using learning techniques that do not rely on hand engineered map
 representations and instead output appropriate control policies directly from
 their sensory input. We first demonstrate that such a problem cannot be solved
 directly by deep reinforcement learning due to the sparse reward structure of
 the environment. Further, we demonstrate that deep supervised learning also
 cannot be used directly to solve this problem. We then investigate network
 models that offer a combination of reinforcement learning and supervised
 learning and highlight the significance of adding fully differentiable memory
 units to such networks. We evaluate our networks on their ability to generalize
 to new environments and show that adding memory to such networks offers huge
 jumps in performance",1,0,0,0,0,0,
441,A Combinatorial Approach to the Opposite Bi-Free Partial $S$-Transform,"In this paper, we present a combinatorial approach to the opposite 2-variable
 bi-free partial $S$-transforms where the opposite multiplication is used on the
 right. In addition, extensions of this partial $S$-transforms to the
 conditional bi-free and operator-valued bi-free settings are discussed.",0,0,1,0,0,0,
442,Roche-lobe overflow in eccentric planet-star systems,"Many giant exoplanets are found near their Roche limit and in mildly
 eccentric orbits. In this study we examine the fate of such planets through
 Roche-lobe overflow as a function of the physical properties of the binary
 components, including the eccentricity and the asynchronicity of the rotating
 planet. We use a direct three-body integrator to compute the trajectories of
 the lost mass in the ballistic limit and investigate the possible outcomes. We
 find three different outcomes for the mass transferred through the Lagrangian
 point $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the
 stellar surface, (iii) disk formation around the star. We explore the parameter
 space of the three different regimes and find that at low eccentricities,
 $e\lesssim 0.2$, mass overflow leads to disk formation for most systems, while
 for higher eccentricities or retrograde orbits self-accretion is the only
 possible outcome. We conclude that the assumption often made in previous work
 that when a planet overflows its Roche lobe it is quickly disrupted and
 accreted by the star is not always valid.",0,1,0,0,0,0,
443,A short-orbit spectrometer for low-energy pion detection in electroproduction experiments at MAMI,"A new Short-Orbit Spectrometer (SOS) has been constructed and installed
 within the experimental facility of the A1 collaboration at Mainz Microtron
 (MAMI), with the goal to detect low-energy pions. It is equipped with a
 Browne-Buechner magnet and a detector system consisting of two helium-ethane
 based drift chambers and a scintillator telescope made of five layers. The
 detector system allows detection of pions in the momentum range of 50 - 147
 MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can
 be placed at a distance range of 54 - 66 cm from the target center. Two
 collimators are available for the measurements, one having 1.8 msr aperture and
 the other having 7 msr aperture. The Short-Orbit Spectrometer has been
 successfully calibrated and used in coincidence measurements together with the
 standard magnetic spectrometers of the A1 collaboration.",0,1,0,0,0,0,
444,A one-dimensional model for water desalination by flow-through electrode capacitive deionization,"Capacitive deionization (CDI) is a fast-emerging water desalination
 technology in which a small cell voltage of ~1 V across porous carbon
 electrodes removes salt from feedwaters via electrosorption. In flow-through
 electrode (FTE) CDI cell architecture, feedwater is pumped through macropores
 or laser perforated channels in porous electrodes, enabling highly compact
 cells with parallel flow and electric field, as well as rapid salt removal. We
 here present a one-dimensional model describing water desalination by FTE CDI,
 and a comparison to data from a custom-built experimental cell. The model
 employs simple cell boundary conditions derived via scaling arguments. We show
 good model-to-data fits with reasonable values for fitting parameters such as
 the Stern layer capacitance, micropore volume, and attraction energy. Thus, we
 demonstrate that from an engineering modeling perspective, an FTE CDI cell may
 be described with simpler one-dimensional models, unlike more typical
 flow-between electrodes architecture where 2D models are required.",1,1,0,0,0,0,
445,Deep Learning Scooping Motion using Bilateral Teleoperations,"We present bilateral teleoperation system for task learning and robot motion
 generation. Our system includes a bilateral teleoperation platform and a deep
 learning software. The deep learning software refers to human demonstration
 using the bilateral teleoperation platform to collect visual images and robotic
 encoder values. It leverages the datasets of images and robotic encoder
 information to learn about the inter-modal correspondence between visual images
 and robot motion. In detail, the deep learning software uses a combination of
 Deep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent
 Neural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor
 angles, to learn motion taught be human teleoperation. The learnt models are
 used to predict new motion trajectories for similar tasks. Experimental results
 show that our system has the adaptivity to generate motion for similar scooping
 tasks. Detailed analysis is performed based on failure cases of the
 experimental results. Some insights about the cans and cannots of the system
 are summarized.",1,0,0,0,0,0,
446,XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification,"We propose two multimodal deep learning architectures that allow for
 cross-modal dataflow (XFlow) between the feature extractors, thereby extracting
 more interpretable features and obtaining a better representation than through
 unimodal learning, for the same amount of training data. These models can
 usefully exploit correlations between audio and visual data, which have a
 different dimensionality and are therefore nontrivially exchangeable. Our work
 improves on existing multimodal deep learning metholodogies in two essential
 ways: (1) it presents a novel method for performing cross-modality (before
 features are learned from individual modalities) and (2) extends the previously
 proposed cross-connections, which only transfer information between streams
 that process compatible data. Both cross-modal architectures outperformed their
 baselines (by up to 7.5%) when evaluated on the AVletters dataset.",1,0,0,1,0,0,
447,Making Sense of Physics through Stories: High School Students Narratives about Electric Charges and Interactions,"Educational research has shown that narratives are useful tools that can help
 young students make sense of scientific phenomena. Based on previous research,
 I argue that narratives can also become tools for high school students to make
 sense of concepts such as the electric field. In this paper I examine high
 school students visual and oral narratives in which they describe the
 interaction among electric charges as if they were characters of a cartoon
 series. The study investigates: given the prompt to produce narratives for
 electrostatic phenomena during a classroom activity prior to receiving formal
 instruction, (1) what ideas of electrostatics do students attend to in their
 narratives?; (2) what role do students narratives play in their understanding
 of electrostatics? The participants were a group of high school students
 engaged in an open-ended classroom activity prior to receiving formal
 instruction about electrostatics. During the activity, the group was asked to
 draw comic strips for electric charges. In addition to individual work,
 students shared their work within small groups as well as with the whole group.
 Post activity, six students from a small group were interviewed individually
 about their work. In this paper I present two cases in which students produced
 narratives to express their ideas about electrostatics in different ways. In
 each case, I present student work for the comic strip activity (visual
 narratives), their oral descriptions of their work (oral narratives) during the
 interview and/or to their peers during class, and the their ideas of the
 electric interactions expressed through their narratives.",0,1,0,0,0,0,
448,Preventing Hospital Acquired Infections Through a Workflow-Based Cyber-Physical System,"Hospital acquired infections (HAI) are infections acquired within the
 hospital from healthcare workers, patients or from the environment, but which
 have no connection to the initial reason for the patient's hospital admission.
 HAI are a serious world-wide problem, leading to an increase in mortality
 rates, duration of hospitalisation as well as significant economic burden on
 hospitals. Although clear preventive guidelines exist, studies show that
 compliance to them is frequently poor. This paper details the software
 perspective for an innovative, business process software based cyber-physical
 system that will be implemented as part of a European Union-funded research
 project. The system is composed of a network of sensors mounted in different
 sites around the hospital, a series of wearables used by the healthcare workers
 and a server side workflow engine. For better understanding, we describe the
 system through the lens of a single, simple clinical workflow that is
 responsible for a significant portion of all hospital infections. The goal is
 that when completed, the system will be configurable in the sense of
 facilitating the creation and automated monitoring of those clinical workflows
 that when combined, account for over 90\% of hospital infections.",1,0,0,0,0,0,
449,OpenML Benchmarking Suites and the OpenML100,"We advocate the use of curated, comprehensive benchmark suites of machine
 learning datasets, backed by standardized OpenML-based interfaces and
 complementary software toolkits written in Python, Java and R. Major
 distinguishing features of OpenML benchmark suites are (a) ease of use through
 standardized data formats, APIs, and existing client libraries; (b)
 machine-readable meta-information regarding the contents of the suite; and (c)
 online sharing of results, enabling large scale comparisons. As a first such
 suite, we propose the OpenML100, a machine learning benchmark suite of
 100~classification datasets carefully curated from the thousands of datasets
 available on OpenML.org.",1,0,0,1,0,0,
450,On orbifold constructions associated with the Leech lattice vertex operator algebra,"In this article, we study orbifold constructions associated with the Leech
 lattice vertex operator algebra. As an application, we prove that the structure
 of a strongly regular holomorphic vertex operator algebra of central charge
 $24$ is uniquely determined by its weight one Lie algebra if the Lie algebra
 has the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,
 $A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse
 orbifold construction. Our result also provides alternative constructions of
 these vertex operator algebras (except for the case $A_{6,7}$) from the Leech
 lattice vertex operator algebra.",0,0,1,0,0,0,
451,From mindless mathematics to thinking meat?,"Deconstruction of the theme of the 2017 FQXi essay contest is already an
 interesting exercise in its own right: Teleology is rarely useful in physics
 --- the only known mainstream physics example (black hole event horizons) has a
 very mixed score-card --- so the ""goals"" and ""aims and intentions"" alluded to
 in the theme of the 2017 FQXi essay contest are already somewhat pushing the
 limits. Furthermore, ""aims and intentions"" certainly carries the implication of
 consciousness, and opens up a whole can of worms related to the mind-body
 problem. As for ""mindless mathematical laws"", that allusion is certainly in
 tension with at least some versions of the ""mathematical universe hypothesis"".
 Finally ""wandering towards a goal"" again carries the implication of
 consciousness, with all its attendant problems.
 In this essay I will argue, simply because we do not yet have any really good
 mathematical or physical theory of consciousness, that the theme of this essay
 contest is premature, and unlikely to lead to any resolution that would be
 widely accepted in the mathematics or physics communities.",0,1,0,0,0,0,
452,Phase correction for ALMA - Investigating water vapour radiometer scaling:The long-baseline science verification data case study,"The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water
 vapour radiometers (WVR), which monitor the atmospheric water vapour line at
 183 GHz along the line of sight above each antenna to correct for phase delays
 introduced by the wet component of the troposphere. The application of WVR
 derived phase corrections improve the image quality and facilitate successful
 observations in weather conditions that were classically marginal or poor. We
 present work to indicate that a scaling factor applied to the WVR solutions can
 act to further improve the phase stability and image quality of ALMA data. We
 find reduced phase noise statistics for 62 out of 75 datasets from the
 long-baseline science verification campaign after a WVR scaling factor is
 applied. The improvement of phase noise translates to an expected coherence
 improvement in 39 datasets. When imaging the bandpass source, we find 33 of the
 39 datasets show an improvement in the signal-to-noise ratio (S/N) between a
 few to ~30 percent. There are 23 datasets where the S/N of the science image is
 improved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies
 studied (band 6 and band 7) are those most improved, specifically datasets with
 low precipitable water vapour (PWV), <1mm, where the dominance of the wet
 component is reduced. Although these improvements are not profound, phase
 stability improvements via the WVR scaling factor come into play for the higher
 frequency (>450 GHz) and long-baseline (>5km) observations. These inherently
 have poorer phase stability and are taken in low PWV (<1mm) conditions for
 which we find the scaling to be most effective. A promising explanation for the
 scaling factor is the mixing of dry and wet air components, although other
 origins are discussed. We have produced a python code to allow ALMA users to
 undertake WVR scaling tests and make improvements to their data.",0,1,0,0,0,0,
453,On the $?¬$-ordinary locus of a Shimura variety,"In this paper, we study the $\mu$-ordinary locus of a Shimura variety with
 parahoric level structure. Under the axioms in \cite{HR}, we show that
 $\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport
 strata introduced in \cite{HR} and we give criteria on the density of the
 $\mu$-ordinary locus.",0,0,1,0,0,0,
454,Scatteract: Automated extraction of data from scatter plots,"Charts are an excellent way to convey patterns and trends in data, but they
 do not facilitate further modeling of the data or close inspection of
 individual data points. We present a fully automated system for extracting the
 numerical values of data points from images of scatter plots. We use deep
 learning techniques to identify the key components of the chart, and optical
 character recognition together with robust regression to map from pixels to the
 coordinate system of the chart. We focus on scatter plots with linear scales,
 which already have several interesting challenges. Previous work has done fully
 automatic extraction for other types of charts, but to our knowledge this is
 the first approach that is fully automatic for scatter plots. Our method
 performs well, achieving successful data extraction on 89% of the plots in our
 test set.",1,0,0,1,0,0,
455,When the Universe Expands Too Fast: Relentless Dark Matter,"We consider a modification to the standard cosmological history consisting of
 introducing a new species $\phi$ whose energy density red-shifts with the scale
 factor $a$ like $\rho_\phi \propto a^{-(4+n)}$. For $n>0$, such a red-shift is
 faster than radiation, hence the new species dominates the energy budget of the
 universe at early times while it is completely negligible at late times. If
 equality with the radiation energy density is achieved at low enough
 temperatures, dark matter can be produced as a thermal relic during the new
 cosmological phase. Dark matter freeze-out then occurs at higher temperatures
 compared to the standard case, implying that reproducing the observed abundance
 requires significantly larger annihilation rates. Here, we point out a
 completely new phenomenon, which we refer to as $\textit{relentless}$ dark
 matter: for large enough $n$, unlike the standard case where annihilation ends
 shortly after the departure from thermal equilibrium, dark matter particles
 keep annihilating long after leaving chemical equilibrium, with a significant
 depletion of the final relic abundance. Relentless annihilation occurs for $n
 \geq 2$ and $n \geq 4$ for s-wave and p-wave annihilation, respectively, and it
 thus occurs in well motivated scenarios such as a quintessence with a kination
 phase. We discuss a few microscopic realizations for the new cosmological
 component and highlight the phenomenological consequences of our calculations
 for dark matter searches.",0,1,0,0,0,0,
456,Polygons with prescribed edge slopes: configuration space and extremal points of perimeter,"We describe the configuration space $\mathbf{S}$ of polygons with prescribed
 edge slopes, and study the perimeter $\mathcal{P}$ as a Morse function on
 $\mathbf{S}$. We characterize critical points of $\mathcal{P}$ (these are
 \textit{tangential} polygons) and compute their Morse indices. This setup is
 motivated by a number of results about critical points and Morse indices of the
 oriented area function defined on the configuration space of polygons with
 prescribed edge lengths (flexible polygons). As a by-product, we present an
 independent computation of the Morse index of the area function (obtained
 earlier by G. Panina and A. Zhukova).",0,0,1,0,0,0,
457,An Asymptotically Efficient Metropolis-Hastings Sampler for Bayesian Inference in Large-Scale Educational Measuremen,"This paper discusses a Metropolis-Hastings algorithm developed by
 \citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is
 proven that the algorithm becomes more efficient with more data and meets the
 growing demands of large scale educational measurement.",0,0,0,1,0,0,
458,When Streams of Optofluidics Meet the Sea of Life,"Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National
 University of Singapore. In this contribution he describes the power of
 optofluidics as a research tool and reviews new insights within the areas of
 single cell analysis, microphysiological analysis, and integrated systems.",0,0,0,0,1,0,
459,"L lines, C points and Chern numbers: understanding band structure topology using polarization fields","Topology has appeared in different physical contexts. The most prominent
 application is topologically protected edge transport in condensed matter
 physics. The Chern number, the topological invariant of gapped Bloch
 Hamiltonians, is an important quantity in this field. Another example of
 topology, in polarization physics, are polarization singularities, called L
 lines and C points. By establishing a connection between these two theories, we
 develop a novel technique to visualize and potentially measure the Chern
 number: it can be expressed either as the winding of the polarization azimuth
 along L lines in reciprocal space, or in terms of the handedness and the index
 of the C points. For mechanical systems, this is directly connected to the
 visible motion patterns.",0,1,0,0,0,0,
460,Willis Theory via Graphs,"We study the scale and tidy subgroups of an endomorphism of a totally
 disconnected locally compact group using a geometric framework. This leads to
 new interpretations of tidy subgroups and the scale function. Foremost, we
 obtain a geometric tidying procedure which applies to endomorphisms as well as
 a geometric proof of the fact that tidiness is equivalent to being minimizing
 for a given endomorphism. Our framework also yields an endomorphism version of
 the Baumgartner-Willis tree representation theorem. We conclude with a
 construction of new endomorphisms of totally disconnected locally compact
 groups from old via HNN-extensions.",0,0,1,0,0,0,
461,The Effect of Site-Specific Spectral Densities on the High-Dimensional Exciton-Vibrational Dynamics in the FMO Complex,"The coupled exciton-vibrational dynamics of a three-site model of the FMO
 complex is investigated using the Multi-layer Multi-configuration
 Time-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of
 the spectral density on the exciton state populations as well as on the
 vibrational and vibronic non-equilibrium excitations. Models which use either a
 single or site-specific spectral densities are contrasted to a spectral density
 adapted from experiment. For the transfer efficiency, the total integrated
 Huang-Rhys factor is found to be more important than details of the spectral
 distributions. However, the latter are relevant for the obtained
 non-equilibrium vibrational and vibronic distributions and thus influence the
 actual pattern of population relaxation.",0,1,0,0,0,0,
462,Space dependent adhesion forces mediated by transient elastic linkages : new convergence and global existence results,"In the first part of this work we show the convergence with respect to an
 asymptotic parameter {\epsilon} of a delayed heat equation. It represents a
 mathematical extension of works considered previously by the authors [Milisic
 et al. 2011, Milisic et al. 2016]. Namely, this is the first result involving
 delay operators approximating protein linkages coupled with a spatial elliptic
 second order operator. For the sake of simplicity we choose the Laplace
 operator, although more general results could be derived. The main arguments
 are (i) new energy estimates and (ii) a stability result extended from the
 previous work to this more involved context. They allow to prove convergence of
 the delay operator to a friction term together with the Laplace operator in the
 same asymptotic regime considered without the space dependence in [Milisic et
 al, 2011]. In a second part we extend fixed-point results for the fully
 non-linear model introduced in [Milisic et al, 2016] and prove global existence
 in time. This shows that the blow-up scenario observed previously does not
 occur. Since the latter result was interpreted as a rupture of adhesion forces,
 we discuss the possibility of bond breaking both from the analytic and
 numerical point of view.",0,0,1,0,0,0,
463,On the nonparametric maximum likelihood estimator for Gaussian location mixture densities with application to Gaussian denoising,"We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for
 estimating Gaussian location mixture densities in $d$-dimensions from
 independent observations. Unlike usual likelihood-based methods for fitting
 mixtures, NPMLEs are based on convex optimization. We prove finite sample
 results on the Hellinger accuracy of every NPMLE. Our results imply, in
 particular, that every NPMLE achieves near parametric risk (up to logarithmic
 multiplicative factors) when the true density is a discrete Gaussian mixture
 without any prior information on the number of mixture components. NPMLEs can
 naturally be used to yield empirical Bayes estimates of the Oracle Bayes
 estimator in the Gaussian denoising problem. We prove bounds for the accuracy
 of the empirical Bayes estimate as an approximation to the Oracle Bayes
 estimator. Here our results imply that the empirical Bayes estimator performs
 at nearly the optimal level (up to logarithmic multiplicative factors) for
 denoising in clustering situations without any prior knowledge of the number of
 clusters.",0,0,1,1,0,0,
464,Gravitational wave production from preheating: parameter dependence,"Parametric resonance is among the most efficient phenomena generating
 gravitational waves (GWs) in the early Universe. The dynamics of parametric
 resonance, and hence of the GWs, depend exclusively on the resonance parameter
 $q$. The latter is determined by the properties of each scenario: the initial
 amplitude and potential curvature of the oscillating field, and its coupling to
 other species. Previous works have only studied the GW production for fixed
 value(s) of $q$. We present an analytical derivation of the GW amplitude
 dependence on $q$, valid for any scenario, which we confront against numerical
 results. By running lattice simulations in an expanding grid, we study for a
 wide range of $q$ values, the production of GWs in post-inflationary preheating
 scenarios driven by parametric resonance. We present simple fits for the final
 amplitude and position of the local maxima in the GW spectrum. Our
 parametrization allows to predict the location and amplitude of the GW
 background today, for an arbitrary $q$. The GW signal can be rather large, as
 $h^2\Omega_{\rm GW}(f_p) \lesssim 10^{-11}$, but it is always peaked at high
 frequencies $f_p \gtrsim 10^{7}$ Hz. We also discuss the case of
 spectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,
 or the Standard Model Higgs.",0,1,0,0,0,0,
465,IP determination and 1+1 REMPI spectrum of SiO at 210-220 nm with implications for SiO$^{+}$ ion trap loading,"The 1+1 REMPI spectrum of SiO in the 210-220 nm range is recorded. Observed
 bands are assigned to the $A-X$ vibrational bands $(v``=0-3, v`=5-10)$ and a
 tentative assignment is given to the 2-photon transition from $X$ to the
 n=12-13 $[X^{2}{\Sigma}^{+},v^{+}=1]$ Rydberg states at 216-217 nm. We estimate
 the IP of SiO to be 11.59(1) eV. The SiO$^{+}$ cation has previously been
 identified as a molecular candidate amenable to laser control. Our work allows
 us to identify an efficient method for loading cold SiO$^{+}$ from an ablated
 sample of SiO into an ion trap via the $(5,0)$ $A-X$ band at 213.977 nm.",0,1,0,0,0,0,
466,Adaptive Model Predictive Control for High-Accuracy Trajectory Tracking in Changing Conditions,"Robots and automated systems are increasingly being introduced to unknown and
 dynamic environments where they are required to handle disturbances, unmodeled
 dynamics, and parametric uncertainties. Robust and adaptive control strategies
 are required to achieve high performance in these dynamic environments. In this
 paper, we propose a novel adaptive model predictive controller that combines
 model predictive control (MPC) with an underlying $\mathcal{L}_1$ adaptive
 controller to improve trajectory tracking of a system subject to unknown and
 changing disturbances. The $\mathcal{L}_1$ adaptive controller forces the
 system to behave in a predefined way, as specified by a reference model. A
 higher-level model predictive controller then uses this reference model to
 calculate the optimal reference input based on a cost function, while taking
 into account input and state constraints. We focus on the experimental
 validation of the proposed approach and demonstrate its effectiveness in
 experiments on a quadrotor. We show that the proposed approach has a lower
 trajectory tracking error compared to non-predictive, adaptive approaches and a
 predictive, non-adaptive approach, even when external wind disturbances are
 applied.",1,0,0,0,0,0,
467,An enhanced method to compute the similarity between concepts of ontology,"With the use of ontologies in several domains such as semantic web,
 information retrieval, artificial intelligence, the concept of similarity
 measuring has become a very important domain of research. Therefore, in the
 current paper, we propose our method of similarity measuring which uses the
 Dijkstra algorithm to define and compute the shortest path. Then, we use this
 one to compute the semantic distance between two concepts defined in the same
 hierarchy of ontology. Afterward, we base on this result to compute the
 semantic similarity. Finally, we present an experimental comparison between our
 method and other methods of similarity measuring.",1,0,0,0,0,0,
468,Eigenvalues of symmetric tridiagonal interval matrices revisited,"In this short note, we present a novel method for computing exact lower and
 upper bounds of eigenvalues of a symmetric tridiagonal interval matrix.
 Compared to the known methods, our approach is fast, simple to present and to
 implement, and avoids any assumptions. Our construction explicitly yields those
 matrices for which particular lower and upper bounds are attained.",1,0,0,0,0,0,
469,PRE-render Content Using Tiles (PRECUT). 1. Large-Scale Compound-Target Relationship Analyses,"Visualizing a complex network is computationally intensive process and
 depends heavily on the number of components in the network. One way to solve
 this problem is not to render the network in real time. PRE-render Content
 Using Tiles (PRECUT) is a process to convert any complex network into a
 pre-rendered network. Tiles are generated from pre-rendered images at different
 zoom levels, and navigating the network simply becomes delivering relevant
 tiles. PRECUT is exemplified by performing large-scale compound-target
 relationship analyses. Matched molecular pair (MMP) networks were created using
 compounds and the target class description found in the ChEMBL database. To
 visualize MMP networks, the MMP network viewer has been implemented in COMBINE
 and as a web application, hosted at this http URL.",1,0,0,0,0,0,
470,The list chromatic number of graphs with small clique number,"We prove that every triangle-free graph with maximum degree $\Delta$ has list
 chromatic number at most $(1+o(1))\frac{\Delta}{\ln \Delta}$. This matches the
 best-known bound for graphs of girth at least 5. We also provide a new proof
 that for any $r\geq 4$ every $K_r$-free graph has list-chromatic number at most
 $200r\frac{\Delta\ln\ln\Delta}{\ln\Delta}$.",0,0,1,0,0,0,
471,Topology of Large-Scale Structures of Galaxies in Two Dimensions - Systematic Effects,"We study the two-dimensional topology of the galactic distribution when
 projected onto two-dimensional spherical shells. Using the latest Horizon Run 4
 simulation data, we construct the genus of the two-dimensional field and
 consider how this statistic is affected by late-time nonlinear effects --
 principally gravitational collapse and redshift space distortion (RSD). We also
 consider systematic and numerical artifacts such as shot noise, galaxy bias,
 and finite pixel effects. We model the systematics using a Hermite polynomial
 expansion and perform a comprehensive analysis of known effects on the
 two-dimensional genus, with a view toward using the statistic for cosmological
 parameter estimation. We find that the finite pixel effect is dominated by an
 amplitude drop and can be made less than $1\%$ by adopting pixels smaller than
 $1/3$ of the angular smoothing length. Nonlinear gravitational evolution
 introduces time-dependent coefficients of the zeroth, first, and second Hermite
 polynomials, but the genus amplitude changes by less than $1\%$ between $z=1$
 and $z=0$ for smoothing scales $R_{\rm G} > 9 {\rm Mpc/h}$. Non-zero terms are
 measured up to third order in the Hermite polynomial expansion when studying
 RSD. Differences in shapes of the genus curves in real and redshift space are
 small when we adopt thick redshift shells, but the amplitude change remains a
 significant $\sim {\cal O}(10\%)$ effect. The combined effects of galaxy
 biasing and shot noise produce systematic effects up to the second Hermite
 polynomial. It is shown that, when sampling, the use of galaxy mass cuts
 significantly reduces the effect of shot noise relative to random sampling.",0,1,0,0,0,0,
472,Wiki-index of authors popularity,"The new index of the author's popularity estimation is represented in the
 paper. The index is calculated on the basis of Wikipedia encyclopedia analysis
 (Wikipedia Index - WI). Unlike the conventional existed citation indices, the
 suggested mark allows to evaluate not only the popularity of the author, as it
 can be done by means of calculating the general citation number or by the
 Hirsch index, which is often used to measure the author's research rate. The
 index gives an opportunity to estimate the author's popularity, his/her
 influence within the sought-after area ""knowledge area"" in the Internet - in
 the Wikipedia. The suggested index is supposed to be calculated in frames of
 the subject domain, and it, on the one hand, avoids the mistaken computation of
 the homonyms, and on the other hand - provides the entirety of the subject
 area. There are proposed algorithms and the technique of the Wikipedia Index
 calculation through the network encyclopedia sounding, the exemplified
 calculations of the index for the prominent researchers, and also the methods
 of the information networks formation - models of the subject domains by the
 automatic monitoring and networks information reference resources analysis. The
 considered in the paper notion network corresponds the terms-heads of the
 Wikipedia articles.",1,0,0,0,0,0,
473,Belyi map for the sporadic group J1,"We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree
 266 and describe the applied method. This yields explicit polynomials having J1
 as a Galois group over K(t), [K:Q] = 7.",0,0,1,0,0,0,
474,"Convolution Semigroups of Probability Measures on Gelfand Pairs, Revisited","Our goal is to find classes of convolution semigroups on Lie groups $G$ that
 give rise to interesting processes in symmetric spaces $G/K$. The
 $K$-bi-invariant convolution semigroups are a well-studied example. An
 appealing direction for the next step is to generalise to right $K$-invariant
 convolution semigroups, but recent work of Liao has shown that these are in
 one-to-one correspondence with $K$-bi-invariant convolution semigroups. We
 investigate a weaker notion of right $K$-invariance, but show that this is, in
 fact, the same as the usual notion. Another possible approach is to use
 generalised notions of negative definite functions, but this also leads to
 nothing new. We finally find an interesting class of convolution semigroups
 that are obtained by making use of the Cartan decomposition of a semisimple Lie
 group, and the solution of certain stochastic differential equations. Examples
 suggest that these are well-suited for generating random motion along geodesics
 in symmetric spaces.",0,0,1,0,0,0,
475,Toward Optimal Coupon Allocation in Social Networks: An Approximate Submodular Optimization Approach,"CMO Council reports that 71\% of internet users in the U.S. were influenced
 by coupons and discounts when making their purchase decisions. It has also been
 shown that offering coupons to a small fraction of users (called seed users)
 may affect the purchase decisions of many other users in a social network. This
 motivates us to study the optimal coupon allocation problem, and our objective
 is to allocate coupons to a set of users so as to maximize the expected
 cascade. Different from existing studies on influence maximizaton (IM), our
 framework allows a general utility function and a more complex set of
 constraints. In particular, we formulate our problem as an approximate
 submodular maximization problem subject to matroid and knapsack constraints.
 Existing techniques relying on the submodularity of the utility function, such
 as greedy algorithm, can not work directly on a non-submodular function. We use
 $\epsilon$ to measure the difference between our function and its closest
 submodular function and propose a novel approximate algorithm with
 approximation ratio $\beta(\epsilon)$ with $\lim_{\epsilon\rightarrow
 0}\beta(\epsilon)=1-1/e$. This is the best approximation guarantee for
 approximate submodular maximization subject to a partition matroid and knapsack
 constraints, our results apply to a broad range of optimization problems that
 can be formulated as an approximate submodular maximization problem.",1,0,0,0,0,0,
476,Lefschetz duality for intersection (co)homology,"We prove the Lefschetz duality for intersection (co)homology in the framework
 of $\partial$-pesudomanifolds. We work with general perversities and without
 restriction on the coefficient ring.",0,0,1,0,0,0,
477,Empirical determination of the optimum attack for fragmentation of modular networks,"All possible removals of $n=5$ nodes from networks of size $N=100$ are
 performed in order to find the optimal set of nodes which fragments the
 original network into the smallest largest connected component. The resulting
 attacks are ordered according to the size of the largest connected component
 and compared with the state of the art methods of network attacks. We chose
 attacks of size $5$ on relative small networks of size $100$ because the number
 of all possible attacks, ${100}\choose{5}$ $\approx 10^8$, is at the verge of
 the possible to compute with the available standard computers. Besides, we
 applied the procedure in a series of networks with controlled and varied
 modularity, comparing the resulting statistics with the effect of removing the
 same amount of vertices, according to the known most efficient disruption
 strategies, i.e., High Betweenness Adaptive attack (HBA), Collective Index
 attack (CI), and Modular Based Attack (MBA). Results show that modularity has
 an inverse relation with robustness, with $Q_c \approx 0.7$ being the critical
 value. For modularities lower than $Q_c$, all heuristic method gives mostly the
 same results than with random attacks, while for bigger $Q$, networks are less
 robust and highly vulnerable to malicious attacks.",1,0,0,0,0,0,
478,Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence,"In this paper, we study stochastic non-convex optimization with non-convex
 random functions. Recent studies on non-convex optimization revolve around
 establishing second-order convergence, i.e., converging to a nearly
 second-order optimal stationary points. However, existing results on stochastic
 non-convex optimization are limited, especially with a high probability
 second-order convergence. We propose a novel updating step (named NCG-S) by
 leveraging a stochastic gradient and a noisy negative curvature of a stochastic
 Hessian, where the stochastic gradient and Hessian are based on a proper
 mini-batch of random functions. Building on this step, we develop two
 algorithms and establish their high probability second-order convergence. To
 the best of our knowledge, the proposed stochastic algorithms are the first
 with a second-order convergence in {\it high probability} and a time complexity
 that is {\it almost linear} in the problem's dimensionality.",1,0,0,1,0,0,
479,On the optimality and sharpness of Laguerre's lower bound on the smallest eigenvalue of a symmetric positive definite matrix,"Lower bounds on the smallest eigenvalue of a symmetric positive definite
 matrices $A\in\mathbb{R}^{m\times m}$ play an important role in condition
 number estimation and in iterative methods for singular value computation. In
 particular, the bounds based on ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$
 attract attention recently because they can be computed in $O(m)$ work when $A$
 is tridiagonal. In this paper, we focus on these bounds and investigate their
 properties in detail. First, we consider the problem of finding the optimal
 bound that can be computed solely from ${\rm Tr}(A^{-1})$ and ${\rm
 Tr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one
 in terms of sharpness. Next, we study the gap between the Laguerre bound and
 the smallest eigenvalue. We characterize the situation in which the gap becomes
 largest in terms of the eigenvalue distribution of $A$ and show that the gap
 becomes smallest when ${\rm Tr}(A^{-2})/\{{\rm Tr}(A^{-1})\}^2$ approaches 1 or
 $\frac{1}{m}$. These results will be useful, for example, in designing
 efficient shift strategies for singular value computation algorithms.",1,0,1,0,0,0,
480,Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques,"This paper describes the development of a magnetic attitude control subsystem
 for a 2U cubesat. Due to the presence of gravity gradient torques, the
 satellite dynamics are open-loop unstable near the desired pointing
 configuration. Nevertheless the linearized time-varying system is completely
 controllable, under easily verifiable conditions, and the system's disturbance
 rejection capabilities can be enhanced by adding air drag panels exemplifying a
 beneficial interplay between hardware design and control. In the paper,
 conditions for the complete controllability for the case of a magnetically
 controlled satellite with passive air drag panels are developed, and simulation
 case studies with the LQR and MPC control designs applied in combination with a
 nonlinear time-varying input transformation are presented to demonstrate the
 ability of the closed-loop system to satisfy mission objectives despite
 disturbance torques.",0,0,1,0,0,0,
481,"Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem","One advantage of decision tree based methods like random forests is their
 ability to natively handle categorical predictors without having to first
 transform them (e.g., by using feature engineering techniques). However, in
 this paper, we show how this capability can lead to an inherent ""absent levels""
 problem for decision tree based methods that has never been thoroughly
 discussed, and whose consequences have never been carefully explored. This
 problem occurs whenever there is an indeterminacy over how to handle an
 observation that has reached a categorical split which was determined when the
 observation in question's level was absent during training. Although these
 incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's
 random forests FORTRAN code and the randomForest R package (Liaw and Wiener,
 2002) as motivating case studies, we examine how overlooking the absent levels
 problem can systematically bias a model. Furthermore, by using three real data
 examples, we illustrate how absent levels can dramatically alter a model's
 performance in practice, and we empirically demonstrate how some simple
 heuristics can be used to help mitigate the effects of the absent levels
 problem until a more robust theoretical solution is found.",1,0,0,1,0,0,
482,Temporal correlation detection using computational phase-change memory,"For decades, conventional computers based on the von Neumann architecture
 have performed computation by repeatedly transferring data between their
 processing and their memory units, which are physically separated. As
 computation becomes increasingly data-centric and as the scalability limits in
 terms of performance and power are being reached, alternative computing
 paradigms are searched for in which computation and storage are collocated. A
 fascinating new approach is that of computational memory where the physics of
 nanoscale memory devices are used to perform certain computational tasks within
 the memory unit in a non-von Neumann manner. Here we present a large-scale
 experimental demonstration using one million phase-change memory devices
 organized to perform a high-level computational primitive by exploiting the
 crystallization dynamics. Also presented is an application of such a
 computational memory to process real-world data-sets. The results show that
 this co-existence of computation and storage at the nanometer scale could be
 the enabler for new, ultra-dense, low power, and massively parallel computing
 systems.",1,0,0,0,0,0,
483,Complexity and capacity bounds for quantum channels,"We generalise some well-known graph parameters to operator systems by
 considering their underlying quantum channels. In particular, we introduce the
 quantum complexity as the dimension of the smallest co-domain Hilbert space a
 quantum channel requires to realise a given operator system as its
 non-commutative confusability graph. We describe quantum complexity as a
 generalised minimum semidefinite rank and, in the case of a graph operator
 system, as a quantum intersection number. The quantum complexity and a closely
 related quantum version of orthogonal rank turn out to be upper bounds for the
 Shannon zero-error capacity of a quantum channel, and we construct examples for
 which these bounds beat the best previously known general upper bound for the
 capacity of quantum channels, given by the quantum Lov?­sz theta number.",0,0,1,0,0,0,
484,Quantum Interference of Glory Rescattering in Strong-Field Atomic Ionization,"During the ionization of atoms irradiated by linearly polarized intense laser
 fields, we find for the first time that the transverse momentum distribution of
 photoelectrons can be well fitted by a squared zeroth-order Bessel function
 because of the quantum interference effect of Glory rescattering. The
 characteristic of the Bessel function is determined by the common angular
 momentum of a bunch of semiclassical paths termed as Glory trajectories, which
 are launched with different nonzero initial transverse momenta distributed on a
 specific circle in the momentum plane and finally deflected to the same
 asymptotic momentum, which is along the polarization direction, through
 post-tunneling rescattering. Glory rescattering theory (GRT) based on the
 semiclassical path-integral formalism is developed to address this effect
 quantitatively. Our theory can resolve the long-standing discrepancies between
 existing theories and experiments on the fringe location, predict the sudden
 transition of the fringe structure in holographic patterns, and shed light on
 the quantum interference aspects of low-energy structures in strong-field
 atomic ionization.",0,1,0,0,0,0,
485,On vector measures and extensions of transfunctions,"We are interested in extending operators defined on positive measures, called
 here transfunctions, to signed measures and vector measures. Our methods use a
 somewhat nonstandard approach to measures and vector measures. The necessary
 background, including proofs of some auxiliary results, is included.",0,0,1,0,0,0,
486,Deep Within-Class Covariance Analysis for Robust Audio Representation Learning,"Convolutional Neural Networks (CNNs) can learn effective features, though
 have been shown to suffer from a performance drop when the distribution of the
 data changes from training to test data. In this paper we analyze the internal
 representations of CNNs and observe that the representations of unseen data in
 each class, spread more (with higher variance) in the embedding space of the
 CNN compared to representations of the training data. More importantly, this
 difference is more extreme if the unseen data comes from a shifted
 distribution. Based on this observation, we objectively evaluate the degree of
 representation's variance in each class via eigenvalue decomposition on the
 within-class covariance of the internal representations of CNNs and observe the
 same behaviour. This can be problematic as larger variances might lead to
 mis-classification if the sample crosses the decision boundary of its class. We
 apply nearest neighbor classification on the representations and empirically
 show that the embeddings with the high variance actually have significantly
 worse KNN classification performances, although this could not be foreseen from
 their end-to-end classification results. To tackle this problem, we propose
 Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that
 significantly reduces the within-class covariance of a DNN's representation,
 improving performance on unseen test data from a shifted distribution. We
 empirically evaluate DWCCA on two datasets for Acoustic Scene Classification
 (DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA
 significantly improve the network's internal representation, it also increases
 the end-to-end classification accuracy, especially when the test set exhibits a
 distribution shift. By adding DWCCA to a VGG network, we achieve around 6
 percentage points improvement in the case of a distribution mismatch.",1,0,0,0,0,0,
487,Efficient Online Bandit Multiclass Learning with $\tilde{O}(\sqrt{T})$ Regret,"We present an efficient second-order algorithm with
 $\tilde{O}(\frac{1}{\eta}\sqrt{T})$ regret for the bandit online multiclass
 problem. The regret bound holds simultaneously with respect to a family of loss
 functions parameterized by $\eta$, for a range of $\eta$ restricted by the norm
 of the competitor. The family of loss functions ranges from hinge loss
 ($\eta=0$) to squared hinge loss ($\eta=1$). This provides a solution to the
 open problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for
 $\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our
 algorithm experimentally, showing that it also performs favorably against
 earlier algorithms.",0,0,0,1,0,0,
488,Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning,"Swarm systems constitute a challenging problem for reinforcement learning
 (RL) as the algorithm needs to learn decentralized control policies that can
 cope with limited local sensing and communication abilities of the agents.
 While it is often difficult to directly define the behavior of the agents,
 simple communication protocols can be defined more easily using prior knowledge
 about the given task. In this paper, we propose a number of simple
 communication protocols that can be exploited by deep reinforcement learning to
 find decentralized control policies in a multi-robot swarm environment. The
 protocols are based on histograms that encode the local neighborhood relations
 of the agents and can also transmit task-specific information, such as the
 shortest distance and direction to a desired target. In our framework, we use
 an adaptation of Trust Region Policy Optimization to learn complex
 collaborative tasks, such as formation building and building a communication
 link. We evaluate our findings in a simulated 2D-physics environment, and
 compare the implications of different communication protocols.",1,0,0,1,0,0,
489,Towards exascale real-time RFI mitigation,"We describe the design and implementation of an extremely scalable real-time
 RFI mitigation method, based on the offline AOFlagger. All algorithms scale
 linearly in the number of samples. We describe how we implemented the flagger
 in the LOFAR real-time pipeline, on both CPUs and GPUs. Additionally, we
 introduce a novel simple history-based flagger that helps reduce the impact of
 our small window on the data.
 By examining an observation of a known pulsar, we demonstrate that our
 flagger can achieve much higher quality than a simple thresholder, even when
 running in real time, on a distributed system. The flagger works on visibility
 data, but also on raw voltages, and beam formed data. The algorithms are
 scale-invariant, and work on microsecond to second time scales. We are
 currently implementing a prototype for the time domain pipeline of the SKA
 central signal processor.",0,1,0,0,0,0,
490,Learning body-affordances to simplify action spaces,"Controlling embodied agents with many actuated degrees of freedom is a
 challenging task. We propose a method that can discover and interpolate between
 context dependent high-level actions or body-affordances. These provide an
 abstract, low-dimensional interface indexing high-dimensional and time-
 extended action policies. Our method is related to recent ap- proaches in the
 machine learning literature but is conceptually simpler and easier to
 implement. More specifically our method requires the choice of a n-dimensional
 target sensor space that is endowed with a distance metric. The method then
 learns an also n-dimensional embedding of possibly reactive body-affordances
 that spread as far as possible throughout the target sensor space.",1,0,0,0,0,0,
491,Cayley properties of the line graphs induced by of consecutive layers of the hypercube,"Let $n >3$ and $ 0< k < \frac{n}{2} $ be integers. In this paper, we
 investigate some algebraic properties of the line graph of the graph $
 {Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$
 which is induced by the set of vertices of weights $k$ and $k+1$. In the first
 step, we determine the automorphism groups of these graphs for all values of
 $k$. In the second step, we study Cayley properties of the line graph of these
 graphs. In particular, we show that for $ k>2, $ if $ 2k+1 \neq n$, then the
 line graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley
 graph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a
 Cayley graph if and only if $ n$ is a power of a prime $p$.",0,0,1,0,0,0,
492,Beyond the technical challenges for deploying Machine Learning solutions in a software company,"Recently software development companies started to embrace Machine Learning
 (ML) techniques for introducing a series of advanced functionality in their
 products such as personalisation of the user experience, improved search,
 content recommendation and automation. The technical challenges for tackling
 these problems are heavily researched in literature. A less studied area is a
 pragmatic approach to the role of humans in a complex modern industrial
 environment where ML based systems are developed. Key stakeholders affect the
 system from inception and up to operation and maintenance. Product managers
 want to embed ""smart"" experiences for their users and drive the decisions on
 what should be built next; software engineers are challenged to build or
 utilise ML software tools that require skills that are well outside of their
 comfort zone; legal and risk departments may influence design choices and data
 access; operations teams are requested to maintain ML systems which are
 non-stationary in their nature and change behaviour over time; and finally ML
 practitioners should communicate with all these stakeholders to successfully
 build a reliable system. This paper discusses some of the challenges we faced
 in Atlassian as we started investing more in the ML space.",1,0,0,1,0,0,
493,Class-Splitting Generative Adversarial Networks,"Generative Adversarial Networks (GANs) produce systematically better quality
 samples when class label information is provided., i.e. in the conditional GAN
 setup. This is still observed for the recently proposed Wasserstein GAN
 formulation which stabilized adversarial training and allows considering high
 capacity network architectures such as ResNet. In this work we show how to
 boost conditional GAN by augmenting available class labels. The new classes
 come from clustering in the representation space learned by the same GAN model.
 The proposed strategy is also feasible when no class information is available,
 i.e. in the unsupervised setup. Our generated samples reach state-of-the-art
 Inception scores for CIFAR-10 and STL-10 datasets in both supervised and
 unsupervised setup.",0,0,0,1,0,0,
494,Dynamical system analysis of dark energy models in scalar coupled metric-torsion theories,"We study the phase space dynamics of cosmological models in the theoretical
 formulations of non-minimal metric-torsion couplings with a scalar field, and
 investigate in particular the critical points which yield stable solutions
 exhibiting cosmic acceleration driven by the {\em dark energy}. The latter is
 defined in a way that it effectively has no direct interaction with the
 cosmological fluid, although in an equivalent scalar-tensor cosmological setup
 the scalar field interacts with the fluid (which we consider to be the
 pressureless dust). Determining the conditions for the existence of the stable
 critical points we check their physical viability, in both Einstein and Jordan
 frames. We also verify that in either of these frames, the evolution of the
 universe at the corresponding stable points matches with that given by the
 respective exact solutions we have found in an earlier work (arXiv: 1611.00654
 [gr-qc]). We not only examine the regions of physical relevance for the
 trajectories in the phase space when the coupling parameter is varied, but also
 demonstrate the evolution profiles of the cosmological parameters of interest
 along fiducial trajectories in the effectively non-interacting scenarios, in
 both Einstein and Jordan frames.",0,1,0,0,0,0,
495,J-MOD$^{2}$: Joint Monocular Obstacle Detection and Depth Estimation,"In this work, we propose an end-to-end deep architecture that jointly learns
 to detect obstacles and estimate their depth for MAV flight applications. Most
 of the existing approaches either rely on Visual SLAM systems or on depth
 estimation models to build 3D maps and detect obstacles. However, for the task
 of avoiding obstacles this level of complexity is not required. Recent works
 have proposed multi task architectures to both perform scene understanding and
 depth estimation. We follow their track and propose a specific architecture to
 jointly estimate depth and obstacles, without the need to compute a global map,
 but maintaining compatibility with a global SLAM system if needed. The network
 architecture is devised to exploit the joint information of the obstacle
 detection task, that produces more reliable bounding boxes, with the depth
 estimation one, increasing the robustness of both to scenario changes. We call
 this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
 experiments on sequences with different appearance and focal lengths and
 compare it to SotA multi task methods that jointly perform semantic
 segmentation and depth estimation. In addition, we show the integration in a
 full system using a set of simulated navigation experiments where a MAV
 explores an unknown scenario and plans safe trajectories by using our detection
 model.",1,0,0,0,0,0,
496,The Calabi flow with rough initial data,"In this paper, we prove that there exists a dimensional constant $\delta > 0$
 such that given any background K??hler metric $\omega$, the Calabi flow with
 initial data $u_0$ satisfying \begin{equation*} \partial \bar \partial u_0 \in
 L^\infty (M) \text{ and } (1- \delta )\omega < \omega_{u_0} < (1+\delta
 )\omega, \end{equation*} admits a unique short time solution and it becomes
 smooth immediately, where $\omega_{u_0} : = \omega +\sqrt{-1}\partial
 \bar\partial u_0$. The existence time depends on initial data $u_0$ and the
 metric $\omega$. As a corollary, we get that Calabi flow has short time
 existence for any initial data satisfying \begin{equation*} \partial \bar
 \partial u_0 \in C^0(M) \text{ and } \omega_{u_0} > 0, \end{equation*} which
 should be interpreted as a ""continuous K??hler metric"". A main technical
 ingredient is Schauder-type estimates for biharmonic heat equation on
 Riemannian manifolds with time weighted H??lder norms.",0,0,1,0,0,0,
497,Star Formation Activity in the molecular cloud G35.20$-$0.74: onset of cloud-cloud collision,"To probe the star-formation (SF) processes, we present results of an analysis
 of the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency
 observations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.
 An almost horseshoe-like structure embedded within the MCG35.2 is evident in
 the infrared and millimeter images and harbors the previously known sites,
 ultra-compact/hyper-compact G35.20$-$0.74N H\,{\sc ii} region, Ap2-1, and
 Mercer 14 at its base. The site, Ap2-1 is found to be excited by a radio
 spectral type of B0.5V star where the distribution of 20 cm and H$\alpha$
 emission is surrounded by the extended molecular hydrogen emission. Using the
 {\it Herschel} 160-500 $\mu$m and photometric 1-24 $\mu$m data analysis,
 several embedded clumps and clusters of young stellar objects (YSOs) are
 investigated within the MCG35.2, revealing the SF activities. Majority of the
 YSOs clusters and massive clumps (500-4250 M$_{\odot}$) are seen toward the
 horseshoe-like structure. The position-velocity analysis of $^{13}$CO emission
 shows a blue-shifted peak (at 33 km s$^{-1}$) and a red-shifted peak (at 37 km
 s$^{-1}$) interconnected by lower intensity intermediated velocity emission,
 tracing a broad bridge feature. The presence of such broad bridge feature
 suggests the onset of a collision between molecular components in the MCG35.2.
 A noticeable change in the H-band starlight mean polarization angles has also
 been observed in the MCG35.2, probably tracing the interaction between
 molecular components. Taken together, it seems that the cloud-cloud collision
 process has influenced the birth of massive stars and YSOs clusters in the
 MCG35.2.",0,1,0,0,0,0,
498,Oblivious Routing via Random Walks,"We present novel oblivious routing algorithms for both splittable and
 unsplittable multicommodity flow. Our algorithm for minimizing congestion for
 \emph{unsplittable} multicommodity flow is the first oblivious routing
 algorithm for this setting. As an intermediate step towards this algorithm, we
 present a novel generalization of Valiant's classical load balancing scheme for
 packet-switched networks to arbitrary graphs, which is of independent interest.
 Our algorithm for minimizing congestion for \emph{splittable} multicommodity
 flow improves upon the state-of-the-art, in terms of both running time and
 performance, for graphs that exhibit good expansion guarantees. Our algorithms
 rely on diffusing traffic via iterative applications of the random walk
 operator. Consequently, the performance guarantees of our algorithms are
 derived from the convergence of the random walk operator to the stationary
 distribution and are expressed in terms of the spectral gap of the graph (which
 dominates the mixing time).",1,0,0,0,0,0,
499,On Functional Graphs of Quadratic Polynomials,"We study functional graphs generated by quadratic polynomials over prime
 fields. We introduce efficient algorithms for methodical computations and
 provide the values of various direct and cumulative statistical parameters of
 interest. These include: the number of connected functional graphs, the number
 of graphs having a maximal cycle, the number of cycles of fixed size, the
 number of components of fixed size, as well as the shape of trees extracted
 from functional graphs. We particularly focus on connected functional graphs,
 that is, the graphs which contain only one component (and thus only one cycle).
 Based on the results of our computations, we formulate several conjectures
 highlighting the similarities and differences between these functional graphs
 and random mappings.",0,0,1,0,0,0,
500,Helmholtz decomposition theorem and Blumenthal's extension by regularization,"Helmholtz decomposition theorem for vector fields is usually presented with
 too strong restrictions on the fields and only for time independent fields.
 Blumenthal showed in 1905 that decomposition is possible for any asymptotically
 weakly decreasing vector field. He used a regularization method in his proof
 which can be extended to prove the theorem even for vector fields
 asymptotically increasing sublinearly. Blumenthal's result is then applied to
 the time-dependent fields of the dipole radiation and an artificial sublinearly
 increasing field.",0,1,0,0,0,0,
501,A homotopy decomposition of the fibre of the squaring map on $??^3S^{17}$,"We use Richter's $2$-primary proof of Gray's conjecture to give a homotopy
 decomposition of the fibre $\Omega^3S^{17}\{2\}$ of the $H$-space squaring map
 on the triple loop space of the $17$-sphere. This induces a splitting of the
 mod-$2$ homotopy groups $\pi_\ast(S^{17}; \mathbb{Z}/2\mathbb{Z})$ in terms of
 the integral homotopy groups of the fibre of the double suspension
 $E^2:S^{2n-1} \to \Omega^2S^{2n+1}$ and refines a result of Cohen and Selick,
 who gave similar decompositions for $S^5$ and $S^9$. We relate these
 decompositions to various Whitehead products in the homotopy groups of mod-$2$
 Moore spaces and Stiefel manifolds to show that the Whitehead square $[i_{2n},
 i_{2n}]$ of the inclusion of the bottom cell of the Moore space $P^{2n+1}(2)$
 is divisible by $2$ if and only if $2n=2, 4, 8$ or $16$.",0,0,1,0,0,0,
502,Spaces of orders of some one-relator groups,"We show that certain orderable groups admit no isolated left orders. The
 groups we consider are cyclic amalgamations of a free group with a general
 orderable group, the HNN extensions of free groups over cyclic subgroups, and a
 particular class of one-relator groups. In order to prove the results about
 orders, we develop perturbation techniques for actions of these groups on the
 line.",0,0,1,0,0,0,
503,Adversarial Attacks on Neural Network Policies,"Machine learning classifiers are known to be vulnerable to inputs maliciously
 constructed by adversaries to force misclassification. Such adversarial
 examples have been extensively studied in the context of computer vision
 applications. In this work, we show adversarial attacks are also effective when
 targeting neural network policies in reinforcement learning. Specifically, we
 show existing adversarial example crafting techniques can be used to
 significantly degrade test-time performance of trained policies. Our threat
 model considers adversaries capable of introducing small perturbations to the
 raw input of the policy. We characterize the degree of vulnerability across
 tasks and training algorithms, for a subclass of adversarial-example attacks in
 white-box and black-box settings. Regardless of the learned task or training
 algorithm, we observe a significant drop in performance, even with small
 adversarial perturbations that do not interfere with human perception. Videos
 are available at this http URL.",1,0,0,1,0,0,
504,Stellar streams as gravitational experiments I. The case of Sagittarius,"Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy
 offer a unique way to constrain the shape of galactic gravitational potentials.
 Such streams can be used as leaning tower gravitational experiments on galactic
 scales. The most well motivated modification of gravity proposed as an
 alternative to dark matter on galactic scales is Milgromian dynamics (MOND),
 and we present here the first ever N-body simulations of the dynamical
 evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a
 realistic baryonic mass model for the Milky Way, we attempt to reproduce the
 present-day spatial and kinematic structure of the Sagittarius dwarf and its
 immense tidal stream that wraps around the Milky Way. With very little freedom
 on the original structure of the progenitor, constrained by the total
 luminosity of the Sagittarius structure and by the observed stellar mass-size
 relation for isolated dwarf galaxies, we find reasonable agreement between our
 simulations and observations of this system. The observed stellar velocities in
 the leading arm can be reproduced if we include a massive hot gas corona around
 the Milky Way that is flattened in the direction of the principal plane of its
 satellites. This is the first time that tidal dissolution in MOND has been
 tested rigorously at these mass and acceleration scales.",0,1,0,0,0,0,
505,Tuning quantum non-local effects in graphene plasmonics,"The response of an electron system to electromagnetic fields with sharp
 spatial variations is strongly dependent on quantum electronic properties, even
 in ambient conditions, but difficult to access experimentally. We use
 propagating graphene plasmons, together with an engineered dielectric-metallic
 environment, to probe the graphene electron liquid and unveil its detailed
 electronic response at short wavelengths.The near-field imaging experiments
 reveal a parameter-free match with the full theoretical quantum description of
 the massless Dirac electron gas, in which we identify three types of quantum
 effects as keys to understanding the experimental response of graphene to
 short-ranged terahertz electric fields. The first type is of single-particle
 nature and is related to shape deformations of the Fermi surface during a
 plasmon oscillations. The second and third types are a many-body effect
 controlled by the inertia and compressibility of the interacting electron
 liquid in graphene. We demonstrate how, in principle, our experimental approach
 can determine the full spatiotemporal response of an electron system.",0,1,0,0,0,0,
506,Flows along arch filaments observed in the GRIS 'very fast spectroscopic mode',"A new generation of solar instruments provides improved spectral, spatial,
 and temporal resolution, thus facilitating a better understanding of dynamic
 processes on the Sun. High-resolution observations often reveal
 multiple-component spectral line profiles, e.g., in the near-infrared He I
 10830 \AA\ triplet, which provides information about the chromospheric velocity
 and magnetic fine structure. We observed an emerging flux region, including two
 small pores and an arch filament system, on 2015 April 17 with the 'very fast
 spectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the
 1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We
 discuss this method of obtaining fast (one per minute) spectral scans of the
 solar surface and its potential to follow dynamic processes on the Sun. We
 demonstrate the performance of the 'very fast spectroscopic mode' by tracking
 chromospheric high-velocity features in the arch filament system.",0,1,0,0,0,0,
